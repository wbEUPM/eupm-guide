---
title: "My Document"
bibliography: references.bib
---

# Area-level estimation: Fay-Herriot model

## Introduction

In this section, we will present a whole estimation procedure of the standard area-level model introduced by [@Fay1979] in R. As with the disclaimer in the preceding section, this practical manual is not intended to serve as a theoretical introduction to area-level models. Instead, it offers a set of straightforward and practical R scripts, accompanied by clear explanations, to demonstrate how these tasks can be carried out in R. For a theoretical foundation please refer to [@Fay1979] and [@RaoMolina2015]. In addition to theoretical information, the vignette "A framework for producing small area estimates based on area-level models in R" of the R package emdi ([@Harmening2023]) provides further code examples for the FH model.

In this chapter, we will describe how to run the univariate (standard) Fay-Herriot (FH) using simulated income data from Spain. The estimation procedure is explained step by step.

Step 1: Data preparation. Compute the direct estimates and their corresponding variances on the area-level and eventually perform variance smoothing. Aggregate the available auxiliary variables to the same area level and combine both input data.

Step 2: Model selection. Select the aggregated auxiliary variables at the area level for the FH model using a stepwise selection based on information criteria like the Akaike, Bayesian or Kullback information criteria.

Step 3: Model estimation of FH point estimates and their mean squared error (MSE) estimates as uncertainty measure. Eventually apply a transformation.

Step 4: Assessment of the estimated model. Check the FH model assumptions, including linearity, normality of predicted area effects and standardized model residuals. When violations of the model assumptions are detected, the application of a transformation might help. Repeat Step 3 including a transformation and check again the model assumptions.

Step 5: Comparison of the FH results with the direct estimates.

Step 6: Benchmark the FH point estimates for consistency with higher results.

Step 7: Preparation of the results. Create tables and/or maps of results,

Step 8: Saving the results. One option is to export the results to known formats like Excel or OpenDocument Spreadsheets.

## Data and preparation

### Load required libraries

First of all, load the required R libraries. The code automatically installs the packages that are not yet installed and loads them. If you need further packages, please add them to the list of `p_load` which contains the packages required to the run the codes.

```{r libraries, message = FALSE, warning = FALSE}
if (sum(installed.packages()[,1] %in% "pacman") != 1){
  
  install.packages("pacman")
  
}

pacman::p_load(sae, survey, spdep, emdi, data.table, MASS, caret, dplyr)

```

### Load the dataset

First, we need to load the dataset "comb_Data_poor.RDS" which contains the direct estimates ("poor") and their variances ("vardir"), the sample sizes ("n"), the design effects ("DEff.poor") and the auxiliary variables.

```{r load_data, message = FALSE, warning = FALSE}
comb_Data_poor <- readRDS("data/comb_Data_poor.RDS")
summary(comb_Data_poor)
```


### Variance smoothing

```{r variance_smoothing, message = FALSE, warning = FALSE}
combined_data <- combined_data |>
  mutate(
    log_s2 = log(dir_fgt0_var),
    logN = log(N),
    logN2 = logN^2,
    logpop = log(pop),
    logpop2 = logpop^2,
    accra = as.numeric(region.x == 3),
    share = log(N_hhsize / pop)
  )

fit <- lm(log_s2 ~ share, data = combined_data)
summary(fit)
phi2 <- summary(fit)$sigma^2
combined_data$xb_fh <- predict(fit, combined_data)

combined_data <- combined_data |>
  mutate(
    exp_xb_fh = exp(xb_fh),
    smoothed_var = exp_xb_fh * (sum(dir_fgt0_var, na.rm = TRUE) / sum(exp_xb_fh, na.rm = TRUE)),
    dir_fgt0_var = ifelse(((num_ea > 1 & !is.na(num_ea)) | (num_ea == 1 & zero != 0 & zero != 1)) & is.na(dir_fgt0_var),
                                   smoothed_var, dir_fgt0_var),
             dir_fgt0 = ifelse(!is.na(dir_fgt0_var), zero, dir_fgt0)
  )
```

## Model selection

### Model preparation

FH does not run if there is any missing value in the auxiliary variables, and therefore, any variable with missing value should be removed in advance.

```{r fh_arcsin, message = FALSE, warning = FALSE}
form <- poor ~ abs+ntl+aec+schyrs+mkt #+
  #gen + age2+age3+age4+age5+educ1+educ2+    
 #educ3+nat1+labor1+labor2+labor3

rowsNAcovarites <- rowSums(sapply(comb_Data_poor[, all.vars(form)[-1]], is.na))
comb_Data_poor <- comb_Data_poor[rowsNAcovarites == 0, ]
```

### Check multicollinearity

The model does not run if there is (perfect) multicollinearity between variables, so we need to omit them in advance. For that, we create the function stepCOL.

```{r multicol, message = FALSE, warning = FALSE}

stepCOL <- function(dt, xvars, y, cor_thresh = 0.95) {
  
#   library(data.table)
 # library(MASS)        # For stepAIC
 # library(caret)       # For findLinearCombos
  
  dt <- as.data.table(dt)
  
  # Drop columns that are entirely NA
  dt <- dt[, which(unlist(lapply(dt, function(x) !all(is.na(x))))), with = FALSE]
  
  xvars <- xvars[xvars %in% colnames(dt)]
  
  # Keep only complete cases
  dt <- na.omit(dt[, c(y, xvars), with = FALSE])
  
  
  # Step 3: Drop highly correlated variables
  xmat <- as.matrix(dt[, ..xvars])
  cor_mat <- abs(cor(xmat))
  diag(cor_mat) <- 0
  while (any(cor_mat > cor_thresh, na.rm = TRUE)) {
    cor_pairs <- which(cor_mat == max(cor_mat, na.rm = TRUE), arr.ind = TRUE)[1, ]
    var1 <- colnames(cor_mat)[cor_pairs[1]]
    var2 <- colnames(cor_mat)[cor_pairs[2]]
    # Drop the variable with higher mean correlation
    drop_var <- if (mean(cor_mat[var1, ]) > mean(cor_mat[var2, ])) var1 else var2
    xvars <- setdiff(xvars, drop_var)
    xmat <- as.matrix(dt[, ..xvars])
    cor_mat <- abs(cor(xmat))
    diag(cor_mat) <- 0
  }
  
  # Step 2: Remove near-linear combinations
 # xmat <- as.matrix(dt[, ..xvars])
  combo_check <- tryCatch(findLinearCombos(xmat), error = function(e) NULL)
  if (!is.null(combo_check) && length(combo_check$remove) > 0) {
    xvars <- xvars[-combo_check$remove]
    xmat <- as.matrix(dt[, ..xvars])
  }
  
  # Step 1: Remove aliased (perfectly collinear) variables
  model_formula <- as.formula(paste(y, "~", paste(xvars, collapse = " + ")))
  lm_model <- lm(model_formula, data = dt)
  aliased <- is.na(coef(lm_model))
  if (any(aliased)) {
    xvars <- names(aliased)[!aliased & names(aliased) != "(Intercept)"]
  }
  return(xvars)
}


```

### Apply the function to remove the variables with high multicollinearity

```{r remove_multicol, message = FALSE, warning = FALSE}
xvars_initial <- colnames(comb_Data_poor[, all.vars(form)[-1]])
xvars <- stepCOL(dt = comb_Data_poor, xvars = xvars_initial,
                                y = "poor",
                                cor_thresh = 0.95)
print(xvars)
```


### Model creation and variable selection

In this example, we use the function fh to calculate the FH estimates. Because we want to estimate a ratio, we need to apply the arcsin transformation to guarantee that the results lie between 0 and 1. For that, we choose "arcsin" as transformation, and a a bias-corrected backtransformation ("bc"). Additionally, the effective sample size, which equals the sample size of each area divided by the design effect, is needed for the arcsin transformation. With the help of the step function of package emdi, we perform a variable selection based on the AIC criterion and directly get the model with fewer variables.

```{r model_select, message = FALSE, warning = FALSE}
comb_Data_poor <- comb_Data_poor |>
  mutate(n_eff = n/DEff.poor)

fh_start <- step(fh(
  fixed = poor ~ abs + ntl + aec + schyrs + mkt,
  vardir = "vardir", combined_data = comb_Data_poor, domains = "prov",
  method = "ml", transformation = "arcsin", backtransformation = "bc",
  eff_smpsize = "n_eff", MSE = FALSE)) 
```

## Model estimation of FH point and their MSE estimates.

```{r model_est, message = FALSE, warning = FALSE}
fh_arcsin <- fh(
  fixed = formula(fh_start),
  vardir = "vardir", combined_data = comb_Data_poor, domains = "prov",
  method = "ml", transformation = "arcsin", backtransformation = "bc",
  eff_smpsize = "n_eff", MSE = TRUE, mse_type = "boot", B = c(50, 0)) 
```

### Comparison of the different FH methods

The commonly used variance estimation methods (e.g. ml/reml) may produce negative variance estimates that are supposed to be strictly positive. Negative variance estimates are set to zero resulting in zero estimates of the shrinkage factor. Therefore no weight is put on the direct estimator ignoring its possible reliability. This poses a problem especially when the number of areas is small. Check the variance estimates of the model and use one of the adjusted variance estimation methods of package emdi (e.g. ampl_yl) in case the variance equals zero.

```{r fh_arcsin_comparison, message = FALSE, warning = FALSE}
fh_arcsin$model$variance

if (fh_arcsin$model$variance == 0){
  fh_arcsin$call$method <- "ampl_yl"
  fh_arcsin <- eval(fh_arcsin$call)
}

```

### Obtain SAE-FH estimates

```{r fh_arcsin_final, message = FALSE, warning = FALSE}
# Summary statistics of EBLUP estimates
summary(fh_arcsin$ind$FH)

# Summary statistics of MSE estimates
summary(fh_arcsin$MSE$FH)
```

## Assessment of the estimated model.

The summary method of emdi provides additional information about the data and model components, in particular the chosen estimation methods, the number of domains, the log-likelihood, the information criteria by Marhuenda et al. (2014), the adjusted R2 of a standard linear model and the adjusted R2 especially for FH models proposed by Lahiri & Suntornchost (2015). Additionally, measures to validate model assumptions about the standardized realized residuals and the random effects are provided: skewness and kurtosis (skewness and kurtosis of package moments, Komsta and Novomestky, 2015) of the standardized realized residuals and the random effects and the test statistics with corresponding p value of the Shapiro-Wilks-test for normality of both error terms.

```{r summary, message = FALSE, warning = FALSE}
summary(fh_arcsin)
```

### Diagnostic plots

We produce normal quantile-quantile (Q-Q) plots of the standardized realized residuals and random effects and plots of the kernel densities of the distribution of both error terms.

```{r plot, message = FALSE, warning = FALSE}
plot(fh_arcsin)
```

## Comparison of the FH results with the direct estimates.

We produce a scatter plot proposed by Brown et al. (2001) and a line plot. Besides the direct and FH estimates, the plot contains the fitted regression and the identity line. Both lines should not differ too much. Preferably, the model-based (FH) estimates should track the direct estimates within the line plot especially for domains with a large sample size/small MSE of the direct estimator. The input arguments MSE and CV of compare_plot can be set to TRUE leading to two extra plots. The MSE/CV estimates of the direct and model-based (FH) estimates are compared via boxplots and ordered scatter plots.

The function compare enables the user to compute a goodness of fit diagnostic (Brown et al., 2001) and a correlation coefficient of the direct estimates and the estimates of the regression-synthetic part of the FH model (Chandra et al., 2015). Following Brown et al. (2001), the difference between the model-based estimates and the direct estimates should not be significant (null hypothesis).

```{r compare, message = FALSE, warning = FALSE}
compare_plot(fh_arcsin, CV = TRUE)
compare(fh_arcsin)

```

## Benchmark the FH point estimates for consistency with higher results.

```{r benchmarking, message = FALSE, warning = FALSE}
fh_bench <- benchmark(fh_arcsin,
                      benchmark = 0.2165049,
                      share = comb_Data_poor$ratio_n, 
                      type = "ratio",
                      overwrite = TRUE)

```

## Preparation of the results.

```{r res_prep, message = FALSE, warning = FALSE}
estimators(fh_arcsin, MSE = TRUE, CV = TRUE)

```

### Visualize the results

```{r visualize, message = FALSE, warning = FALSE}
# map

```

## Saving the results.

### Save and output Final dataset

```{r save, message = FALSE, warning = FALSE}
write.excel(fh_arcsin,
  file = "fh_arcsin_output.xlsx",
  MSE = TRUE, CV = TRUE
)
```
