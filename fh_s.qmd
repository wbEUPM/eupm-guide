---
title: "My Document"
bibliography: references.bib
---

# Area-level estimation: Fay-Herriot model

## Introduction

In this section, we will present a whole estimation procedure of the standard area-level model introduced by [@Fay1979] in R. As with the disclaimer in the preceding section, this practical manual is not intended to serve as a theoretical introduction to area-level models. Instead, it offers a set of straightforward and practical R scripts, accompanied by clear explanations, to demonstrate how these tasks can be carried out in R. For a theoretical foundation please refer to [@Fay1979] and [@RaoMolina2015]. In addition to theoretical information, the vignette "A framework for producing small area estimates based on area-level models in R" of the R package emdi ([@Harmening2023]) provides further code examples for the FH model.

In this chapter, we will describe how to run the univariate (standard) Fay-Herriot (FH) using simulated income data from Spain. The estimation procedure is explained step by step.

Step 1: Data preparation. Compute the direct estimates and their corresponding variances on the area-level and eventually perform variance smoothing. Aggregate the available auxiliary variables to the same area level and combine both input data.

Step 2: Model selection. Select the aggregated auxiliary variables at the area level for the FH model using a stepwise selection based on information criteria like the Akaike, Bayesian or Kullback information criteria.

Step 3: Model estimation of FH point estimates and their mean squared error (MSE) estimates as uncertainty measure. Eventually apply a transformation.

Step 4: Assessment of the estimated model. Check the FH model assumptions, including linearity, normality of predicted area effects and standardized model residuals. When violations of the model assumptions are detected, the application of a transformation might help. Repeat Step 3 including a transformation and check again the model assumptions.

Step 5: Comparison of the FH results with the direct estimates.

Step 6: Benchmark the FH point estimates for consistency with higher results.

Step 7: Preparation of the results. Create tables and/or maps of results,

Step 8: Saving the results. One option is to export the results to known formats like Excel or OpenDocument Spreadsheets.

We will show below the use of the `eblupMFH2()` and `eblupMFH3()` from the R package msae [@permatasari2022package] compute the EBLUPs and their MSE estimates under the MFH models 2 and 3, respectively. The calls to these functions are:

`eblupMFH2(formula, vardir, MAXITER = 100, PRECISION = 1e-04, data)`

`eblupMFH3(formula, vardir, MAXITER = 100, PRECISION = 1e-04, data)`

## Data and preparation

### Load required libraries

First of all, load the required R libraries. The code automatically installs the packages that are not yet installed and loads them. If you need further packages, please add them to the list of `p_load` which contains the packages required to the run the codes.

```{r libraries, message = FALSE, warning = FALSE}
if (sum(installed.packages()[,1] %in% "pacman") != 1){
  
  install.packages("pacman")
  
}

pacman::p_load(sae, survey, spdep, emdi, data.table, MASS, caret, dplyr)

```

### Load the dataset

In this example, we use a synthetic data set adapted from R package `sae` called `incomedata`. The original data contains information for $n = 17,119$ fictitious individuals residing across $D = 52$ Spanish provinces. The variables include the name of the province of residence (`provlab`), province code (`prov`), as well as several correlates of income.

For this tutorial, we use a random 10% sample of the `incomedata` to estimate the poverty rates. We use the income variable from 2012.

First, we need to load the dataset "comb_Data.RDS" which contains the direct estimates ("poor") and their variances ("vardir"), the sample sizes ("n"), the design effects ("DEff.poor") and the auxiliary variables.

```{r load_data, message = FALSE, warning = FALSE}
income_dt <- readRDS("data/incomedata_sample.RDS")

glimpse(income_dt)
```

### Direct estimation

We will use the direct HT estimators that use the survey weights in `weight` variable. First, we calculate the total sample size, the number of provinces, the sample sizes for each province and extract the population sizes for each province/target area from the `sizeprov` file. For those using the household/individual level survey data, this may be obtained from the sum of the household or individual weights as appropriate.

```{r direct, message = FALSE, warning = FALSE}
## The poverty line for each is already included within the data. 
## Lets compute the direct estimates for each year of data and create a  data.frame containing the direct estimate, the standard errors, the variances, the coefficient of variation and the design effects, that are needed for arcsin transformation. We use
## the direct function of the emdi package here. Other options are e.g. the direct function of package sae or the svyby command of package survey. In some areas with a 
## very small sample size, it may occur, that the area only consists of zeros and ones, resulting in a direct estimate of zero or one and a direct variance of zero.
## We set those areas to out-of-sample and for the final estimation results only the synthetic part of the FH model is used.

## quickly compute sample size for each province
sampsize_dt <- 
income_dt |>
  group_by(provlab) |>
  summarize(N = n())

direct_dt <- emdi::direct(y = "income2012",
                       smp_data = income_dt %>% as.data.table(),
                       smp_domains = "provlab",
                       weights = "weight",
                       threshold = unique(income_dt$povline2012),
                       var = TRUE)

direct_dt <- 
       direct_dt$ind |>
       dplyr::select(Domain, Head_Count) |>
       rename(Direct = "Head_Count") |>
       merge(direct_dt$MSE |>
               dplyr::select(Domain, Head_Count) |>
               rename(vardir = "Head_Count"),
             by = "Domain") |>
       mutate(SD = sqrt(vardir)) |>
       mutate(CV = SD / Direct) |>
       merge(sampsize_dt |> 
               mutate(provlab = as.factor(provlab)), 
             by.x = "Domain", 
             by.y = "provlab")

## set zero variance to OOS
direct_dt <- direct_dt[complete.cases(direct_dt), ]

## The design effect is the ratio of the variance considering the sampling design to the variance estimated under
## simple random sampling.
direct_dt$var_SRS <- direct_dt$Direct * (1 - direct_dt$Direct) / direct_dt$N
direct_dt$deff <- direct_dt$vardir / direct_dt$var_SRS
direct_dt <- direct_dt |>
  mutate(n_eff = N/deff)
summary(direct_dt$N)    
```

### Variance smoothing

A quick inspection of the preceding results will show some provinces contain low sample sizes which sometimes result in extreme value poverty rates and hence 0 variance. To avoid this, we will show you how to apply the variance smoothing method suggested by [@you2023application]. Please see the code and Roxygen comments below explaining the use of the `varsmoothie_king()` function which computes smoothed variances. In case, the arcsin transformation will be applied, the variance smoothing described here is not necessary, since the arcsin transformation works variance stabilizing itself. When applying the arcsin transformation, the direct variances are automatically set to 1/(4\*effective sampling size) when using the fh function of package emdi. The effective sample size equals the sample size of each area divided by the design effect. If the variance stabilizing effect is not enough, the design effect of a higher area level could also be used here (in this example the regions ac).

```{r varsmooting}

#' A function to perform variance smoothing
#' 
#' The variance smoothing function applies the methodology of (Hiridoglou and You, 2023)
#' which uses simply log linear regression to estimate direct variances for sample 
#' poverty rates which is useful for replacing poverty rates in areas with low sampling.
#' 
#' @param domain a vector of unique domain/target areas
#' @param direct_var the raw variances estimated from sample data e=
#' @param sampsize the sample size for each domain
#' 
#' @export

varsmoothie_king <- function(domain,
                             direct_var,
                             sampsize){

  dt <- data.table(Domain = domain,
                   var = direct_var,
                   n = sampsize)

  dt$log_n <- log(dt$n)
  dt$log_var <- log(dt$var)

  lm_model <- lm(formula = log_var ~ log_n,
                 data = dt[!(abs(dt$log_var) == Inf),])

  dt$pred_var <- predict(lm_model, newdata = dt)
  residual_var <-  summary(lm_model)$sigma^2
  dt$var_smooth <- exp(dt$pred_var) * exp(residual_var/2)

  return(dt[, c("Domain", "var_smooth"), with = F])

}


```

OK, the goal now is to use the above `varsmoothie_king()` function to add an additional column of smoothed variances into our `direct_dt` dataframe.

```{r}
var_smooth <- varsmoothie_king(domain = direct_dt$Domain,
                                 direct_var = direct_dt$vardir,
                                 sampsize = direct_dt$N)
direct_dt <- var_smooth %>% merge(direct_dt, by = "Domain")
```

```{r variance_smoothing, message = FALSE, warning = FALSE}

# all districts where the variance could not be estimated because only 1 enumeration area was sampled and because the district’s direct estimate is equal to 0 or 1 are not used for the modelling. Consequently, their small area estimate is composed entirely of their synthetic estimator. 
#For all others, either the district’s sampling variance is used and in cases where the sampling variance is not available smoothed variance is used.
```

The FH model is a model of poverty rates at the target area level, hence the data format required for this exercise has the province as its unit of observation. This format has a few essential columns:

-   Variable for poverty rates

-   The set of candidate variables from which the most predicted of poverty rates will be selected

-   The target area variable identifier (i.e. in this case the province variable `prov` and `provlab`)

We prepare this dataset as follows:

```{r}

## create the candidate variables
candidate_vars <- colnames(income_dt)[!colnames(income_dt) %in% 
                                         c("provlab", "prov", "income",
                                           "weight", "povline", "y",
                                           "poverty", "y0", "y1", "y2",
                                           "p0_prov", "p1_prov", "p2_prov",
                                           "ac", "nat", "educ", "labor",
                                           "age")]

candidate_vars <- candidate_vars[!grepl("sampsize|prov|poverty|income|povline|^v[0-9]|^y[0-9]", candidate_vars)] 

### computing the province level data
prov_dt <- 
income_dt |>
  group_by(provlab) |>
  summarize(
    across(
      any_of(candidate_vars),
      ~ weighted.mean(x = ., w = weight, na.rm = TRUE),
      .names = "{.col}"
    )
  ) 

```

Now, we combine the dataframe containing the direct estimates and their variances with the province level data.

```{r comb, message = FALSE, warning = FALSE}
comb_Data <- merge(direct_dt, prov_dt,
    by.x = "Domain", by.y = "provlab",
    all = TRUE)
```

## Model selection

### Model preparation

FH does not run if there is any missing value in the auxiliary variables, and therefore, any variable with missing value should be removed in advance.

```{r fh_arcsin, message = FALSE, warning = FALSE}
rowsNAcovariates <- rowSums(sapply(comb_Data[, candidate_vars], is.na))
comb_Data <- comb_Data[rowsNAcovariates == 0, ]
```

### Build initial model

First we build the initial model, to which the variable selection is then applied.

In this example, we use the function fh to calculate the FH estimates. Because we want to estimate a ratio, we need to apply the arcsin transformation to guarantee that the results lie between 0 and 1. For that, we choose "arcsin" as transformation, and a bias-corrected backtransformation ("bc"). Additionally, the effective sample size, which equals the sample size of each area divided by the design effect, is needed for the arcsin transformation. For the variable selection, we set the MSE estimation to FALSE. In case, no transformation is desired, the transformation argument must be set to "no" and the inputs backtransformation and eff_smpsize are no longer needed.

### Check multicollinearity

With the help of the `step()` function of package emdi, we perform a variable selection based on the AIC criterion and directly get the model with fewer variables. The function `stepAIC_wrapper()` implemented below is a wrapper to the `step()` function carries all the perfunctory cleaning necessary use the `step()` function. This includes dropping columns that are entirely missing (`NA`) and keep only complete cases/observations (for the model selection only the in-sample domains are used) and remove perfectly or near collinear variables and combinations using the variance inflation method.

```{r}

#' A function to perform stepwise variable selection with AIC selection criteria
#' 
#' @param dt data.frame, dataset containing the set of outcome and independent variables
#' @param xvars character vector, the set of x variables
#' @param y chr, the name of the y variable
#' @param cor_thresh double, a correlation threshold between 0 and 1
#' @param criteria character string, criteria that can be chosen are "AIC", "AICc", "AICb1", "AICb2", "BIC", "KIC", "KICc", "KICb1", or "KICb2". Defaults to "AIC". If transformation is set to "arcsin", only "AIC" and "BIC" can be chosen.
#' @param vardir character string, name of the variable containing the domain-specific sampling variances of the direct estimates that are included in dt
#' @param transformation character string, either "no" (default) or "arcsin".
#' @param eff_smpsize character string, name of the variable containing the effective sample sizes that are included in dt. Required argument when the arcsin transformation is chosen. Defaults to NULL.
#' 
#' @import data.table
#' @importFrom emdi step

step_wrapper <- function(dt, xvars, y, cor_thresh = 0.95, criteria = "AIC",
                         vardir, transformation = "no", eff_smpsize) {
  
  dt <- as.data.table(dt)
  
  # Drop columns that are entirely NA
  dt <- dt[, which(unlist(lapply(dt, function(x) !all(is.na(x))))), with = FALSE]
  
  xvars <- xvars[xvars %in% colnames(dt)]
  
  # Keep only complete cases
  dt <- dt[complete.cases(dt),] 
  
  # Step 1: Remove aliased (perfectly collinear) variables
  model_formula <- as.formula(paste(y, "~", paste(xvars, collapse = " + ")))
  lm_model <- lm(model_formula, data = dt)
  aliased <- is.na(coef(lm_model))
  if (any(aliased)) {
    xvars <- names(aliased)[!aliased & names(aliased) != "(Intercept)"]
  }
  
  # Step 2: Remove near-linear combinations
  xmat <- as.matrix(dt[, ..xvars])
  combo_check <- tryCatch(findLinearCombos(xmat), error = function(e) NULL)
  if (!is.null(combo_check) && length(combo_check$remove) > 0) {
    xvars <- xvars[-combo_check$remove]
    xmat <- as.matrix(dt[, ..xvars])
  }
  
  # Step 3: Drop highly correlated variables
  cor_mat <- abs(cor(xmat))
  diag(cor_mat) <- 0
  while (any(cor_mat > cor_thresh, na.rm = TRUE)) {
    cor_pairs <- which(cor_mat == max(cor_mat, na.rm = TRUE), arr.ind = TRUE)[1, ]
    var1 <- colnames(cor_mat)[cor_pairs[1]]
    var2 <- colnames(cor_mat)[cor_pairs[2]]
    # Drop the variable with higher mean correlation
    drop_var <- if (mean(cor_mat[var1, ]) > mean(cor_mat[var2, ])) var1 else var2
    xvars <- setdiff(xvars, drop_var)
    xmat <- as.matrix(dt[, ..xvars])
    cor_mat <- abs(cor(xmat))
    diag(cor_mat) <- 0
  }
  
  # Step 4: Warn if still ill-conditioned
  cond_number <- kappa(xmat, exact = TRUE)
  if (cond_number > 1e10) {
    warning("Design matrix is ill-conditioned (condition number > 1e10). Consider reviewing variable selection.")
  }
  
  # Final model fit
  model_formula <- as.formula(paste(y, "~", paste(xvars, collapse = " + ")))
  
  # Stepwise selection
  if (transformation == "no"){
    stepwise_model <- step(fh(
  fixed = model_formula,
  vardir = vardir, combined_data = dt,
  method = "ml", transformation = "no", MSE = FALSE, B = c(0, 50)),
  criteria = criteria)
  }
  
  if (transformation == "arcsin"){
    stepwise_model <- step(fh(
  fixed = model_formula,
  vardir = vardir, combined_data = dt,
  method = "ml", transformation = "arcsin", backtransformation = "bc",
  eff_smpsize = "n_eff", MSE = FALSE),
  criteria = criteria)
  }
  
  
  return(stepwise_model)
  
}


```

### Apply the function to remove the variables with high multicollinearity

```{r remove_multicol, message = FALSE, warning = FALSE}
fh_step <- step_wrapper(dt = comb_Data, 
                      xvars = candidate_vars,
                      y = "Direct",
                      cor_thresh = 0.8,
                      criteria = "AIC",
                      vardir = "vardir", 
                      transformation = "arcsin", 
                      eff_smpsize = "n_eff")
print(fh_step$fixed)
```

## Model estimation of FH point and their MSE estimates.

```{r model_est, message = FALSE, warning = FALSE}
fh_arcsin <- fh(
  fixed = formula(fh_step$fixed),
  vardir = "vardir", combined_data = comb_Data, domains = "Domain",
  method = "ml", transformation = "arcsin", backtransformation = "bc",
  eff_smpsize = "n_eff", MSE = TRUE, mse_type = "boot", B = c(50, 0)) 

# Summary statistics of EBLUP estimates
summary(fh_arcsin$ind$FH)

# Summary statistics of MSE estimates
summary(fh_arcsin$MSE$FH)
```

## Assessment of the estimated model.

With the help of the `summary` method in the **emdi** package, we gain detailed insights into the data and model components. It includes information on the estimation methods used, the number of domains, the log-likelihood, and information criteria as proposed by Marhuenda et al. (2014). It also reports the adjusted R² from a standard linear model and the adjusted R² specific to FH models, as introduced by Lahiri and Suntornchost (2015). It also offers diagnostic measures to assess model assumptions regarding the standardized realized residuals and random effects. These include skewness and kurtosis (based on the *moments* package by Komsta and Novomestky, 2015), as well as Shapiro-Wilk test statistics and corresponding p-values to evaluate the normality of both error components.

```{r summary, message = FALSE, warning = FALSE}
summary(fh_arcsin)
```

### Diagnostic plots

We produce normal quantile-quantile (Q-Q) plots of the standardized realized residuals and random effects and plots of the kernel densities of the distribution of both error terms.

```{r plot, message = FALSE, warning = FALSE}
plot(fh_arcsin)
```

## Comparison of the FH results with the direct estimates.

The FH estimates are expected to align closely with the direct estimates in domains with small direct MSEs and/or large sample sizes. Moreover, incorporating auxiliary information should enhance the precision of the direct estimates. We produce a scatter plot proposed by Brown et al. (2001) and a line plot. The plot also contains the fitted regression and the identity line. Both lines should not differ too much. The FH estimates should track the direct estimates within the line plot especially for domains with a large sample size/small MSE of the direct estimator. Furthermore, we compare the MSE and CV estimates for the direct and FH estimators using boxplots and ordered scatter plots (by setting the input arguments MSE and CV to TRUE).

Additionally, we compute a correlation coefficient of the direct estimates and the estimates of the regression-synthetic part of the FH model (Chandra et al., 2015) and a goodness of fit diagnostic (Brown et al., 2001).

```{r compare, message = FALSE, warning = FALSE}
compare_plot(fh_arcsin, MSE = TRUE, CV = TRUE)
compare(fh_arcsin)

```

## Benchmark the FH point estimates for consistency with higher results.

```{r benchmarking, message = FALSE, warning = FALSE}
## quickly creating the poverty indicator
income_dt <- 
  income_dt |>
  mutate(poor2012 = ifelse(income2012 < povline2012, 1, 0))
## computing the benchmark value
benchmark_value <- weighted.mean(income_dt$poor2012, income_dt$weight)
## computing the share of population size in the total population size (Ni/N) per area
data("sizeprov")
comb_Data <- comb_Data |>
  left_join(sizeprov |>
  mutate(ratio_n = Nd/sum(Nd)), by = c("Domain" ="provlab"))


fh_bench <- benchmark(fh_arcsin,
                      benchmark = benchmark_value,
                      share = comb_Data$ratio_n, 
                      type = "ratio",
                      overwrite = TRUE)

```

## Preparation of the results.

```{r res_prep, message = FALSE, warning = FALSE}
estimators(fh_arcsin, MSE = TRUE, CV = TRUE)

```

## Saving the results.

### Export the model output and estimation results.

```{r save, message = FALSE, warning = FALSE}
write.excel(fh_arcsin,
  file = "fh_arcsin_output.xlsx",
  MSE = TRUE, CV = TRUE
)
```
