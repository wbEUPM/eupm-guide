# Fay--Herriot

This chapter describes how to run the simple Fay-Herriot using the real data from Ghana. If there is a need to consider the temporal autocorrelation or spacial correlation, please refer to the next chapter.

FH estimation procedure: Step 1 Compute the selected direct area estimators Y, and estimators of their corresponding sampling variances (perhaps previously smoothed)

Step 2 Select the aggregated auxiliary variables at the area level for the FH model. A simple approach is to perform a model selection procedure in a linear regression model without the area effects, e.g., using stepwise selection, exhaustive search or LASSO.

Step 3 Fit the model and check the FH model assumptions, including linearity, normality of predicted area effects and standardized model residuals, and the presence of outlying areas.

Step 4 In case of clear systematic model departures, the model should be changed. In case of isolated departures because of outlying areas, either do not obtain the FH estimate for those areas or change some aspect of the model or the data. Then, repeat Steps 2 and 3.

Step 5 If the model assumptions hold, using the above direct estimates, estimated sampling variances and the selected auxiliary variables, compute the FH estimators and their estimated MSEs.

## Data and variables preparation

The example code in this guideline uses the Ghana data from the [summer school](https://github.com/pcorralrodas/wb_sae_training/tree/SummerU_2024).

### Load required libraries

The following is the list of libraries required to run the codes. If it is the first time to use the library, first you need to install the packages.

```{r libraries, message = FALSE, warning = FALSE}
library(readstata13)      # For reading .dta files
library(dplyr)      # For data manipulation
library(sae)       # For SAE modeling
library(car)        # For VIF calculation
library(ggplot2)    # For histogram and Q-Q plots
library(fastDummies) # Create dummy variables
library(stringr) # Wrappers for string operators
library(emdi) # For SAE modeling
```

### Load the dataset

First, we need to download the dataset "FHcensus_district.dta" which contains the list of aggregated auxiliary variables.

```{r load_data, message = FALSE, warning = FALSE}

data_agg <- read.dta13("https://github.com/pcorralrodas/wb_sae_training/raw/refs/heads/SummerU_2024/00.Data/input/FHcensus_district.dta")

```

### Define the variable list

The following is the list of candidate auxiliary variables we use for this example.

```{r list_variable, message = FALSE, warning = FALSE}
vars <- c("male", "head_age", "age", "depratio", "head_ghanaian", "ghanaian",
          "head_ethnicity1", "head_ethnicity2", "head_ethnicity3", "head_ethnicity4",
          "head_ethnicity5", "head_ethnicity6", "head_ethnicity7", "head_ethnicity8",
          "head_ethnicity9", "head_birthplace1", "head_birthplace2", "head_birthplace3",
          "head_birthplace4", "head_birthplace5", "head_birthplace6", "head_birthplace7",
          #"head_birthplace8", 
          "head_birthplace9", "head_birthplace10", "head_birthplace11",
          "head_religion1", "head_religion2", "head_religion3", "head_religion4", "christian",
          "married", "noschooling", "head_schlvl1", "head_schlvl2", "head_schlvl4", "head_schlvl5",
          "employed", "head_empstatus1", "head_empstatus2", "head_empstatus3", "head_empstatus4",
          "head_empstatus5", "head_empstatus6", "head_empstatus8", "head_empstatus9", "employee",
          "internetuse", "fixedphone", "pc", "aghouse", "conventional", "wall2", "wall3", "floor2",
          "floor3", "roof1", "roof2", "roof3", "tenure1", "tenure3", "rooms", "bedrooms", "lighting2",
          "lighting3", "lighting4", "water_drinking1", "water_drinking3", "water_drinking4",
          "water_general2", "water_general3", "fuel1", "fuel2", "fuel3", "toilet1", "toilet3",
          "toilet4", "toilet5", "solidwaste1", "solidwaste2", "solidwaste3")
```

### Normalize variables and create D variable

First we normalize all the auxiliary variables, and then create the area ID, which in this case is the combination of region and district. Then, create the regional dummies to be added as auxiliary variables.

```{r normalize_variable, message = FALSE, warning = FALSE}
# Normalize the auxiliary variables and create area ID
data_agg <- data_agg %>%
  mutate(across(all_of(vars), ~ ifelse(!is.na(.), (.-mean(.))/sd(.), NA))) %>%
  mutate(D = region * 100 + district) %>% 
  dplyr::select(-district) %>%
  rename(district = D)

# Add regional dummies
data_agg <- dummy_cols(data_agg, select_columns = "region")
data_agg <- data_agg %>% rename_with(~str_replace(.,"region_",replacement = "thereg"),starts_with("region_")) 
hhvars <- c(vars, "thereg1", "thereg2", "thereg3", "thereg4", "thereg5","thereg6", "thereg7", "thereg8", "thereg9") #, "thereg10")
```

### Merge with direct estimates and create region indicators

The direct estimate of Y is already prepared in the seperate dataset "direct_glss7.dta" (see the previous chapter for the details). Now, we merge this data of direct estimate with the auxiliary variables.

```{r merge, message = FALSE, warning = FALSE}
direct_data <- read.dta13("https://github.com/pcorralrodas/wb_sae_training/raw/refs/heads/SummerU_2024/00.Data/direct_glss7.dta")

# Combine input data
combined_data <- combine_data(
  pop_data = data_agg, pop_domains = "district",
  smp_data = direct_data, smp_domains = "district"
)
```

### Variance smoothing

```{r variance_smoothing, message = FALSE, warning = FALSE}
combined_data <- combined_data %>%
  mutate(
    log_s2 = log(dir_fgt0_var),
    logN = log(N),
    logN2 = logN^2,
    logpop = log(pop),
    logpop2 = logpop^2,
    accra = as.numeric(region.x == 3),
    share = log(N_hhsize / pop)
  )

fit <- lm(log_s2 ~ share, data = combined_data)
summary(fit)
phi2 <- summary(fit)$sigma^2
combined_data$xb_fh <- predict(fit, combined_data)

combined_data <- combined_data %>%
  mutate(
    exp_xb_fh = exp(xb_fh),
    smoothed_var = exp_xb_fh * (sum(dir_fgt0_var, na.rm = TRUE) / sum(exp_xb_fh, na.rm = TRUE)),
    dir_fgt0_var = ifelse(((num_ea > 1 & !is.na(num_ea)) | (num_ea == 1 & zero != 0 & zero != 1)) & is.na(dir_fgt0_var),
                                   smoothed_var, dir_fgt0_var),
             dir_fgt0 = ifelse(!is.na(dir_fgt0_var), zero, dir_fgt0)
  )
```

## Fay-Herriot SAE modeling

### Model preparation

FH does not run if there is any missing value in the dataset, and therefore, any variable with missing value should be removed in advance.

```{r fh_model, message = FALSE, warning = FALSE}
form <- as.formula(paste("dir_fgt0 ~",
                         hhvars %>% paste(collapse = " + ")))

rowsNAcovarites <- rowSums(sapply(combined_data[, all.vars(form)[-1]], is.na))
combined_data <- combined_data[rowsNAcovarites == 0, ]
```

### Check the perfect multicollinearity

With eblupFH, the model does not run if there is perfect multicollinearity between variables, so we need to omit them in advance. The function alias() checks the multicollinearity. According to the result, head_birthplace is causing the multicollinearity, and therefore, we omit "head_birthplace8".

```{r check_multicollinearity, message = FALSE, warning = FALSE}
alias(form, data = combined_data)
hhvars2 <- setdiff(hhvars, "head_birthplace8")

cor_data <- cor(na.omit(combined_data[, hhvars]))

apply(cor_data, MARGIN = 2, FUN = function(x) which(abs(x) > 0.8))
```

## Model creation and variable selection

In this example, we use the function fh to calculate the FH estimates. Because we want to estimate a ratio, we need to apply the arcsin transformation to guarantee that the results lie between 0 and 1. For that, we choose "arcsin" as transformation, and a a bias-corrected backtransformation ("bc"). Additionally, the effective sample size is needed for the arcsin transformation.

```{r fh_model2, message = FALSE, warning = FALSE}
fh_model <- fh(fixed = form, vardir = "dir_fgt0_var", 
               combined_data = combined_data, domains = "district", 
               method = "ml",
               transformation = "arcsin", backtransformation = "bc",
               eff_smpsize = "N"
               )
```

### Variable selection

Here, we perform a variable selection based on the AIC criterion and directly get the model with fewer variables.

```{r fh_model_refinement1, message = FALSE, warning = FALSE}
fh_model <- step(fh_model, criteria = "AIC")
```

### Function to remove the variables with high VIF

Next, we omit the variables with strong multicolliearity. For that, we create the function stepwise_vif.

```{r vif_function, message = FALSE, warning = FALSE}
colnames(combined_data)
combined_data[, hhvars]



stepwise_vif <- function(data, predictors, weight, threshold) {
  
  # Select predictor variables 
  dir_fgt0 <- data3$dir_fgt0
  x_data <- data3[, hhvars2]
  formula <- as.formula(paste("dir_fgt0 ~", paste(hhvars2, collapse = "+")))
  # Function to calculate VIF for a set of predictors
  vif_values <- vif(lm(formula, weights = rep(1,nrow(data3)), data = x_data))
  
  # Stepwise elimination
  while (max(vif_values, na.rm = TRUE) > threshold) {
    # Find the variable with the highest VIF
    max_vif_var <- names(which.max(vif_values))
    
    # Remove that variable from the dataset
    predictors <- setdiff(predictors, max_vif_var)
    x_data <- data3[, predictors]
    
    # Recalculate VIF with the updated variable set
    formula <- as.formula(paste("dir_fgt0 ~", paste(predictors, collapse = "+")))
    vif_values <- vif(lm(formula, weights = rep(1,nrow(data3)), data = x_data))
  }
  
  return(predictors)  # Return the final set of predictors with VIF â‰¤ threshold
}
```

### Apply the function to remove the variables with high multicollinearity

```{r remove_vif, message = FALSE, warning = FALSE}
hhvars3 <- stepwise_vif(data3, hhvars2, weight = rep(1,nrow(data3)), threshold = 5)
print(hhvars3)
```

### Comparison of the different FH methods

The commonly used variance estimation methods (e.g. ml/reml) may produce negative variance estimates that are supposed to be strictly positive. Negative variance estimates are set to zero resulting in zero estimates of the shrinkage factor. Therefore no weight is put on the direct estimator ignoring its possible reliability. This poses a problem especially when the number of areas is small. Check the variance estimates of the model and use one of the adjusted variance estimation methods of package emdi (e.g. ampl_yl) in case the variance equals zero.

```{r fh_model_comparison, message = FALSE, warning = FALSE}
fh_model$model$variance

if (fh_model$model$variance == 0){
  fh_model$call$method <- "ampl_yl"
  fh_model <- eval(fh_model$call)
}

```

### Obtain SAE-FH estimates

```{r fh_model_final, message = FALSE, warning = FALSE}
# Summary statistics of EBLUP estimates
summary(fh_model$ind$FH)

# Summary statistics of MSE estimates
summary(fh_model$ind$FH)
```

## Assessment of the model

The summary method of emdi provides additional information about the data and model components,
in particular the chosen estimation methods, the number of domains, the log-likelihood, the information
criteria by Marhuenda et al. (2014), the adjusted R2 of a standard linear model and the adjusted R2
especially for FH models proposed by Lahiri & Suntornchost (2015). Additionally, measures to validate
model assumptions about the standardized realized residuals and the random effects are provided: skewness and kurtosis (skewness and kurtosis of package moments, Komsta and Novomestky, 2015) of the
standardized realized residuals and the random effects and the test statistics with corresponding p value
of the Shapiro-Wilks-test for normality of both error terms

```{r summary, message = FALSE, warning = FALSE}
summary(fh_model)
```

### Diagnostic plots

We produce normal quantile-quantile (Q-Q) plots of the standardized realized residuals and random effects and plots of the kernel densities of the distribution of both error terms.

```{r plot, message = FALSE, warning = FALSE}
plot(fh_model)
```

### Compare the FH to the direct results 

We produce a scatter plot proposed by Brown et al. (2001) and a line plot. Besides the direct and FH estimates, the plot contains the fitted regression and the identity line. Both lines should not differ too much. Preferably, the model-based (FH) estimates should track the direct estimates within the line plot especially for domains with a large sample size/small MSE of the direct estimator. The input arguments MSE and CV of compare_plot can be set to TRUE leading to two extra plots. The MSE/CV estimates of the direct and model-based (FH) estimates are compared via boxplots and ordered scatter plots.

The function compare enables the user to compute a goodness of fit diagnostic (Brown et al.,
2001) and a correlation coefficient of the direct estimates and the estimates of the regression-synthetic part of the FH model (Chandra et al., 2015). Following Brown et al. (2001), the difference between the model-based estimates and the direct estimates should not be significant (null hypothesis).

```{r compare, message = FALSE, warning = FALSE}
compare_plot(fh_model, CV = TRUE)
compare(fh_model)

```

### Visualize the results

```{r visualize, message = FALSE, warning = FALSE}
# map

```

## Save the final results

### Save and output Final dataset

```{r save, message = FALSE, warning = FALSE}
write.excel(fh_model,
  file = "fh_model_output.xlsx",
  MSE = TRUE, CV = TRUE
)
```
