---
title: "Multivariate Fay Herriot Modelling"
author: "Ifeanyi Edochie"
date: "2025-06-04"
output: html_document
---

# Stable FH Estimators over T time periods: The Multivariate Fay Herriot Modelling Approach

This section describes procedures that yield stable small area estimators for each of $D$ areas over $T$ subsequent time instants. Area populations, the samples and the data might change between time periods. Accordingly, we denote $U_t$ the overall population at time $t$, which is partitioned into $D$ areas $U_{1t}, ... ,U_{Dt}$, of respective population sizes $N_{1t}$

An overview of the MFH model estimation process is as follows:

-   Step 1: Compute the selected direct area estimators for each target area $d = 1, ..., D$ for each time $t = 1, ..., T$ and estimators of their corresponding sampling variances and covariances.

-   Step 2: Prepare variables (from administrative data or other sources of data representative at the target area level) at the level of the target area for each time instant in the MFH model. We present a simple approach which performs model selection in a pooled linear regression model without time effects.

-   Step 3: Fit the MFH models to test for homoskedastic area-time effects $({\mu_d}_1, ... , {\mu_d}_T)$ are homoskedastic or not. If we reject the homoskedasticity of variances, implement the MFH3 model. Otherwise, we proceed with the MFH2 model. For the purposes of this training, we assume a homoskedastic model for simplicity. 

-   Step 4: Check the selected model assumptions, including linearity, normality of predicted area effects and standardized model residuals, and the presence of the outlying areas.

-   Step 5: In case of systematic model departures such as isolated departures because of outlying areas, some adjustments might need to be implemented before returning to Step 2 to recompute the MFH model.

-   Step 6: If model assumptions hold, using the above direct estimates and estimated sampling variances and covariances, and the selected auxiliary variables, compute MFH estimators $\hat{\delta}_{dt}^{MFH},\quad d = 1,...,D$ and $t = 1, ..., T$ and their corresponding estimated MSEs.

We will show below the use of the `eblupMFH2()` and `eblupMFH3()` from the R package msae [@permatasari2022package] compute the EBLUPs and their MSE estimates under the MFH models 2 and 3, respectively. The calls to these functions are:

`eblupMFH2(formula, vardir, MAXITER = 100, PRECISION = 1e-04, data)`

`eblupMFH3(formula, vardir, MAXITER = 100, PRECISION = 1e-04, data)`

## MFH Estimation of Poverty Rates for T time periods

In this example, we use a synthetic data set adapted from R package `sae` called `incomedata`. The original data contains information for $n = 17,119$ fictitious individuals residing across $D = 52$ Spanish provinces. The variables include the name of the province of residence (`provlab`), province code (`prov`), as well as several correlates of income. We have added two additional income vectors corresponding to two additional years of data.

We will show how to estimate a poverty map for each year by using the Multivariate Fay Herriot modelling approach. This approach allows us to take advantage of the temporal correlation between poverty rates i.e. an individuals income in year $t$ is likely correlated with their income in year $t+1$.

The rest of this tutorial shows how to prepare MFH models using a random 10% sample of the `incomedata` to estimate the poverty rates.

```{r load-data, message = FALSE, warning = FALSE}

if (sum(installed.packages()[,1] %in% "pacman") != 1){
  
  install.packages("pacman")
  
}

pacman::p_load(sf, data.table, tidyverse, car, msae, 
               sae, survey, spdep, knitr, MASS, caret,
               purrr, here)

source(here::here("scripts/Function_pbmcpeMFH2_test.R"))

income_dt <- readRDS(here::here("data/incomedata_survey.RDS"))

prov_dt <- readRDS(here::here("data/shapes/simadmin.RDS"))

shp_dt <- readRDS(here::here("data/shapes/spainshape.RDS"))

glimpse(income_dt)


```

### Step 1: Direct Estimation

We will use the direct HT estimators that use the survey weights in `weight` variable. First, we calculate the total sample size, the number of provinces, the sample sizes for each province and extract the population sizes for each province/target area from the `sizeprov` file. For those using the household/individual level survey data, this may be obtained from the sum of the household or individual weights as appropriate.

```{r}

data(sizeprov)

## quickly compute sample size for each province
sampsize_dt <- 
income_dt |>
  group_by(prov) |>
  summarize(N = n())

## the poverty line for each is already included within the data. 
## Lets compute the direct estimates for each year of data and create a list of data.frames (equal in length to the number of years) 
## containing the direct estimate, the standard errors and the coefficient of variation. 

direct_list <- 
  mapply(FUN = function(y, threshold){
    
     z <- emdi::direct(y = y,
                       smp_data = income_dt %>% as.data.table(),
                       smp_domains = "prov",
                       weights = "weight",
                       threshold = unique(income_dt[[threshold]]),
                       var = TRUE)
     
     z <- 
       z$ind |>
       dplyr::select(Domain, Head_Count) |>
       rename(Direct = "Head_Count") |>
       merge(z$MSE |>
               dplyr::select(Domain, Head_Count) |>
               rename(MSE = "Head_Count"),
             by = "Domain") |>
       mutate(SD = sqrt(MSE)) |>
       mutate(CV = SD / Direct) |>
       merge(sampsize_dt |> 
               mutate(prov = as.factor(prov)), 
             by.x = "Domain", 
             by.y = "prov") |>
       mutate(var_SRS = Direct * (1 - Direct) / N) |>
       mutate(deff = MSE / var_SRS) |>
       mutate(n_eff = N / deff)
       
      
     return(z)

  }, SIMPLIFY = FALSE,
  y = c("income2012", "income2013", "income2014"),
  threshold = c("povline2012", "povline2013", "povline2014"))


direct_list %>%
  lapply(X = .,
         FUN = function(x){
           
           y <- head(x)
           
           y %>% kable()
           
         })

```

#### Estimating the Variance-Covariance Matrix of the Sampling Error Distribution

Next, we quickly estimate the sample variance and covariance for the direct estimator using the survey R package as follows:

```{r}

## quickly creating the poverty indicator
income_dt <- 
  income_dt |>
  mutate(poor2012 = ifelse(income2012 < povline2012, 1, 0),
         poor2013 = ifelse(income2013 < povline2013, 1, 0),
         poor2014 = ifelse(income2014 < povline2014, 1, 0))


### a simple worker function for computing the variance covariance matrix of the sampling error of the direct estimate

compute_vcov <- function(dt,
                         domain,
                         ids,
                         weights = NULL,
                         fpc = NULL,
                         strata = NULL,
                         probs = NULL,
                         yvars,
                         deff = FALSE,
                         ...){
  
  ## a little worker function to convert expressions that are character to the right format for the survey package
  convert_expr <- function(x){
    
    y <- if (is.character(x)){
      
      rlang::expr(~!!sym(x))
      
    } else {
      
      rlang::expr(~ !!x)
      
    }
    
    y <- eval(y)
    
    return(y)
    
  }
  
  ### run it on the proper arguments
  ids <- convert_expr(x = ids)
  
  if (is.null(weights)){
    
    dt$weights <- 1
    
  }
  
  weights <- convert_expr(x = weights)
  
  if (!is.null(fpc)){
    
    fpc <- convert_expr(x = fpc)

  }
    
  if (!is.null(probs)){
    
    probs <- convert_expr(x = probs)

  }

  if (!is.null(strata)){
    
    strata <- convert_expr(x = strata)

  }

  ### create a survey design object from the survey package 
  
  dom_list <- unique(dt[[domain]])
  
  surv_vcov <- function(x){
    
    design_obj <- svydesign(ids = ids,
                            probs = probs,
                            strata = strata,
                            fpc = fpc,
                            data = dt |>
                              dplyr::filter(!!sym(domain) == x),
                            weights = weights)
    
    ### prepare the y formula for the svymean function
    yform <- reformulate(yvars)
    
    var_dt <- svymean(yform, design_obj, ...) ## use the svymean object to compute variance
    
    var_dt <- vcov(var_dt) ### compute variance covariance matrix
    
    var_dt <- as.numeric(c(diag(var_dt), var_dt[lower.tri(var_dt, diag = FALSE)]))
    
    pair_list <- c(lapply(yvars, rep, 2),
                   combn(yvars, 2, simplify = FALSE))
  
    
    pair_list <- lapply(pair_list,
                        function(x){
                          
                          zz <- paste0("v_", paste(x, collapse = ""))
                          
                          return(zz)
                          
                        })
    
    pair_list <- unlist(pair_list)
    
    names(var_dt) <- pair_list
    
    return(var_dt)

  }
  
  vcov_list <- lapply(X = dom_list,
                      FUN = surv_vcov)
  
  var_dt <- Reduce(f = "rbind",
                   x = vcov_list) %>%
    as_tibble() %>%
    mutate(domain = dom_list)
  
  return(var_dt)  
  
}

### running the compute_vcov function to prepare the variance covariance matrix 
var_dt <- 
compute_vcov(dt = income_dt, 
             domain = "prov",
             ids = 1, 
             weights = "weight", 
             yvars = c("poor2012", "poor2013", "poor2014"))

### sample size
var_dt <- 
  var_dt |> 
  merge(sampsize_dt, by.x = "domain", by.y = "prov")





```

#### Handling Low Sample Size: Variance Smoothing

A quick inspection of the preceding results will show some provinces contain low sample sizes which sometimes result in extreme value poverty rates and hence 0 variance. To avoid this, we will show you how to apply the variance smoothing method suggested by [@you2023application]. Please see the code and Roxygen comments below explaining the use of the `varsmoothie_king()` function which computes smoothed variances.

```{r}

#' A function to perform variance smoothing
#' 
#' The variance smoothing function applies the methodology of (Hiridoglou and You, 2023)
#' which uses simply log linear regression to estimate direct variances for sample 
#' poverty rates which is useful for replacing poverty rates in areas with low sampling
#' 
#' @param domain a vector of unique domain/target areas
#' @param direct_var the raw variances estimated from sample data e=
#' @param sampsize the sample size for each domain
#' @param y indicator variable at the survey sample level for each household/individual/unit i.e. poor = 1, non-poor = 0
#' 
#' @export

varsmoothie_king <- function(domain,
                             direct_var,
                             sampsize,
                             y){

  dt <- data.table(domain = domain,
                   var = direct_var,
                   n = sampsize)

  dt$log_n <- log(dt$n)
  dt$log_var <- log(dt$var)

  lm_model <- lm(formula = log_var ~ log_n,
                 data = dt[!(abs(dt$log_var) == Inf),])

  dt$pred_var <- predict(lm_model, newdata = dt)

  residual_var <-  summary(lm_model)$sigma^2
  
  dt$var_smooth <- exp(dt$pred_var) * exp(residual_var/2)

  return(dt[, c("domain", "var_smooth"), with = F])

}


```

OK, the goal now is to use the above `varsmoothie_king()` function to add an additional column of smoothed variances into our `direct_list` object.

```{r}

vardir <- grep("^v_", names(var_dt), value = TRUE)

var_dt <-
  lapply(X = vardir,
         FUN = function(x){

           z <- varsmoothie_king(domain = var_dt[["domain"]],
                                 direct_var = var_dt[[x]],
                                 sampsize = var_dt[["N"]]) |>
             as.data.table() |>
             setnames(old = "var_smooth", new = paste0("vs", x)) |>
             as_tibble()
           

           return(z)

         }) %>%
  Reduce(f = "merge",
         x = .) %>%
  merge(x = var_dt,
        y = .,
        by = "domain") |>
  as_tibble()

```

-   Now, you can replace the zero/near zero sample size area MSEs with their smoothed variances.

### Step 2: Model Selection

More (and probably better correlates of income and poverty) can be created to improve the predictive power of the model. The next step is to take target level variables. The MFH model is a model of poverty rates at the target area level, hence the data format required for this exercise has the province as its unit of observation. This format has a few essential columns:

-   (1) Variables for poverty rates for each time period

-   (2) The set of candidate variables from which the most predicted of poverty rates will be selected

-   (3) The target area variable identifier (i.e. in this case the province variable `prov` and `provlab`)


#### Model Selection

Next, we apply a simple variable selection process which employs the stepwise regression algorithm using the AIC selection criteria as in described by [@yamashita2007stepwise]. The function `step_wrapper()` implemented below is a wrapper to the `stepAIC()` function carries all the perfunctory cleaning necessary use the `stepAIC()` function. This includes dropping columns that are entirely missing (`NA`) and keep only complete cases/observations and remove perfectly or near collinear variables and combinations using the variance inflation method.

```{r}


#' A function to perform stepwise variable selection with AIC selection criteria
#' 
#' @param dt data.frame, dataset containing the set of outcome and independent variables
#' @param xvars character vector, the set of x variables
#' @param y chr, the name of the y variable
#' @param vif_threshold integer, a variance inflation factor threshold
#' 
#' @import data.table
#' @importFrom cars vif
#' @importFrom MASS stepAIC

step_wrapper <- function(dt, xvars, y, cor_thresh = 0.95) {
  
  dt <- as.data.table(dt)
  
  # Drop columns that are entirely NA
  dt <- dt[, which(unlist(lapply(dt, function(x) !all(is.na(x))))), with = FALSE]
  
  xvars <- xvars[xvars %in% colnames(dt)]
  
  # Keep only complete cases
  dt <- na.omit(dt[, c(y, xvars), with = FALSE])
  
  # Step 1: Remove aliased (perfectly collinear) variables
  model_formula <- as.formula(paste(y, "~", paste(xvars, collapse = " + ")))
  lm_model <- lm(model_formula, data = dt)
  aliased <- is.na(coef(lm_model))
  if (any(aliased)) {
    xvars <- names(aliased)[!aliased & names(aliased) != "(Intercept)"]
  }
  
  # Step 2: Remove near-linear combinations
  xmat <- as.matrix(dt[, ..xvars])
  combo_check <- tryCatch(findLinearCombos(xmat), error = function(e) NULL)
  if (!is.null(combo_check) && length(combo_check$remove) > 0) {
    xvars <- xvars[-combo_check$remove]
    xmat <- as.matrix(dt[, ..xvars])
  }
  
  # Step 3: Drop highly correlated variables
  cor_mat <- abs(cor(xmat))
  diag(cor_mat) <- 0
  while (any(cor_mat > cor_thresh, na.rm = TRUE)) {
    cor_pairs <- which(cor_mat == max(cor_mat, na.rm = TRUE), arr.ind = TRUE)[1, ]
    var1 <- colnames(cor_mat)[cor_pairs[1]]
    var2 <- colnames(cor_mat)[cor_pairs[2]]
    # Drop the variable with higher mean correlation
    drop_var <- if (mean(cor_mat[var1, ]) > mean(cor_mat[var2, ])) var1 else var2
    xvars <- setdiff(xvars, drop_var)
    xmat <- as.matrix(dt[, ..xvars])
    cor_mat <- abs(cor(xmat))
    diag(cor_mat) <- 0
  }
  
  # Step 4: Warn if still ill-conditioned
  cond_number <- kappa(xmat, exact = TRUE)
  if (cond_number > 1e10) {
    warning("Design matrix is ill-conditioned (condition number > 1e10). Consider reviewing variable selection.")
  }
  
  # Final model fit
  model_formula <- as.formula(paste(y, "~", paste(xvars, collapse = " + ")))
  full_model <- lm(model_formula, data = dt)
  
  # Stepwise selection
  stepwise_model <- stepAIC(full_model, direction = "both", trace = 0)
  
  return(stepwise_model)
  
}


```

We apply the variable selection process for the selection of each time period model. See the application below:

```{r, warning = FALSE, message = FALSE}

### the set of outcome variables
candidate_vars <- colnames(prov_dt)[!colnames(prov_dt) %in% c("prov", "provlab")]


### creating the province level data for variable selection and poverty mapping
prov_dt <- 
  map2(.x = list("poor2012", "poor2013", "poor2014"), ### first we create a direct estimate dataset from direct_list
       .y = direct_list,
       ~ .y |> 
         rename(!!sym(.x) := Direct) |>
         dplyr::select(Domain, all_of(.x))) |>
  Reduce(f = "merge") |> ### merge the selected variables from each dataframe of the list
  merge(prov_dt, ### merging the prepared simulated administrative data variables
        by.x = "Domain", by.y = "prov", all = TRUE) |>
  merge(var_dt,
        by.x = "Domain", by.y = "domain", all = TRUE)
  
fh_step <- 
step_wrapper(dt = prov_dt,
             xvars = candidate_vars,
             y = "poor2012",
             cor_thresh = 0.7)

fh_step <- names(fh_step$coefficients)[!grepl("(Intercept)", names(fh_step$coefficients))]

fh_step <- as.formula(paste0("poor2012 ~ ",  paste(fh_step, collapse = " + ")))

rhs <- paste(deparse(fh_step[[3]]), collapse = " ")


outcome_list <- list("poor2012", "poor2013", "poor2014")
# Then generate the list of formulas
mfh_formula <- 
  lapply(outcome_list, 
         function(x) {
           
            as.formula(paste(x, "~", rhs)) 
           
           })

names(mfh_formula) <- outcome_list


```

Let's run some linear regressions for set of selected variables to get a sense for the predicted power of the models independently

```{r}

lmcheck_obj <- 
lapply(X = mfh_formula,
       FUN = lm,
       data = prov_dt)


lapply(X = lmcheck_obj,
       FUN = summary)



```

### Step 3: Model Estimations

Next, we show how to use the `msae` R package to estimate the Empirical Best Linear Unbiased Predictor (EBLUP) for the poverty map using the `eblupMFH2()` which allow for time series fay herriot estimation under homoskedastic assumptions. For completeness, we also briefly perform the previous described direct estimation in step 1, using the `eblupUFH()` function as well as the `eblupMFH1()` for the fay herriot model. 

```{r}

### first lets add the sampling variance and covariance matrix to the prov_dt dataset

# prov_dt <- 
#   prov_dt |>
#   merge(var_dt, by.x = "prov", by.y = "domain")

### variance-covariance matrix columns
varcols <- colnames(var_dt)[grepl(pattern = "^v_", x = colnames(var_dt))] 

## replace the variances-covariances that are zero with their smoothed counterparts
prov_dt <- 
  prov_dt |>
  mutate(across(
    starts_with("v_"),
    ~ if_else(abs(.x) <= 1e-4, get(paste0("vsv", str_remove(cur_column(), "^v"))), .x),
    .names = "{.col}"
  ))



#### now we estimate all 4 models including the multivariate fay herriot models

model0_obj <- eblupUFH(mfh_formula, vardir = varcols, data = prov_dt)
model1_obj <- eblupMFH1(mfh_formula, vardir = varcols, data = prov_dt, MAXITER = 1e10, PRECISION = 1e-2)
model2_obj <- eblupMFH2(mfh_formula, vardir = varcols, data = prov_dt, MAXITER = 1e10, PRECISION = 1e-2)
# model3_obj <- eblupMFH3(mfh_formula, vardir = varcols, data = prov_dt, MAXITER = 1e10, PRECISION = 1e-2)

```

With the MFH3 estimation, we can check if the variance-covariance matrix structure points to heteroskedastic random effects. We do so as follows:

```{r}

# model3_obj$fit$refvarTest

```

This tests the null hypothesis that the variances $\sigma_t^2$ at each pair of time instants $t$ and $s$ are equal against the alternative that they are not. In this case, at significance level of 0.05, we reject the equality of variances between $t = 2013$ and $t = 2014$. Hence we can use the MFH3 model (`eblupMFH3()` function). If these tests supported the equality of variances, then we would use instead the function `eblupMFH2` for estimation.

### Step 4: Post Estimation Diagnostics: Model Assumption Checks for Linearity, Normality and Outliers

We now verify the assumptions of the MFH3 model. This includes assessing linearity, the normality of the predicted area effects and standardized residuals, as well as checking for the presence of outlying areas.

#### The Check for Linearity

We first check the linearity assumption. This can be addressed by regressing the model residuals against the predicted values (EBLUPs).

```{r}

resids_3 <- cbind(prov_dt$poor2012 - model2_obj$eblup$poor2012,
                  prov_dt$poor2013 - model2_obj$eblup$poor2013,
                  prov_dt$poor2014 - model2_obj$eblup$poor2014)

layout(matrix(1:3, nrow = 1, byrow = TRUE))

plot(model2_obj$eblup$poor2012, resids_3[,1], pch = 19, xlab = "EBLUPs t = 1", ylab = "Residuals t = 1")
plot(model2_obj$eblup$poor2013, resids_3[,2], pch = 19, xlab = "EBLUPs t = 2", ylab = "Residuals t = 2")
plot(model2_obj$eblup$poor2014, resids_3[,3], pch = 19, xlab = "EBLUPs t = 3", ylab = "Residuals t = 3")

```

```{r}

fits_3 <- model2_obj$eblup  # EBLUPs (predicted values)

# Run regression of residuals on fitted values for each time period
lm_resid_fit_t1 <- lm(resids_3[,1] ~ model2_obj$eblup$poor2012 + model2_obj$eblup$poor2012^2)
lm_resid_fit_t2 <- lm(resids_3[,2] ~ model2_obj$eblup$poor2013 + model2_obj$eblup$poor2013^2)
lm_resid_fit_t3 <- lm(resids_3[,3] ~ model2_obj$eblup$poor2014 + model2_obj$eblup$poor2014^2)

# View summaries
summary(lm_resid_fit_t1)
summary(lm_resid_fit_t2)
summary(lm_resid_fit_t3)




```

### Evaluating the Normality Assumption

#### The Shapiro Wilks Test

We use the shapiro wilks test of normality using the `shapiro.test()` function in base R. The Shapiro-Wilk test assesses whether a sample of data is drawn from a normally distributed population. It does so by comparing the order statistics (i.e., sorted values) of the sample to the expected values under a normal distribution. Specifically, the test statistic $W$ is a ratio of the squared correlation between the observed sample quantiles and the corresponding normal quantiles.

First, we perform the shapiro wilks normality test on the model errors, $\varepsilon$. We show both the normality distribution histogram as well as the qqplots as below:

```{r}

### evaluating the normality assumption
resid_dt <- prov_dt[,c("poor2012", "poor2013", "poor2014")] - model2_obj$eblup

### perform the shapiro test

shapiro_obj <- apply(resid_dt, 2, shapiro.test)

summary_dt <- 
  data.frame(Time = names(shapiro_obj),
             W = lapply(X = shapiro_obj,
                        FUN = function(x){
                          
                          return(x$statistic[[1]])
                          
                        }) %>%
               as.numeric(),
             p_value = lapply(X = shapiro_obj,
                              FUN = function(x){
                                
                                return(x$p.value)
                                
                              }) %>%
               as.numeric())

### plot the results
summary_dt <- 
  summary_dt %>%
  mutate(label = paste0("W = ", round(W, 3), "\n", "p = ", signif(p_value, 3)))

resid_dt %>%
  pivot_longer(cols = everything(), 
               names_to = "Time", 
               values_to = "Residual") %>%
  ggplot(aes(x = Residual)) + 
  geom_histogram(bins = 10, fill = "steelblue", color = "white") + 
  geom_text(data = summary_dt, aes(x = -Inf, y = Inf, label = label),
            hjust = -0.1, vjust = 1.2, inherit.aes = FALSE, size = 3.5) +
  facet_wrap(~Time, scales = "free") + 
  theme_minimal() + 
  labs(title = "Residual Histograms by Time Period")


### here's how to create qqplots
resid_dt %>%
  pivot_longer(cols = everything(),
               names_to = "Time",
               values_to = "Residual") %>%
  ggplot(aes(sample = Residual)) +
  stat_qq() +
  stat_qq_line() +
  facet_wrap(~Time, scales = "free") +
  theme_minimal() +
  labs(title = "QQ Plots of Residuals by Time Period")

```

Likewise, we test the normality of the random effect variable

```{r}


#### For the random effects
raneff_dt <- as.data.frame(model2_obj$randomEffect)

### lets run the shapiro wilks tests again
shapiro_obj <- apply(raneff_dt, 2, shapiro.test)


summary_dt <- 
  data.frame(Time = names(shapiro_obj),
             W = lapply(X = shapiro_obj,
                        FUN = function(x){
                          
                          return(x$statistic[[1]])
                          
                        }) %>%
               as.numeric(),
             p_value = lapply(X = shapiro_obj,
                              FUN = function(x){
                                
                                return(x$p.value)
                                
                              }) %>%
               as.numeric())

### plot the results
summary_dt <- 
  summary_dt %>%
  mutate(label = paste0("W = ", round(W, 3), "\n", "p = ", signif(p_value, 3)))

raneff_dt %>%
  pivot_longer(cols = everything(), 
               names_to = "Time", 
               values_to = "RandEff") %>%
  ggplot(aes(x = RandEff)) + 
  geom_histogram(bins = 10, fill = "darkorange", color = "white") + 
  geom_text(data = summary_dt, aes(x = -Inf, y = Inf, label = label),
            hjust = -0.1, vjust = 1.2, inherit.aes = FALSE, size = 3.5) +
  facet_wrap(~Time, scales = "free") + 
  theme_minimal() + 
  labs(title = "Randon Effects Histograms by Time Period")


```

In both cases, we compare the p-value to the 0.05 level of significance. The results suggest we cannot reject the null hypothesis of normally distributed model errors and random effects.

In some cases, the assumptions described by the MFH2 model are not met. Steps 5 and 6 recommend re-estimate the models by creating additional variables via transformation or variable interactions.

### Comparing Direct Estimation to Multivariate Model Outputs

```{r}

#### first let us compare the gains in precision

direct_list <- 
  mapply(x = direct_list,
         y = c(2012, 2013, 2014),
         FUN = function(x, y){
           
           x$year <- y
           
           return(x)
           
         }, SIMPLIFY = FALSE)

direct_dt <- Reduce("rbind", direct_list)


model2mse_dt <- 
  model2_obj$MSE |>
  mutate(Domain = 1:n()) |>
  pivot_longer(
    cols = starts_with("poor"),         # columns to pivot
    names_to = "year",                  # new column for the years
    values_to = "modelMSE"
  ) |>
  mutate(year = as.integer(substr(year, 5, 8)))

model2pov_dt <- 
  model2_obj$eblup |>
  mutate(Domain = 1:n()) |>
  pivot_longer(
    cols = starts_with("poor"),         # columns to pivot
    names_to = "year",                  # new column for the years
    values_to = "modelpov"
  ) |>
  mutate(year = as.integer(substr(year, 5, 8)))

model2pov_dt <- merge(model2mse_dt, model2pov_dt, by = c("Domain", "year"))


model2pov_dt <- 
  model2pov_dt |>
  mutate(modelCV = sqrt(modelMSE) / modelpov)


cv_dt <- merge(model2pov_dt, direct_dt, by = c("Domain", "year"))

cv_dt |>
  dplyr::select(Domain, year, CV, modelCV) |>
  pivot_longer(cols = c(CV, modelCV), names_to = "Type", values_to = "CV_value") |>
  ggplot(aes(x = factor(year), y = CV_value, color = Type, group = Type)) +
  geom_line(size = 1) +
  facet_wrap(~ Domain, scales = "free_y") +
  labs(title = "Comparison of Direct vs Model-based CVs",
       y = "Coefficient of Variation (CV)",
       x = "Year",
       color = "Estimation Type") +
  theme_minimal() + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))




```

### Did the poverty rates change over time? 

Next we can call the `pbmcpeMFH2()` function, which returns the EBLUP, as well as, the MSEs of the EBLUPs for each time point, and the MCPEs for each pair of time points based on the MFH model 2 as follows: 

```{r}

mcpemfh2_obj <- 
pbmcpeMFH2(formula = mfh_formula,
           vardir = varcols,
           nB = 50,
           data = prov_dt)


#' A function to compare and plot differences from the mcpe object
#' 

compare_mfh2 <- function(period_list = c(1,2),
                         mcpe_obj = mcpemfh2_obj,
                         alpha = 0.05){
  
  col_chr <- paste0("(", period_list[1], ",", period_list[2], ")")

  df <- 
    tibble(diff = mcpe_obj$eblup[,period_list[2]] - mcpe_obj$eblup[,period_list[1]],
           mse = mcpe_obj$mse[,period_list[1]] + mcpe_obj$mse[,period_list[2]] - 2*mcpe_obj$mcpe[,col_chr],
           alpha = rep(alpha, nrow(mcpe_obj$eblup)),
           zq = qnorm(alpha / 2, lower.tail = F)) |>
    mutate(lb = diff - zq * sqrt(mse),
           ub = diff + zq * sqrt(mse)) |>
    mutate(significant = ifelse(lb > 0 | ub < 0, 
                                "Significant", 
                                "Not Significant")) |>
    mutate(index = row_number())
  
  p1 <- 
  ggplot(df, aes(x = index, y = diff)) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "gray50") +
  geom_errorbar(aes(ymin = lb, ymax = ub, color = significant), width = 0.2, linewidth = 1) +
  geom_point(color = "black", size = 2) +
  scale_color_manual(values = c("Significant" = "red", "Not Significant" = "gray40")) +
  labs(
    x = "Area (Index)",
    y = "Difference between time periods",
    color = "Significance",
    title = paste0("Change in Poverty Rates based on MCPE between period ", 
                   period_list[1], 
                   " and ", 
                   period_list[2])
  ) +
  theme_minimal()
    
  
  return(list(df = df,
              plot = p1))
  
}


```

The above function takes the difference between any two time periods and prepares a table of differences for each area include the MCPE estimated error rates as well as lower and upper bounds given a specified confidence level. See the function implemented to compare periods 1 and 2 (years 2012 and 2013)

```{r}


comp12_obj <- compare_mfh2()

comp23_obj <- compare_mfh2(period_list = c(2, 3))

comp13_obj <- compare_mfh2(period_list = c(1, 3))

```

See the plots below

```{r}


comp12_obj$plot

comp23_obj$plot

comp13_obj$plot


```




See the data created 

```{r}

comp12_obj$df %>%
  head() %>%
  kable()

```

```{r}

comp23_obj$df %>%
  head() %>%
  kable()

```

```{r}

comp13_obj$df %>%
  head() %>%
  kable()


```



