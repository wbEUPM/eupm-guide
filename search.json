[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "EUPM Practitioners Guide",
    "section": "",
    "text": "Introduction\nTODO",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Small Area Estimations for the poverty mapping: an overview",
    "section": "",
    "text": "As discussed with Nobuo, Danielle and Eduard, place for an practitioners overview chapter.\nSome overview ideas:\n\nMake a decision tree of methodology given data availability.\nMake a table to take a stock of methods and Corresponding R funciton/packages that implement it. For example, see Table 1.1\n\n\n\n\nTable 1.1: Methods overview\n\n\n\n\n\n\n\n\n\n\n\nMethods\nsae\nemdi\nSUMMER\n\n\n\n\nSpatial Fay-Herriot\nsae::mseFH()\n\nSUMMER::smoothArea()\n\n\nFH Miltivariate\n\n\n\n\n\nAutocorrelation\n\n\n\n\n\n\n\n\n\nPlace figures under images and use them in the text as follows making sure to refer the sources correctly. For example, Figure 1.1 is adapted from (Corral et al. 2022, 5).\n\n\n\n\n\n\nFigure 1.1: SAE decision tree\n\n\n\nOne can also embed mathematical formulas following the latex syntax: \\(y=a + b \\log x\\) . For more information, see quarto help on authoring.\n\n\n\n\n\n\nCorral, Paul, Isabel Molina, Alexandru Cojocaru, and Sandra Segovia. 2022. Guidelines to Small Area Estimation for Poverty Mapping. World Bank Washington.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Small Area Estimations for the poverty mapping: an overview</span>"
    ]
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "2  Data preparation",
    "section": "",
    "text": "2.1 Workflow and reproducibility principals\nIntroduction: this chapter provides a template for organizing the data flow in the EUPM analysis.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data preparation</span>"
    ]
  },
  {
    "objectID": "data.html#workflow-and-reproducibility-principals",
    "href": "data.html#workflow-and-reproducibility-principals",
    "title": "2  Data preparation",
    "section": "",
    "text": "Brief overview of the data workflow between raw, auxiliary and clean data types.\nBasic principles of reproducibility.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data preparation</span>"
    ]
  },
  {
    "objectID": "data.html#geospatial-data",
    "href": "data.html#geospatial-data",
    "title": "2  Data preparation",
    "section": "2.2 Geospatial data",
    "text": "2.2 Geospatial data\n\nDescription of the basics of the GIS data preparation for countries at different admin levels. Key problems addressed.\nSpatial validity of polygons.\nNested geospatial structure and non-intersecting boundaries.\nPolygon -unique identifiers.\nGIS boundaries harmonization over time.\n\nSynthetic regions aggregation constant in time\n\nQuality assurance of the administrative boundaries.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data preparation</span>"
    ]
  },
  {
    "objectID": "data.html#raw-data-search-collection-and-documentation",
    "href": "data.html#raw-data-search-collection-and-documentation",
    "title": "2  Data preparation",
    "section": "2.3 Raw data search, collection, and documentation",
    "text": "2.3 Raw data search, collection, and documentation\nDescription of the process of data search, collection and documentation that yields with a systematized, but unstructured data library\n\nVariables search, and priority indicators.\nKey challenges and considerations for data inclusion:\n\nThematic relevance\nTime range available\nTerritorial unit available\nCoverage and completeness\n\nDetails on specific data sources:\n\nAPI-based data\nManually downloaded spreadsheets\nBulk downloads\nRemote sensing and GIS-based data, zonal statistcs, etc.\n\nPrincipals of data storage and systematization\nDocumenting collected data with metadata and it notes on search\n\nKey validation requirements – source type, survey type\n\n\nCountry-based examples: use one country as an example.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data preparation</span>"
    ]
  },
  {
    "objectID": "data.html#preparing-auxiliary-data",
    "href": "data.html#preparing-auxiliary-data",
    "title": "2  Data preparation",
    "section": "2.4 Preparing auxiliary data",
    "text": "2.4 Preparing auxiliary data\nAdding structure to the raw data by transforming it into a normalized data set with columns: id, year, variable, value.\n\ncreating and storing the auxiliary data\ndata reproduction and version-control\nprincipals of the data quality assurance and quality control\n\nCountry-based examples: use one country as an example.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data preparation</span>"
    ]
  },
  {
    "objectID": "data.html#clean-and-analysis-ready-data",
    "href": "data.html#clean-and-analysis-ready-data",
    "title": "2  Data preparation",
    "section": "2.5 Clean and analysis-ready data",
    "text": "2.5 Clean and analysis-ready data\nGetting meaningful and relevant indicators out of the data.\n\nReshaping auxiliary data into the analysis-ready dataset.\nComputing relevant indicators: means, ratios, fractions, etc.\n\nGroup-wise operations by year, and across regions.\n\nRegression-data quality assurance: spatial and temporal completeness\nAdding data important data from elsewhere: SILK poverty estimates",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data preparation</span>"
    ]
  },
  {
    "objectID": "data.html#descriptive-analysis",
    "href": "data.html#descriptive-analysis",
    "title": "2  Data preparation",
    "section": "2.6 Descriptive analysis",
    "text": "2.6 Descriptive analysis\nKey principals and examples of descriptive statistics.\n\nExamples of existing R functional for this.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data preparation</span>"
    ]
  },
  {
    "objectID": "fh.html",
    "href": "fh.html",
    "title": "3  The Univariate Fay-Herriot model",
    "section": "",
    "text": "3.1 Introduction\nIn this section, we will present a whole estimation procedure of the standard area-level model introduced by Fay and Herriot (1979) in R. As with the disclaimer in the preceding section, this practical manual is not intended to serve as a theoretical introduction to area-level models. Instead, it offers a set of straightforward and practical R scripts, accompanied by clear explanations, to demonstrate how these tasks can be carried out in R. For a theoretical foundation please refer to Fay and Herriot (1979) and Rao and Molina (2015). In addition to theoretical information, the vignette “A framework for producing small area estimates based on area-level models in R” of the R package emdi (Harmening et al. 2023) provides further code examples for the FH model.\nIn this chapter, we will describe how to run the univariate Fay-Herriot (FH) using simulated income data from Spain. The estimation procedure is explained step by step.\nStep 1: Data preparation. Compute the direct estimates and their corresponding variances on the area-level and eventually perform variance smoothing. Aggregate the available auxiliary variables to the same area level and combine both input data.\nStep 2: Model selection. Select the aggregated auxiliary variables at the area level for the FH model using a stepwise selection based on information criteria like the Akaike, Bayesian or Kullback information criteria.\nStep 3: Model estimation of FH point estimates and their mean squared error (MSE) estimates as uncertainty measure. Eventually apply a transformation.\nStep 4: Assessment of the estimated model. Check the FH model assumptions, including linearity, normality of predicted area effects and standardized model residuals. When violations of the model assumptions are detected, the application of a transformation might help. Repeat Step 3 including a transformation and check again the model assumptions.\nStep 5: Comparison of the FH results with the direct estimates.\nStep 6: Benchmark the FH point estimates for consistency with higher results.\nStep 7: Preparation of the results. Create tables and maps of results.\nStep 8: Saving the results. One option is to export the results to known formats like Excel or OpenDocument Spreadsheets.\nWe will show below the use of the fh() function of the R package emdi (Harmening et al. 2023) which computes the EBLUPs and their MSE estimates of the standard FH model and several extensions of it, among others it allows for the application of transformations. Because the poverty rate is a ratio, it might be helpful to apply the arcsin transformation to guarantee that the results lie between 0 and 1. The function call is:\nfh(fixed, vardir, combined_data, domains = NULL, method = \"reml\", interval = NULL,   k = 1.345, mult_constant = 1, transformation = \"no\", backtransformation = NULL,   eff_smpsize = NULL, correlation = \"no\", corMatrix = NULL, Ci = NULL, tol = 1e-04,   maxit = 100, MSE = FALSE, mse_type = \"analytical\", B = c(50, 0), seed = 123)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Univariate Fay-Herriot model</span>"
    ]
  },
  {
    "objectID": "fh.html#data-and-preparation",
    "href": "fh.html#data-and-preparation",
    "title": "3  The Univariate Fay-Herriot model",
    "section": "3.2 Data and preparation",
    "text": "3.2 Data and preparation\n\n3.2.1 Load required libraries\nFirst of all, load the required R libraries. The code automatically installs the packages that are not yet installed and loads them. If you need further packages, please add them to the list of p_load which contains the packages required to the run the codes.\n\nif (sum(installed.packages()[,1] %in% \"pacman\") != 1){\n  \n  install.packages(\"pacman\")\n  \n}\n\npacman::p_load(sae, survey, spdep, emdi, data.table, MASS, caret, dplyr, sf)\n\n\n\n3.2.2 Load the dataset\nUsually, SAE combines multiple data sources: a survey data set and a census or administrative/register dataset. For the estimation of area-level models, we need area-level aggregates of the same area-level (e.g. NUTS3) of both datasets. The target variable (typically household welfare/income for poverty mapping) is available in the survey but not in the census data.\nIn this example, we use a synthetic data set adapted from R package sae called incomedata. The original data contains information for \\(n = 17,119\\) fictitious individuals residing across \\(D = 52\\) Spanish provinces. The variables include the name of the province of residence (provlab), province code (prov), as well as several correlates of income.\nFor this tutorial, we use a random 10% sample of the incomedata to estimate the poverty rates. For the univariate case, we use the income variable from 2012.\n\nincome_dt &lt;- readRDS(\"data/incomedata_sample.RDS\")\n\nglimpse(income_dt)\n\nRows: 1,716\nColumns: 31\nGroups: provlab [52]\n$ provlab     &lt;fct&gt; Alava, Alava, Alava, Alava, Alava, Alava, Alava, Alava, Al…\n$ prov        &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n$ ac          &lt;int&gt; 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 8, 8, 8, 8, 8, 8, …\n$ gen         &lt;int&gt; 2, 1, 2, 1, 2, 2, 1, 2, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2…\n$ age         &lt;int&gt; 4, 4, 5, 3, 3, 2, 5, 4, 4, 0, 0, 3, 3, 1, 2, 0, 3, 3, 0, 2…\n$ nat         &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ educ        &lt;int&gt; 3, 3, 1, 3, 3, 0, 1, 3, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0…\n$ labor       &lt;int&gt; 1, 1, 3, 1, 3, 0, 3, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0…\n$ age2        &lt;int&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1…\n$ age3        &lt;int&gt; 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0…\n$ age4        &lt;int&gt; 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ age5        &lt;int&gt; 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ educ1       &lt;int&gt; 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0…\n$ educ2       &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0…\n$ educ3       &lt;int&gt; 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ nat1        &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ labor1      &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0…\n$ labor2      &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ labor3      &lt;int&gt; 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ income2012  &lt;dbl&gt; 26125.271, 27404.099, 13117.838, 27920.737, 8705.592, 6340…\n$ weight      &lt;dbl&gt; 7424.759, 23577.245, 41976.969, 19471.566, 12738.217, 2463…\n$ income2013  &lt;dbl&gt; 27500.673, 28174.985, 13581.315, 28508.978, 9182.841, 6272…\n$ income2014  &lt;dbl&gt; 34739.7326, 32137.8461, 15980.5806, 31512.1529, 11699.9025…\n$ povline2012 &lt;dbl&gt; 6477.484, 6477.484, 6477.484, 6477.484, 6477.484, 6477.484…\n$ povline2013 &lt;dbl&gt; 6515.865, 6515.865, 6515.865, 6515.865, 6515.865, 6515.865…\n$ povline2014 &lt;dbl&gt; 6515.865, 6515.865, 6515.865, 6515.865, 6515.865, 6515.865…\n$ abs         &lt;dbl&gt; 0.8493392, 0.8493392, 0.8493392, 0.8493392, 0.8493392, 0.8…\n$ ntl         &lt;dbl&gt; 0.2091905, 0.2091905, 0.2091905, 0.2091905, 0.2091905, 0.2…\n$ aec         &lt;dbl&gt; 1.1651876, 1.1651876, 1.1651876, 1.1651876, 1.1651876, 1.1…\n$ schyrs      &lt;dbl&gt; 1.7099321, 1.7099321, 1.7099321, 1.7099321, 1.7099321, 1.7…\n$ mkt         &lt;dbl&gt; 2.578254, 2.578254, 2.578254, 2.578254, 2.578254, 2.578254…\n\n\n\n\n3.2.3 Direct estimation\nWe will use the direct Horvitz-Thompons estimators that use the survey weights in weight variable. We calculate the sample sizes for each provinces and compute the direct estimates and their variances. We use the direct function of the emdi package here. Other options are e.g. the direct function of package sae (Molina and Marhuenda 2015) or the svyby command of package survey (Lumley 2024). Then, we create a dataframe containing the direct estimate, the standard errors, the variances, the coefficient of variation and the design effects, that are needed for the arcsin transformation. The design effect is the ratio of the variance considering the sampling design to the variance estimated under simple random sampling. For detailled information about the arcsin transformation please refer to Casas-Cordero, Encina, and Lahiri (2016) and Schmid et al. (2017). In some areas with a very small sample size, it may occur, that the individual data only consists of zeros and ones, resulting in a direct estimate of zero or one and a direct variance of zero. We set those areas to out-of-sample and for the final estimation results only the synthetic part of the FH model is used.\n\n## calculate sample size for each province\nsampsize_dt &lt;- \nincome_dt |&gt;\n  group_by(provlab) |&gt;\n  summarize(N = n())\n\n## computation of direct estimates and their variances\ndirect_dt &lt;- emdi::direct(y = \"income2012\",\n                       smp_data = income_dt |&gt; as.data.table(),\n                       smp_domains = \"provlab\",\n                       weights = \"weight\",\n                       threshold = unique(income_dt$povline2012),\n                       var = TRUE)\n\n## create dataframe\ndirect_dt &lt;- \n       direct_dt$ind |&gt;\n       dplyr::select(Domain, Head_Count) |&gt;\n       rename(Direct = \"Head_Count\") |&gt;\n       merge(direct_dt$MSE |&gt;\n               dplyr::select(Domain, Head_Count) |&gt;\n               rename(vardir = \"Head_Count\"),\n             by = \"Domain\") |&gt;\n       mutate(SD = sqrt(vardir)) |&gt;\n       mutate(CV = SD / Direct) |&gt;\n       merge(sampsize_dt |&gt; \n               mutate(provlab = as.factor(provlab)), \n             by.x = \"Domain\", \n             by.y = \"provlab\") |&gt;\n      mutate(var_SRS = Direct * (1 - Direct) / N) |&gt;\n      mutate(deff = vardir / var_SRS) |&gt;\n      mutate(n_eff = N/deff)\n\n## set zero variance to OOS\ndirect_dt &lt;- direct_dt[complete.cases(direct_dt), ]\n\n## have a look at sample sizes\nsummary(direct_dt$N)    \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   6.00   14.00   25.00   34.61   49.00  142.00 \n\n\n\n\n3.2.4 Variance smoothing\nA quick inspection of the preceding results will show some provinces contain low sample sizes which sometimes result in extreme value poverty rates and hence 0 variance. To avoid this, we will show you how to apply the variance smoothing method suggested by You and Hidiroglou (2023). Please see the code and Roxygen comments below explaining the use of the varsmoothie_king() function which computes smoothed variances. In case, the arcsin transformation will be applied, the variance smoothing described here is not necessary, since the arcsin transformation works variance stabilizing itself. When applying the arcsin transformation, the direct variances are automatically set to 1/(4*effective sampling size) when using the fh function of package emdi. The effective sample size equals the sample size of each area divided by the design effect. If the variance stabilizing effect is not enough, the design effect of a higher area level could also be used here (in this example the regions ac).\n\n#' A function to perform variance smoothing\n#' \n#' The variance smoothing function applies the methodology of You and Hiridoglou (2023)\n#' which uses simply log linear regression to estimate direct variances for sample \n#' poverty rates which is useful for replacing poverty rates in areas with low sampling.\n#' \n#' @param domain a vector of unique domain/target areas\n#' @param direct_var the raw variances estimated from sample data\n#' @param sampsize the sample size for each domain\n#' \n#' @export\n\nvarsmoothie_king &lt;- function(domain,\n                             direct_var,\n                             sampsize){\n\n  dt &lt;- data.table(Domain = domain,\n                   var = direct_var,\n                   n = sampsize)\n\n  dt$log_n &lt;- log(dt$n)\n  dt$log_var &lt;- log(dt$var)\n\n  lm_model &lt;- lm(formula = log_var ~ log_n,\n                 data = dt[!(abs(dt$log_var) == Inf),])\n\n  dt$pred_var &lt;- predict(lm_model, newdata = dt)\n  residual_var &lt;-  summary(lm_model)$sigma^2\n  dt$var_smooth &lt;- exp(dt$pred_var) * exp(residual_var/2)\n\n  return(dt[, c(\"Domain\", \"var_smooth\"), with = F])\n\n}\n\nOk, the goal now is to use the above varsmoothie_king() function to add an additional column of smoothed variances into our direct_dt dataframe.\n\nvar_smooth &lt;- varsmoothie_king(domain = direct_dt$Domain,\n                               direct_var = direct_dt$vardir,\n                               sampsize = direct_dt$N)\n\ndirect_dt &lt;- var_smooth |&gt; merge(direct_dt, by = \"Domain\")\n\n\n\n3.2.5 Auxiliary variable preparation\nThe FH model is a model of poverty rates at the target area level, hence the data format required for this exercise has the province as its unit of observation. This format has a few essential columns:\n\nVariable for poverty rates\nThe set of candidate variables from which the most predicted of poverty rates will be selected\nThe target area variable identifier (i.e. in this case the province variable prov and provlab)\n\nWe prepare this dataset as follows:\n\n## create the candidate variables\ncandidate_vars &lt;- colnames(income_dt)[!colnames(income_dt) %in% \n                                         c(\"provlab\", \"prov\", \n                                           \"income2012\", \"income2013\", \"income2014\",\n                                           \"povline2012\", \"povline2013\", \"povline2014\",\n                                           \"ac\", \"nat\", \"educ\", \"labor\",\n                                           \"age\")]\n \n## change dummy of gen to 0 and 1\nincome_dt &lt;- \n  income_dt |&gt;\n  mutate(across(c(gen), ~ case_when(\n    .x == 1 ~ 0,\n    .x == 2 ~ 1,\n    TRUE ~ NA_real_\n  )))\n\n## aggregating the unit-level data to the province level\nprov_dt &lt;- \nincome_dt |&gt;\n  group_by(provlab) |&gt;\n  summarize(\n    across(\n      any_of(candidate_vars),\n      ~ weighted.mean(x = ., w = weight, na.rm = TRUE),\n      .names = \"{.col}\"\n    )\n  )\n\n### combine the the dataframe containing the direct estimates and their variances with the province level data\ncomb_Data &lt;- merge(direct_dt, prov_dt,\n    by.x = \"Domain\", by.y = \"provlab\",\n    all = TRUE)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Univariate Fay-Herriot model</span>"
    ]
  },
  {
    "objectID": "fh.html#model-selection",
    "href": "fh.html#model-selection",
    "title": "3  The Univariate Fay-Herriot model",
    "section": "3.3 Model selection",
    "text": "3.3 Model selection\n\n3.3.1 Model preparation\nFH does not run if there is any missing value in the auxiliary variables, and therefore, any variable with missing value should be removed in advance.\n\nrowsNAcovariates &lt;- rowSums(sapply(comb_Data[,..candidate_vars], is.na))\ncomb_Data &lt;- comb_Data[rowsNAcovariates == 0, ]\n\n\n\n3.3.2 Check multicollinearity\nWith the help of the step() function of package emdi, we perform a variable selection based on the chosen variable selection criterion and directly get the model with fewer variables. The function step_wrapper() implemented below is a wrapper to the emdi::step() function and performs all the perfunctory cleaning necessary to use step(). This includes dropping columns that are entirely missing (NA) and keep only complete cases/observations (for the model selection only the in-sample domains are used) and remove perfectly or near collinear variables and combinations.\n\n#' A function to perform stepwise variable selection based on selection criteria\n#' \n#' @param dt data.frame, dataset containing the set of outcome and independent variables\n#' @param xvars character vector, the set of x variables\n#' @param y character, the name of the y variable\n#' @param cor_thresh double, a correlation threshold between 0 and 1\n#' @param criteria character, criteria that can be chosen are \"AIC\", \"AICc\", \"AICb1\", \"AICb2\", \"BIC\", \"KIC\", \"KICc\", \"KICb1\", or \"KICb2\". Defaults to \"AIC\". If transformation is set to \"arcsin\", only \"AIC\" and \"BIC\" can be chosen.\n#' @param vardir character, name of the variable containing the domain-specific sampling variances of the direct estimates that are included in dt\n#' @param transformation character, either \"no\" (default) or \"arcsin\".\n#' @param eff_smpsize character, name of the variable containing the effective sample sizes that are included in dt. Required argument when the arcsin transformation is chosen. Defaults to NULL.\n#' \n#' @import data.table\n#' @import caret\n#' @importFrom emdi step fh\n\nstep_wrapper &lt;- function(dt, xvars, y, cor_thresh = 0.95, criteria = \"AIC\",\n                         vardir, transformation = \"no\", eff_smpsize) {\n  \n  dt &lt;- as.data.table(dt)\n \n  # Drop columns that are entirely NA\n  dt &lt;- dt[, which(unlist(lapply(dt, function(x) !all(is.na(x))))), with = FALSE]\n  \n  xvars &lt;- xvars[xvars %in% colnames(dt)]\n  \n  # Keep only complete cases\n  dt &lt;- dt[complete.cases(dt),] \n  \n  # Step 1: Remove aliased (perfectly collinear) variables\n  model_formula &lt;- as.formula(paste(y, \"~\", paste(xvars, collapse = \" + \")))\n  lm_model &lt;- lm(model_formula, data = dt)\n  aliased &lt;- is.na(coef(lm_model))\n  if (any(aliased)) {\n    xvars &lt;- names(aliased)[!aliased & names(aliased) != \"(Intercept)\"]\n  }\n  \n  # Step 2: Remove near-linear combinations\n  xmat &lt;- as.matrix(dt[, ..xvars])\n  combo_check &lt;- tryCatch(findLinearCombos(xmat), error = function(e) NULL)\n  if (!is.null(combo_check) && length(combo_check$remove) &gt; 0) {\n    xvars &lt;- xvars[-combo_check$remove]\n    xmat &lt;- as.matrix(dt[, ..xvars])\n  }\n  \n  # Step 3: Drop highly correlated variables\n  cor_mat &lt;- abs(cor(xmat))\n  diag(cor_mat) &lt;- 0\n  while (any(cor_mat &gt; cor_thresh, na.rm = TRUE)) {\n    cor_pairs &lt;- which(cor_mat == max(cor_mat, na.rm = TRUE), arr.ind = TRUE)[1, ]\n    var1 &lt;- colnames(cor_mat)[cor_pairs[1]]\n    var2 &lt;- colnames(cor_mat)[cor_pairs[2]]\n    # Drop the variable with higher mean correlation\n    drop_var &lt;- if (mean(cor_mat[var1, ]) &gt; mean(cor_mat[var2, ])) var1 else var2\n    xvars &lt;- setdiff(xvars, drop_var)\n    xmat &lt;- as.matrix(dt[, ..xvars])\n    cor_mat &lt;- abs(cor(xmat))\n    diag(cor_mat) &lt;- 0\n  }\n  \n  # Step 4: Warn if still ill-conditioned\n  cond_number &lt;- kappa(xmat, exact = TRUE)\n  if (cond_number &gt; 1e10) {\n    warning(\"Design matrix is ill-conditioned (condition number &gt; 1e10). Consider reviewing variable selection.\")\n  }\n  \n  # Final model fit\n  model_formula &lt;- as.formula(paste(y, \"~\", paste(xvars, collapse = \" + \")))\n  \n  # Stepwise selection\n  fh_args &lt;- list(\n    fixed = model_formula,\n    vardir = vardir,\n    combined_data = dt,\n    method = \"ml\",\n    MSE = FALSE\n  )\n\n  if (transformation == \"arcsin\") {\n    fh_args$transformation &lt;- \"arcsin\"\n    fh_args$backtransformation &lt;- \"bc\"\n    fh_args$eff_smpsize &lt;- eff_smpsize\n  } else {\n    fh_args$transformation &lt;- \"no\"\n    fh_args$B &lt;- c(0, 50)\n  }\n\n  fh_model &lt;- do.call(emdi::fh, fh_args)\n  \n  stepwise_model &lt;- emdi::step(fh_model, criteria = criteria)\n\n  return(stepwise_model)\n\n}\n\nWe apply the function to select the variables.\n\nfh_step &lt;- step_wrapper(dt = comb_Data, \n                        xvars = candidate_vars,\n                        y = \"Direct\",\n                        cor_thresh = 0.8,\n                        criteria = \"AIC\",\n                        vardir = \"vardir\", \n                        transformation = \"arcsin\", \n                        eff_smpsize = \"n_eff\")\n\n         df     AIC\n- gen     1 -63.164\n- mkt     1 -63.163\n- labor1  1 -63.059\n- nat1    1 -62.934\n- labor2  1 -62.613\n- weight  1 -62.543\n- aec     1 -61.789\n- educ2   1 -61.408\n- abs     1 -61.311\n&lt;none&gt;      -61.165\n- educ3   1 -59.668\n- age2    1 -59.182\n- ntl     1 -58.846\n- educ1   1 -58.643\n- age4    1 -58.363\n- age3    1 -58.149\n- age5    1 -57.609\n- schyrs  1 -49.426\n\n\n         df     AIC\n- mkt     1 -65.162\n- labor1  1 -65.016\n- nat1    1 -64.893\n- labor2  1 -64.599\n- weight  1 -64.389\n- aec     1 -63.782\n- abs     1 -63.293\n- educ2   1 -63.211\n&lt;none&gt;      -63.164\n- educ3   1 -61.628\n- age2    1 -61.174\n- ntl     1 -60.377\n- age4    1 -60.327\n- educ1   1 -60.282\n- age3    1 -60.114\n- age5    1 -59.585\n- schyrs  1 -49.299\n\n\n         df     AIC\n- labor1  1 -66.996\n- nat1    1 -66.892\n- labor2  1 -66.572\n- weight  1 -66.299\n- aec     1 -65.757\n- abs     1 -65.265\n- educ2   1 -65.205\n&lt;none&gt;      -65.162\n- educ3   1 -63.586\n- age2    1 -63.156\n- ntl     1 -62.339\n- age4    1 -62.282\n- educ1   1 -62.281\n- age3    1 -62.103\n- age5    1 -61.529\n- schyrs  1 -44.722\n\n\n         df     AIC\n- nat1    1 -68.744\n- weight  1 -68.194\n- labor2  1 -68.091\n- aec     1 -67.444\n- educ2   1 -67.204\n- abs     1 -67.056\n&lt;none&gt;      -66.996\n- educ3   1 -65.559\n- ntl     1 -64.281\n- educ1   1 -64.280\n- age2    1 -64.146\n- age5    1 -63.528\n- age4    1 -63.153\n- age3    1 -62.968\n- schyrs  1 -44.445\n\n\n         df     AIC\n- labor2  1 -69.873\n- weight  1 -69.862\n- aec     1 -69.064\n- educ2   1 -68.990\n- abs     1 -68.835\n&lt;none&gt;      -68.744\n- educ3   1 -67.146\n- educ1   1 -66.088\n- age2    1 -66.018\n- ntl     1 -65.874\n- age5    1 -65.351\n- age4    1 -65.001\n- age3    1 -64.922\n- schyrs  1 -45.918\n\n\n         df     AIC\n- weight  1 -70.966\n- aec     1 -70.254\n- educ2   1 -70.002\n&lt;none&gt;      -69.873\n- abs     1 -69.619\n- educ3   1 -68.510\n- age2    1 -67.269\n- ntl     1 -67.101\n- educ1   1 -66.763\n- age5    1 -66.706\n- age3    1 -65.948\n- age4    1 -65.936\n- schyrs  1 -45.910\n\n\n         df     AIC\n- aec     1 -71.617\n- educ2   1 -71.408\n&lt;none&gt;      -70.966\n- abs     1 -70.484\n- educ3   1 -69.733\n- age2    1 -68.714\n- ntl     1 -68.470\n- educ1   1 -68.334\n- age5    1 -67.634\n- age3    1 -67.530\n- age4    1 -67.073\n- schyrs  1 -46.344\n\n\n         df     AIC\n- educ2   1 -72.299\n&lt;none&gt;      -71.617\n- abs     1 -71.486\n- educ3   1 -70.772\n- age2    1 -69.871\n- educ1   1 -69.716\n- ntl     1 -68.932\n- age5    1 -68.705\n- age4    1 -68.546\n- age3    1 -68.297\n- schyrs  1 -46.078\n\n\n         df     AIC\n- educ3   1 -72.474\n&lt;none&gt;      -72.299\n- abs     1 -72.265\n- age2    1 -71.479\n- age5    1 -69.915\n- educ1   1 -69.674\n- ntl     1 -69.367\n- age4    1 -69.075\n- age3    1 -67.544\n- schyrs  1 -45.239\n\n\n         df     AIC\n- abs     1 -73.093\n&lt;none&gt;      -72.474\n- age2    1 -71.628\n- age5    1 -71.235\n- educ1   1 -71.185\n- age4    1 -70.720\n- ntl     1 -70.406\n- age3    1 -69.189\n- schyrs  1 -45.949\n\n\n         df     AIC\n&lt;none&gt;      -73.093\n- age5    1 -72.249\n- educ1   1 -72.137\n- age2    1 -71.922\n- ntl     1 -71.327\n- age4    1 -71.134\n- age3    1 -70.271\n- schyrs  1 -47.230\n\nprint(fh_step$fixed)\n\nDirect ~ age2 + age3 + age4 + age5 + educ1 + ntl + schyrs\n&lt;environment: 0x000002e70931b6c8&gt;",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Univariate Fay-Herriot model</span>"
    ]
  },
  {
    "objectID": "fh.html#model-estimation-of-fh-point-and-their-mse-estimates.",
    "href": "fh.html#model-estimation-of-fh-point-and-their-mse-estimates.",
    "title": "3  The Univariate Fay-Herriot model",
    "section": "3.4 Model estimation of FH point and their MSE estimates.",
    "text": "3.4 Model estimation of FH point and their MSE estimates.\nIn this example, we use the function fh to calculate the FH estimates. Because we want to estimate a ratio, we need to apply the arcsin transformation to guarantee that the results lie between 0 and 1. For that, we choose “arcsin” as transformation, and a bias-corrected backtransformation (“bc”). Additionally, the effective sample size, which equals the sample size of each area divided by the design effect, is needed for the arcsin transformation. We set the MSE estimation to TRUE, the mse_type to “boot” (necessary for the type of transformation) and determine the number of bootstrap iterations. For practical applications, values larger than 200 are recommended. In case, no transformation is desired, the transformation argument must be set to “no” and the inputs backtransformation and eff_smpsize are no longer needed.\n\nfh_model &lt;- fh(fixed = formula(fh_step$fixed),\n               vardir = \"vardir\", \n               combined_data = comb_Data, \n               domains = \"Domain\",\n               method = \"ml\", \n               transformation = \"arcsin\", \n               backtransformation = \"bc\",\n               eff_smpsize = \"n_eff\", \n               MSE = TRUE, \n               mse_type = \"boot\", B = c(50, 0)) \n\n## In case, no transformation is desired, the call would like this:\n# fh_model &lt;- fh(\n#   fixed = Direct ~ age2 + age3 + age4 + age5 + educ1 + ntl + schyrs, #formula(fh_step$fixed),\n#   vardir = \"vardir\", combined_data = comb_Data, domains = \"Domain\",\n#   method = \"ml\", MSE = TRUE)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Univariate Fay-Herriot model</span>"
    ]
  },
  {
    "objectID": "fh.html#assessment-of-the-estimated-model.",
    "href": "fh.html#assessment-of-the-estimated-model.",
    "title": "3  The Univariate Fay-Herriot model",
    "section": "3.5 Assessment of the estimated model.",
    "text": "3.5 Assessment of the estimated model.\nWith the help of the summary method of emdi, we gain detailed insights into the data and model components. It includes information on the estimation methods used, the number of domains, the log-likelihood, and information criteria as proposed by Marhuenda, Morales, and Camen Pardo (2014). It also reports the adjusted \\(R^2\\) from a standard linear model and the adjusted \\(R^2\\) specific to FH models, as introduced by Lahiri and Suntornchost (2015). It also offers diagnostic measures to assess model assumptions regarding the standardized realized residuals and random effects. These include skewness and kurtosis (based on the moments package by Komsta and Novomestky (2015)), as well as Shapiro-Wilk test statistics and corresponding p-values to evaluate the normality of both error components.\n\nsummary(fh_model)\n\nCall:\n fh(fixed = formula(fh_step$fixed), vardir = \"vardir\", combined_data = comb_Data, \n    domains = \"Domain\", method = \"ml\", transformation = \"arcsin\", \n    backtransformation = \"bc\", eff_smpsize = \"n_eff\", MSE = TRUE, \n    mse_type = \"boot\", B = c(50, 0))\n\nOut-of-sample domains:  3 \nIn-sample domains:  49 \n\nVariance and MSE estimation:\nVariance estimation method:  ml \nEstimated variance component(s):  7.643268e-05 \nMSE method:  bootstrap \n\nCoefficients:\n            coefficients std.error t.value   p.value    \n(Intercept)     0.803839  0.161793  4.9683 6.753e-07 ***\nage2           -0.596737  0.335123 -1.7807   0.07497 .  \nage3           -0.492147  0.224118 -2.1959   0.02810 *  \nage4           -0.501614  0.252090 -1.9898   0.04661 *  \nage5           -0.436006  0.258521 -1.6865   0.09169 .  \neduc1           0.251093  0.146038  1.7194   0.08555 .  \nntl            -0.032500  0.016747 -1.9406   0.05230 .  \nschyrs          0.093883  0.016794  5.5904 2.265e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nExplanatory measures:\n   loglike       AIC       BIC     AdjR2     FH_R2\n1 45.54653 -73.09307 -56.06668 0.4342252 0.7365157\n\nResidual diagnostics:\n                         Skewness Kurtosis Shapiro_W Shapiro_p\nStandardized_Residuals  0.1976387 2.604752 0.9829282 0.6919975\nRandom_effects         -0.3420849 3.629614 0.9694044 0.2292763\n\nTransformation:\n Transformation Back_transformation\n         arcsin                  bc\n\n\nWe can see, that 49 domains are in-sample domains. The 3 out-of-sample domains belong to the domains with 0 direct and variance estimates that we set to NA in the beginning. The variance of the random effects equals 7.643268e-05. All of the included auxiliary variables are significant on a 0.05 significance level and their explanatory power is large with an adjusted \\(R^2\\) (for FH models) of around 0.74. The results of the Shapiro-Wilk-test indicate that normality is not rejected for both errors.\n\n3.5.1 Diagnostic plots\nWe produce normal quantile-quantile (Q-Q) plots of the standardized realized residuals and random effects and plots of the kernel densities of the distribution of both error terms by the plot method of emdi.\n\nplot(fh_model)\n\n\n\n\n\n\n\n\nPress [enter] to continue\n\n\n\n\n\n\n\n\n\nPress [enter] to continue\n\n\n\n\n\n\n\n\n\nThe plots show slight deviations of the distributions from normality, but together with the results of the Shapiro-Wilk-test, we do not reject the normality assumption.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Univariate Fay-Herriot model</span>"
    ]
  },
  {
    "objectID": "fh.html#comparison-of-the-fh-results-with-the-direct-estimates.",
    "href": "fh.html#comparison-of-the-fh-results-with-the-direct-estimates.",
    "title": "3  The Univariate Fay-Herriot model",
    "section": "3.6 Comparison of the FH results with the direct estimates.",
    "text": "3.6 Comparison of the FH results with the direct estimates.\nThe FH estimates are expected to align closely with the direct estimates in domains with small direct MSEs and/or large sample sizes. Moreover, incorporating auxiliary information should enhance the precision of the direct estimates. We produce a scatter plot proposed by Brown et al. (2001) and a line plot. The fitted regression and the identity line of the scatter plot should not differ too much. The FH estimates should track the direct estimates within the line plot especially for domains with a large sample size/small MSE of the direct estimator. Furthermore, we compare the MSE and CV estimates for the direct and FH estimators using boxplots and ordered scatter plots (by setting the input arguments MSE and CV to TRUE).\nAdditionally, we compute a correlation coefficient of the direct estimates and the estimates of the regression-synthetic part of the FH model (Chandra, Salvati, and Chambers 2015) and a goodness of fit diagnostic (Brown et al. 2001).\n\ncompare_plot(fh_model, MSE = TRUE, CV = TRUE)\n\n\n\n\n\n\n\n\nPress [enter] to continue\n\n\n\n\n\n\n\n\n\nPress [enter] to continue\n\n\n\n\n\n\n\n\n\nPress [enter] to continue\n\n\n\n\n\n\n\n\n\nPress [enter] to continue\n\n\n\n\n\n\n\n\n\nPress [enter] to continue\n\n\n\n\n\n\n\n\ncompare(fh_model)\n\nBrown test \n\nNull hypothesis: EBLUP estimates do not differ significantly from the\n      direct estimates \n\n  W.value Df   p.value\n 31.14936 49 0.9780679\n\nCorrelation between synthetic part and direct estimator:  0.68 \n\n\nThe direct estimates are tracked by most of the FH estimates within the line plot. The precision of the direct estimates could be improved by the usage of the FH model in terms of MSEs and CVs. The null hypothesis of the Brown test is not rejected and the correlation coefficient indicates a positive correlation (0.68) between the direct and FH estimates.\nIf the result of the model assessment is not satisfactory, the following should be checked again: Can the direct estimation including variance estimation be improved? Are there further auxiliary variables and/or must possible interaction effects be taken into account? Does a (different) transformation need to be used?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Univariate Fay-Herriot model</span>"
    ]
  },
  {
    "objectID": "fh.html#benchmark-the-fh-point-estimates-for-consistency-with-higher-results.",
    "href": "fh.html#benchmark-the-fh-point-estimates-for-consistency-with-higher-results.",
    "title": "3  The Univariate Fay-Herriot model",
    "section": "3.7 Benchmark the FH point estimates for consistency with higher results.",
    "text": "3.7 Benchmark the FH point estimates for consistency with higher results.\nBenchmarking is based on the principle that aggregated FH estimates should sum up to the estimates at a higher regional level. For the benchmark function, a benchmark value and a vector containing the shares of the population size per area (\\(N_d/N\\)) is required. Please note, that only the FH estimates are benchmarked and not their MSE estimates. As benchmark types “raking”, “ratio” and “MSE_adj” can be chosen. For further details about using the function, please refer to the emdi vignette and for general information about the benchmarking options to Datta et al. (2011).\n\n## create the poverty indicator\nincome_dt &lt;- \n  income_dt |&gt;\n  mutate(poor2012 = ifelse(income2012 &lt; povline2012, 1, 0))\n\n## compute the benchmark value (mean of poor2012 for the whole country of Spain)\nbenchmark_value &lt;- weighted.mean(income_dt$poor2012, income_dt$weight)\n\n## compute the share of population size in the total population size (N_d/N) per area\ndata(\"sizeprov\")\ncomb_Data &lt;- comb_Data |&gt;\n  left_join(sizeprov |&gt;\n  mutate(ratio_n = Nd/sum(Nd)), by = c(\"Domain\" =\"provlab\"))\n\nfh_bench &lt;- benchmark(fh_model,\n                      benchmark = benchmark_value,\n                      share = comb_Data$ratio_n, \n                      type = \"ratio\",\n                      overwrite = TRUE)\nhead(fh_bench$ind)\n\n    Domain     Direct        FH  FH_Bench Out\n1    Alava 0.30885799 0.2930754 0.3055698   0\n2 Albacete 0.05339445 0.1166571 0.1216305   0\n3 Alicante 0.19412452 0.1629585 0.1699058   0\n4  Almeria 0.42100526 0.1954816 0.2038155   0\n5    Avila         NA 0.1156717 0.1206030   1\n6  Badajoz 0.15381464 0.1664585 0.1735550   0",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Univariate Fay-Herriot model</span>"
    ]
  },
  {
    "objectID": "fh.html#preparation-of-the-results.",
    "href": "fh.html#preparation-of-the-results.",
    "title": "3  The Univariate Fay-Herriot model",
    "section": "3.8 Preparation of the results.",
    "text": "3.8 Preparation of the results.\nCreate one dataframe that contains the direct and FH estimation results including MSE and CV results.\n\nresults &lt;-as.data.frame(estimators(fh_model, MSE = TRUE, CV = TRUE))\nhead(results)\n\n    Domain     Direct  Direct_MSE Direct_CV        FH       FH_MSE     FH_CV\n1    Alava 0.30885799 0.025486256 0.5168853 0.2930754 0.0051610993 0.2451274\n2 Albacete 0.05339445 0.003324107 1.0797952 0.1166571 0.0009149401 0.2592896\n3 Alicante 0.19412452 0.003357358 0.2984822 0.1629585 0.0006634858 0.1580661\n4  Almeria 0.42100526 0.015853486 0.2990714 0.1954816 0.0009920503 0.1611242\n5    Avila         NA          NA        NA 0.1156717 0.0031406044 0.4844842\n6  Badajoz 0.15381464 0.004314773 0.4270526 0.1664585 0.0005706955 0.1435147\n\n\n\n3.8.1 Poverty map\nWith the help of geographical maps, the results can be presented in a user-friendly way and differences among the areas can be detected more easily. For the map, a shape file is reqired. The domain identifiers in the results object (fh_model) need to match to the respective identifiers of the shape file. Therefore, we create a mapping table first and then produce the map by emdi::map_plot.\n\n## load the shapefile dataframe and convert it to an object of type sf which is a necessary input for the map_plot function\nspain_dt &lt;- readRDS(\"data/shapes/spainshape.RDS\")\nspain_dt &lt;- sf::st_as_sf(spain_dt)\n\n## Create a suitable mapping table\n## Find the right order\ndomain_ord &lt;- match(spain_dt$provlab, fh_model$ind$Domain)\n\n## Create the mapping table based on the order obtained above\nmap_tab &lt;- data.frame(pop_data_id = fh_model$ind$Domain[domain_ord],\n                      shape_id = spain_dt$provlab)\n\n## Create map\nmap_plot(object = fh_model, MSE = TRUE, map_obj = spain_dt,\n map_dom_id = \"provlab\", map_tab = map_tab)\n\nPress [enter] to continue\n\n\nPress [enter] to continue\n\n\nPress [enter] to continue\n\n\n\n\n\n\n\n\nFigure 3.1\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.2\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.3\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.4",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Univariate Fay-Herriot model</span>"
    ]
  },
  {
    "objectID": "fh.html#saving-the-results.",
    "href": "fh.html#saving-the-results.",
    "title": "3  The Univariate Fay-Herriot model",
    "section": "3.9 Saving the results.",
    "text": "3.9 Saving the results.\n\n3.9.1 Save workspace.\n\nsave.image(\"fh_estimation.RData\")\n\n\n\n3.9.2 Export the model output and estimation results.\n\nwrite.excel(fh_model,\n  file = \"fh_model_output.xlsx\",\n  MSE = TRUE, CV = TRUE\n)\n\n\n\n\n\n\n\nBrown, G., R. Chambers, P. Heady, and D. Heasman. 2001. “Evaluation of Small Area Estimation Methods - an Application to Unemployment Estimates from the UK LFS.” In Proceedings of Statistics Canada Symposium.\n\n\nCasas-Cordero, C., J. Encina, and P. Lahiri. 2016. “Poverty Mapping for the Chilean Comunas.” In Analysis of Poverty by Small Area Estimation, by M. Pratesi, 379–403. John Wiley & Sons. https://doi.org/10.1002/9781118814963.ch20.\n\n\nChandra, H., N. Salvati, and R. Chambers. 2015. “A Spatially Nonstationary Fay-Herriot Model for Small Area Estimation.” Journal of the Survey Statistics and Methodology 3 (2): 109–35. https://doi.org/10.1093/jssam/smu026.\n\n\nDatta, G. S., M. Ghosh, R. Steorts, and J. Maples. 2011. “Bayesian Benchmarking with Applications to Small Area Estimation.” TEST 20 (3): 574–88. https://doi.org/10.1007/s11749-010-0218-y.\n\n\nFay, Robert E., and Roger A. Herriot. 1979. “Estimates of Income for Small Places: An Application of James-Stein Procedures to Census Data.” Journal of the American Statistical Association 74: 269–77.\n\n\nHarmening, Sylvia, Ann-Kristin Kreutzmann, Sören Schmidt, Nicola Salvati, and Timo Schmid. 2023. “A Framework for Producing Small Area Estimates Based on Area-Level Models in r.” The R Journal 15 (1): 316–41. https://doi.org/10.32614/RJ-2023-039.\n\n\nKomsta, Lukasz, and Frederick Novomestky. 2015. Moments: Moments, Cumulants, Skewness, Kurtosis and Related Tests. https://CRAN.R-project.org/package=moments.\n\n\nLahiri, P., and J. Suntornchost. 2015. “Variable Selection for Linear Mixed Models with Applications in Small Area Estimation.” The Indian Journal of Statistics 77-B (2): 312–20. https://www.jstor.org/stable/43694416.\n\n\nLumley, Thomas. 2024. “Survey: Analysis of Complex Survey Samples.”\n\n\nMarhuenda, Y., D. Morales, and M. del Camen Pardo. 2014. “Information Criteria for Fay-Herriot Model Selection.” Computational Statistics and Data Analysis 70: 268–80. https://doi.org/10.1016/j.csda.2013.09.016.\n\n\nMolina, Isabel, and Yolanda Marhuenda. 2015. “sae: An R Package for Small Area Estimation.” The R Journal 7 (1): 81–98. https://journal.r-project.org/archive/2015/RJ-2015-007/RJ-2015-007.pdf.\n\n\nRao, J. N. K., and Isabel Molina. 2015. Small Area Estimation. John Wiley; Sons, Inc, Hoboken, NJ, USA.\n\n\nSchmid, T., F. Bruckschen, N. Salvati, and T. Zbiranski. 2017. “Constructing Sociodemographic Indicators for National Statistical Institutes Using Mobile Phone Data: Estimating Literacy Rates in Senegal.” Journal of the Royal Statistical Society A 180 (4): 1163–90. https://doi.org/10.1111/rssa.12305.\n\n\nYou, Yong, and Mike Hidiroglou. 2023. “Application of Sampling Variance Smoothing Methods for Small Area Proportion Estimation.” Journal of Official Statistics 39 (4): 571–90.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>The Univariate Fay-Herriot model</span>"
    ]
  },
  {
    "objectID": "fh_multivariate.html",
    "href": "fh_multivariate.html",
    "title": "4  Multivariate Fay–Herriot",
    "section": "",
    "text": "4.1 Stable FH estimators over T time periods\nThis section describes procedures that yield stable small area estimators for each of D areas over T subsequent time instants.\nMFH estimation procedure for T time instants: Step 1 Compute the selected direct area estimators for each area d = 1, . . . , D for each time t = 1, . . . , T, and estimators of their corresponding sampling variances and covariances.\nStep 2 Select the area-level auxiliary variables for each time instant in the MFH model. A simple approach is to perform a model selection procedure in a linear regression model without the area effects for each time instant t = 1, . . . , T .\nStep 3 Fit MFH3 model and test whether the area-time effects (ud1, . . . , udT ) are homoscedastic or not. If we reject the homoscedasticity of variances, consider MFH3 model. Otherwise, consider MFH2 model.\nStep 4 Check the selected model assumptions, including linearity, normality of pre- dicted area effects and standardized model residuals, and the presence of outlying areas.\nStep 5 In case of clear systematic model departures, the model should be changed. In case of isolated departures because of outlying areas, either do not obtain the MFH estimate for those areas or change some aspect of the model or the data. Then, go to Step 2.\nStep 6 If model assumptions hold, using the above direct estimates and estimated sampling variances and covariances, and the selected auxiliary variables, compute MFH estimators for, d = 1, . . . , D and t = 1, . . . , T , and their corresponding estimated MSEs.\nThe functions eblupMFH2() and eblupMFH3() from the R package msae (Per- matasari and Ubaidillah, 2022) compute the EBLUPs and their MSE estimates under the MFH models 2 and 3, respectively. The calls to these functions are: \\(eblupMFH2(formula, vardir, MAXITER = 100, PRECISION = 1e-04, data)\\) or \\(eblupMFH3(formula, vardir, MAXITER = 100, PRECISION = 1e-04, data)\\).\nThe arguments of these two functions are the same, differing only in the outputs. They require specifying a list with T R formula objects, one for each time instant, separated by commas. In each of these regression formulas (one for each time instant), we place the vector of direct estimates on the left-hand side and the area-level independent variables on the right-hand side, separated by “+”. As usual, by default an intercept is automatically included in each regression formula.\nAs in the case of the eblupFH() function, they also require specifying the estimated sampling variances and covariances of the direct estimators for each area and time in the argument vardir. The user can also modify the maximum number of iterations, MAXITER, which is set by default to 100, and the convergence tolerance criteria, PRECISION, of the Fisher-scoring algorithm, which is set to 1e-4. The final argument, data, can be optionally specified to indicate a data object that includes the variables found in formula and vardir as its columns. Similar to the eblupFH() function, these functions do not accept NA values, and they will not return estimates for areas with zero sample sizes. Consequently, such areas should be excluded from the dataset.\nBoth functions return a list containing the following objects: eblup, a vector of EBLUPs for the areas; MSE, a data frame with the estimated mean squared errors of the EBLUPs; randomEffect, a data frame containing the predicted area-time effects; Rmatrix, a diagonal matrix with the sampling errors; and fit, a list with additional information from the model fitting. In the fit list, we can find the fitting method used (method); a logical value indicating the convergence of the Fisher scoring algorithm (convergence); the number of iterations performed by the Fisher scoring algorithm (iterations); a data frame containing the estimated regression coefficients in the first column, their standard errors in the second, the t statistics in the third, and the p-values of the significance of each coefficient in the last column (estcoef); a data frame with the estimated random effects variance (refvar); a data frame with the estimated autocorrelation coefficient ρ of the random effects; and the estimated Fisher information matrix (informationFisher).\nThe function eblupMFH3() additionally includes in the fit list, a contrast to test the homogeneity of random effects variance, called refvarTest. This test helps the user to choose between the heteroscedastic Model 3 or the homoscedastic Model 2, for a particular data set.\nExample 1 below illustrates the calculation of MFH estimators of area poverty rates for T time instants, using the functions eblupMFH2() and eblupMFH3() of the R package msae (Permatasari and Ubaidillah, 2022).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multivariate Fay–Herriot</span>"
    ]
  },
  {
    "objectID": "fh_multivariate.html#stable-fh-estimators-over-t-time-periods",
    "href": "fh_multivariate.html#stable-fh-estimators-over-t-time-periods",
    "title": "4  Multivariate Fay–Herriot",
    "section": "",
    "text": "4.1.1 Example 1 (MFH estimators of poverty rates for T time periods, in R)\nIn this example, we use the data set datasae3 from the R package msae (Permatasari and Ubaidillah, 2022). This data set contains simulated data generated under a FH model with heteroscedastic AR(1) area-time effects. There are two auxiliary variables, X1 and X2. Direct estimates for time instants 1,2 and 3 are given in Y1, Y2, and Y3, respectively. The elements of the variance-covariance matrix of the sampling errors are given in v1, v2, v3, v12, v13 and v23.\nWe first load the package and the data set:\n\nlibrary(msae) \ndata(datasae3)\n\nStep 1: The direct area estimators and their sample variances are already given in the dataset. Step 2: We should select the auxiliary variables at each time point. This example is only for illustration of application of the R function, and hence we use all the available variables, but we should emphasize the importance of this variable selection process in real-world applications. Step 3: In order to choose between the two alternative MFH models with heteroscedastic or homoscedastic area-time effects, we fit the heteroscedastic Model 3, and then check for the equality of the variances over the three time points. Hence, we use the function eblupMFH3 to fit the model:\n\nFo &lt;- list(f1=Y1~X1+X2,\nf2=Y2~X1+X2, f3=Y3~X1+X2)\nvardir &lt;- c(\"v1\", \"v2\", \"v3\", \"v12\", \"v13\", \"v23\") \nm3 &lt;- eblupMFH3(Fo, vardir, data=datasae3)\n\nNext we check the equality of the random effect variances. For this, we use the results of a hypothesis test included in the output of the function eblupMFH3: \\(m3\\)fit\\(refvarTest\\).\nThis test tests the null hypothesis that the variances \\(σ^2\\) at each pair of instants t and s are equal against the alternative that they are not. In this case, at significance level of 0.05, we reject the equality of variances between t = 3 and t = 1, as well as between t = 3 and t = 2. Note that this is a multiple test, and hence it is advisable to adjust the significance level. In this case, the tests clearly indicate heteroscedasticity, and we proceed with Model 3. If these tests supported equality of variances, then we would use instead the function eblupMFH2 for estimation.\nStep 4: We now verify the assumptions of the MFH3 model. This includes assessing linearity, the normality of the predicted area effects and standardized residuals, as well as checking for the presence of outlying areas. We first check the linearity assumption. This can be addressed by examining the scatter plot of residuals against predicted values (EBLUPs). The plot is generated for each of the T = 3 time periods.\n\nresids_3 &lt;- cbind(datasae3$Y1-m3$eblup$Y1, datasae3$Y2-m3$eblup$Y2, datasae3$Y3-m3$eblup$Y3)\nlayout(matrix(1:3,nrow = 1, byrow = TRUE)) \nplot(m3$eblup$Y1,resids_3[,1],pch=19,xlab=\"EBLUPs t=1\",ylab=\"Residuals t=1\") \nplot(m3$eblup$Y2,resids_3[,2],pch=19,xlab=\"EBLUPs t=2\",ylab=\"Residuals t=2\") \nplot(m3$eblup$Y3,resids_3[,3],pch=19,xlab=\"EBLUPs t=3\",ylab=\"Residuals t=3\")\n\n\n\n\n\n\n\n\nThese three plots do not provide evidences against the linearity assumption. We now evaluate the normality assumption of residuals, again for the 3 time periods:\n\nlayout(matrix(1:2,nrow = 1, byrow = TRUE))\nhist(resids_3[,1], probability=TRUE, main=\"\",xlab=\"Residuals t=1\", ylim=c(0,3.7))\nmean_est &lt;- mean(resids_3[,1]) \nsd_est &lt;- sd(resids_3[,1])\ncurve(dnorm(x, mean=mean_est, sd=sd_est), add=TRUE, col=\"red\", lwd=2) \nqqnorm(resids_3[,1], main=\"\")\nqqline(resids_3[,1], col=\"red\") \n\n\n\n\n\n\n\nshapiro.test(resids_3[,1])\n\n\n    Shapiro-Wilk normality test\n\ndata:  resids_3[, 1]\nW = 0.98616, p-value = 0.8202\n\n\n\nlayout(matrix(1:2,nrow = 1, byrow = TRUE))\nhist(resids_3[,2], probability=TRUE, main=\"\",xlab=\"Residuals t=2\") \nmean_est &lt;- mean(resids_3[,2])\nsd_est &lt;- sd(resids_3[,2])\ncurve(dnorm(x, mean=mean_est, sd=sd_est), add=TRUE, col=\"red\", lwd=2) \nqqnorm(resids_3[,2], main=\"\")\nqqline(resids_3[,2], col=\"red\") \n\n\n\n\n\n\n\nshapiro.test(resids_3[,2])\n\n\n    Shapiro-Wilk normality test\n\ndata:  resids_3[, 2]\nW = 0.97649, p-value = 0.4151\n\n\n\nlayout(matrix(1:2,nrow = 1, byrow = TRUE))\nhist(resids_3[,3], probability=TRUE, main=\"\",xlab=\"Residuals t=3\") \nmean_est &lt;- mean(resids_3[,3])\nsd_est &lt;- sd(resids_3[,3])\ncurve(dnorm(x, mean=mean_est, sd=sd_est), add=TRUE, col=\"red\", lwd=2) \nqqnorm(resids_3[,3], main=\"\")\nqqline(resids_3[,3], col=\"red\") \n\n\n\n\n\n\n\nshapiro.test(resids_3[,3])\n\n\n    Shapiro-Wilk normality test\n\ndata:  resids_3[, 3]\nW = 0.97556, p-value = 0.3833\n\n\nIn this case, the above histograms and Q-Q normal plots, as well as the Shapiro-Wilk tests, indicate nothing against the normality assumption of residuals. We next evaluate the normality of the random effects. Luckily, the function eblupMFH3 provides the random effects in its output, for the three time periods:\n\nran_eff &lt;- m3$randomEffect \nlayout(matrix(1:2,nrow = 1, byrow = TRUE))\nhist(ran_eff$Y1, probability=TRUE, main=\"\",xlab=\"Random Effects\", ylim = c(0,0.45))\nmean_est &lt;- mean(ran_eff$Y1) \nsd_est &lt;- sd(ran_eff$Y1)\ncurve(dnorm(x, mean=mean_est, sd=sd_est), add=TRUE, col=\"red\", lwd=2) \nqqnorm(ran_eff$Y1, main=\"\")\nqqline(ran_eff$Y1, col=\"red\") \n\n\n\n\n\n\n\nshapiro.test(ran_eff$Y1)\n\n\n    Shapiro-Wilk normality test\n\ndata:  ran_eff$Y1\nW = 0.97539, p-value = 0.3778\n\n\n\nlayout(matrix(1:2,nrow = 1, byrow = TRUE)) \nhist(ran_eff$Y2,probability=TRUE,main=\"\",xlab=\"Random Effects\",breaks=5) \nmean_est &lt;- mean(ran_eff$Y2)\nsd_est &lt;- sd(ran_eff$Y2)\ncurve(dnorm(x, mean=mean_est, sd=sd_est), add=TRUE, col=\"red\", lwd=2) \nqqnorm(ran_eff$Y2, main=\"\")\nqqline(ran_eff$Y2, col=\"red\") \n\n\n\n\n\n\n\nshapiro.test(ran_eff$Y2)\n\n\n    Shapiro-Wilk normality test\n\ndata:  ran_eff$Y2\nW = 0.95267, p-value = 0.04398\n\n\n\nlayout(matrix(1:2,nrow = 1, byrow = TRUE))\nhist(ran_eff$Y3, probability=TRUE, main=\"\",xlab=\"Random Effects\") \nmean_est &lt;- mean(ran_eff$Y3)\nsd_est &lt;- sd(ran_eff$Y3)\ncurve(dnorm(x, mean=mean_est, sd=sd_est), add=TRUE, col=\"red\", lwd=2) \nqqnorm(ran_eff$Y3, main=\"\")\nqqline(ran_eff$Y3, col=\"red\") \n\n\n\n\n\n\n\nshapiro.test(ran_eff$Y3)\n\n\n    Shapiro-Wilk normality test\n\ndata:  ran_eff$Y3\nW = 0.97513, p-value = 0.3692\n\n\nAgain, histograms and Q-Q normal plots show no evidences of departure from normality, while the Shapiro-Wilk test also supports normality for all time periods at the usual significance level of 0.05, except for t = 2, which is supported at 0.01 level.\nStep 5: Since no indications are found against the MFH3 model assumptions, we proceed to obtain the small area estimates, as well as their estimated MSEs. These are already been generated at output of the function together with the model fit, and can be printed as follows. \\(m2\\)eblup$ # To see the EBLUPs \\(m2\\)MSE$ # To see estimated MSEs of EBLUPs\nWe next illustrate the use of the function eblupMFH2, which should be used in the case of homoscedastic area-time effects over time. For this, we employ datasae2 from the R package msae (Permatasari and Ubaidillah, 2022). In this new data set, again X1 and X2 are the auxiliary variables, and Y1, Y2, and Y3 are the direct estimates at times 1, 2, and 3, respectively. The elements of the variance-covariance matrix of the sampling errors are provided in the variables v1, v2, v3, v12, v13, and v23. Again, we first load the package and the data set:\n\nlibrary(msae) \ndata(datasae2)\n\nWe now fit model MFH2 and check the model assumptions:\n\nFo &lt;- list(f1=Y1~X1+X2,\nf2=Y2~X1+X2, f3=Y3~X1+X2)\nvardir &lt;- c(\"v1\", \"v2\", \"v3\", \"v12\", \"v13\", \"v23\") \nm2 &lt;- eblupMFH2(Fo, vardir, data=datasae2) \nm2$eblup    # To see the EBLUPs\n\n       Y1     Y2     Y3\n1  133.11 132.45 132.43\n2  121.38 120.53 119.37\n3  121.36 121.43 121.42\n4  117.38 118.03 117.13\n5  125.11 124.95 124.39\n6  126.60 126.44 126.24\n7  121.63 122.53 123.14\n8  128.87 128.24 128.38\n9  133.72 135.14 134.88\n10 120.21 120.50 120.57\n11 127.48 126.93 127.01\n12 127.75 127.83 127.38\n13 126.17 126.28 126.36\n14 117.20 117.41 117.59\n15 124.88 124.13 123.33\n16 119.66 119.59 119.68\n17 118.07 117.70 118.28\n18 126.11 126.26 126.68\n19 117.92 118.28 118.59\n20 130.23 130.05 129.84\n21 120.79 121.42 122.09\n22 128.44 128.28 128.52\n23 124.22 124.13 123.72\n24 124.81 123.99 124.32\n25 123.56 122.82 122.43\n26 129.03 129.27 129.49\n27 124.09 124.13 124.39\n28 124.12 124.24 125.02\n29 119.82 120.62 120.53\n30 119.91 119.44 118.61\n31 125.20 124.70 123.07\n32 127.43 128.19 128.86\n33 128.02 127.48 127.50\n34 131.37 131.94 131.90\n35 126.15 125.13 125.67\n36 122.84 123.86 124.25\n37 115.91 115.59 115.63\n38 122.95 123.40 122.79\n39 119.37 119.46 119.31\n40 123.10 124.22 123.90\n41 125.85 125.66 124.47\n42 120.57 121.42 121.42\n43 122.73 122.24 122.52\n44 126.63 125.68 125.10\n45 126.39 126.83 126.47\n46 126.73 127.06 128.06\n47 125.50 126.05 127.22\n48 129.26 128.67 128.50\n49 121.72 122.23 121.97\n50 133.12 132.55 132.80\n\nm2$MSE  # To see the estimated MSEs of EBLUPs\n\n         Y1      Y2      Y3\n1  0.091458 0.16358 0.23077\n2  0.091516 0.16382 0.23122\n3  0.090675 0.16032 0.22464\n4  0.091326 0.16303 0.22974\n5  0.090705 0.16045 0.22488\n6  0.091248 0.16270 0.22913\n7  0.090540 0.15976 0.22359\n8  0.090636 0.16016 0.22434\n9  0.091453 0.16356 0.23073\n10 0.091002 0.16168 0.22720\n11 0.090798 0.16084 0.22561\n12 0.090721 0.16051 0.22500\n13 0.090585 0.15995 0.22394\n14 0.091286 0.16286 0.22943\n15 0.090624 0.16011 0.22424\n16 0.091357 0.16316 0.22998\n17 0.090914 0.16132 0.22652\n18 0.090681 0.16035 0.22469\n19 0.090854 0.16107 0.22604\n20 0.091048 0.16187 0.22756\n21 0.090902 0.16127 0.22642\n22 0.090761 0.16068 0.22532\n23 0.090572 0.15989 0.22384\n24 0.090554 0.15982 0.22370\n25 0.091682 0.16451 0.23253\n26 0.090862 0.16110 0.22611\n27 0.090613 0.16007 0.22416\n28 0.090568 0.15988 0.22380\n29 0.091143 0.16227 0.22831\n30 0.091106 0.16211 0.22801\n31 0.090708 0.16046 0.22491\n32 0.090695 0.16041 0.22480\n33 0.090834 0.16099 0.22589\n34 0.091157 0.16233 0.22842\n35 0.090599 0.16001 0.22405\n36 0.090701 0.16043 0.22485\n37 0.091466 0.16361 0.23084\n38 0.091331 0.16305 0.22978\n39 0.090977 0.16158 0.22701\n40 0.091079 0.16200 0.22781\n41 0.091038 0.16183 0.22749\n42 0.090656 0.16025 0.22450\n43 0.090963 0.16152 0.22690\n44 0.090697 0.16042 0.22482\n45 0.090576 0.15991 0.22387\n46 0.090632 0.16015 0.22431\n47 0.091277 0.16283 0.22936\n48 0.092014 0.16589 0.23512\n49 0.090601 0.16002 0.22407\n50 0.091675 0.16448 0.23247",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multivariate Fay–Herriot</span>"
    ]
  },
  {
    "objectID": "fh_multivariate.html#spatio-temporal-fh-estimators-over-t-time-periods",
    "href": "fh_multivariate.html#spatio-temporal-fh-estimators-over-t-time-periods",
    "title": "4  Multivariate Fay–Herriot",
    "section": "4.2 Spatio-temporal FH estimators over T time periods",
    "text": "4.2 Spatio-temporal FH estimators over T time periods\nThis section describes an extension of the FH model by Marhuenda et al. (2013), which incorporates spatial correlation between neighboring areas and temporal correlation over T time instants, leading to more stable small area estimates over time.\nSTFH estimation procedure for T time instants: Step 1 Compute the selected direct area estimators for each area, d = 1, . . . , D, and for each time t = 1, . . . , T , and estimators of their corresponding sampling variances.\nStep 2 Select the area-level auxiliary variables for each time instant in the STFH model. A simple approach is to perform a model selection procedure in a linear regression model without the area effects for each time instant t = 1, . . . , T .\nStep 3 Fit the STFH model and check the model assumptions, including linearity, normality of predicted area effects and standardized model residuals, and the presence of outlying areas, for each time instant T .\nStep 4 In case of clear systematic model departures, the model should be changed. In case of isolated departures because of outlying areas, either do not obtain the STFH estimate for those areas or change some aspect of the model or the data. Then, go to Step 2.\nStep 5 If model assumptions hold, using the above direct estimates, their estimated sampling variances, and the selected auxiliary variables, compute STFH estimators for each area d = 1, . . . , D and each time t = 1, . . . , T , and their corresponding estimated MSEs.\nEBLUPs for all areas and time instants, and parametric bootstrap MSE estimates can be obtained calling functions eblupSTFH() and pbmseSTFH() respectively. The calls to these functions are: \\(eblupSTFH(formula, D, T, vardir, proxmat, model = \"ST\", MAXITER =100, PRECISION = 0.0001, data)\\) \\(pbmseSTFH(formula, D, T, vardir, proxmat, B = 100, model = \"ST\", MAXITER = 100, PRECISION = 0.0001, data)\\)\nSome of the arguments are exactly the same as in the functions for FH model described in Section 4. Among the additional arguments, we have the number of areas D and the number of time periods T for each area. We remark that these functions may be used only when data are available for all the D domains at all T time periods. Moreover, data in formula and vardir must be sorted in ascending order by time instant, for each domain. Note that a single formula is specified in this function, unlike in the functions for the MFH modes of Section 5. The argument model can be chosen between the default value ST (AR(1) time-effects within each domain) or value S (with uncorrelated time effects within each domain). The rwo-standardized proximity matrix, W, must be also given as input in proxmat. The elements of this matrix are in [0,1], zeros on the diagonal and rows adding up to 1, as described above.\nThe function pbmseSTFH() providing bootstrap MSE estimates requires additionally to specify the number of bootstrap replicates B. A number of bootstrap replicates B ≥ 400 is advisable to achieve stable MSE estimates. By default, the argument B is set to 100 to save computing time. To obtain the same MSE estimates every time the function pbmseSTFH() is run, the seed for random number generation should be fixed previously using set.seed().\nExample 2 illustrates the calculation of STFH estimators of area poverty rates for T time instants, using the above functions.\n\n4.2.1 Example 4 (Spatio-temporal FH estimators of poverty rates, in R)\nIn this example, we use the data set spacetime included in the R package sae, which contains synthetic area level data for T = 3 time points, for each of D = 11 areas. The data set contains the following variables: Area, area code, Time, time point, X1 and X2, the auxiliary variables for each area and time instant, Y2, direct estimates for each area and time instant, and Var, sampling variances of the direct estimators. We calculate EBLUPs of the means for each area at each time, based on the STFH model with prox- imity matrix given in the data set spacetimeprox. We also obtain the corresponding MSE estimates by parametric bootstrap. The steps of the STFH procedure described above should be followed but, in this example, we only illustrate the calculation of the small area estimators and their estimated MSEs.\nWe first load the two data sets and obtain the number of areas and of time instants. Then, we apply the function pbmseSTFH() that delivers both, the STFH estimates and their estimated MSEs:\n\nlibrary(sae)\ndata(\"spacetime\") \ndata(\"spacetimeprox\")\nD &lt;- nrow(spacetimeprox)    # number of areas\nT &lt;- length(unique(spacetime$Time)) # number of time periods set.seed(123)\nSTFH &lt;- pbmseSTFH(Y ~ X1 + X2, D, T, vardir = Var, spacetimeprox, data = spacetime)\n\n\nBootstrap procedure with B = 100 iterations starts.\nb = 1 \nb = 2 \nb = 3 \nb = 4 \nb = 5 \nb = 6 \nb = 7 \nb = 8 \nb = 9 \nb = 10 \nb = 11 \nb = 12 \nb = 13 \nb = 14 \nb = 15 \nb = 16 \nb = 17 \nb = 18 \nb = 19 \nb = 20 \nb = 21 \nb = 22 \nb = 23 \nb = 24 \nb = 25 \nb = 26 \nb = 27 \nb = 28 \nb = 29 \nb = 30 \nb = 31 \nb = 32 \nb = 33 \nb = 34 \nb = 35 \nb = 36 \nb = 37 \nb = 38 \nb = 39 \nb = 40 \nb = 41 \nb = 42 \nb = 43 \nb = 44 \nb = 45 \nb = 46 \nb = 47 \nb = 48 \nb = 49 \nb = 50 \nb = 51 \nb = 52 \nb = 53 \nb = 54 \nb = 55 \nb = 56 \nb = 57 \nb = 58 \nb = 59 \nb = 60 \nb = 61 \nb = 62 \nb = 63 \nb = 64 \nb = 65 \nb = 66 \nb = 67 \nb = 68 \nb = 69 \nb = 70 \nb = 71 \nb = 72 \nb = 73 \nb = 74 \nb = 75 \nb = 76 \nb = 77 \nb = 78 \nb = 79 \nb = 80 \nb = 81 \nb = 82 \nb = 83 \nb = 84 \nb = 85 \nb = 86 \nb = 87 \nb = 88 \nb = 89 \nb = 90 \nb = 91 \nb = 92 \nb = 93 \nb = 94 \nb = 95 \nb = 96 \nb = 97 \nb = 98 \nb = 99 \nb = 100 \n\n\nThe bootstrap procedure for MSE estimation displays the iteration number for each step, as follows: Bootstrap procedure with B = 100 iterations starts. Once we have obtained the STFH estimators, we compute their CVs, and the same is done for the direct estimators. We print the results for the last time instant (T = 3):\n\ncv.STFH &lt;- 100 * sqrt(STFH$mse) / STFH$est$eblup \ncv.DIR &lt;- 100 * sqrt(spacetime$Var) / spacetime$Y\nresults &lt;- data.frame(Area = spacetime$Area, Time = spacetime$Time,\nDIR = spacetime$Y, eblup.STFH = STFH$est$eblup, cv.DIR, cv.STFH)\nresults.lasttime &lt;- results[results$Time == 3, ] \nprint(results.lasttime, row.names = FALSE)\n\n Area Time      DIR eblup.STFH    cv.DIR   cv.STFH\n    2    3 0.261484 0.27343181 10.944523  7.251764\n    3    3 0.175358 0.17722992  7.777336  6.851550\n    8    3 0.096230 0.09653879  6.059391  6.153638\n   12    3 0.122160 0.13740348 21.904205 14.736419\n   13    3 0.294176 0.29129477  8.812059  5.508984\n   16    3 0.412106 0.31887378 13.584403  7.220082\n   17    3 0.057924 0.06912566 25.195980 21.078426\n   25    3 0.209146 0.17377084 15.411972 13.717529\n   43    3 0.148671 0.14398844 15.788815 13.157155\n   45    3 0.234361 0.22810227  9.550663  8.284190\n   46    3 0.137869 0.14354272  8.853735  7.992519\n\n\nNext we plot the STFH and the direct estimates together for comparison. Additionally, we plot their corresponding CVs. The following R code generates both plots:\n\nlayout(1)\nresults.lasttime &lt;- results.lasttime[order(results.lasttime$cv.DIR), ] \nplot(results.lasttime$DIR, type = \"n\", xlab = \"area (time=3)\", ylab = \"Estimate\",ylim = c(0.05, 0.45), cex.axis = 1.5, cex.lab = 1.5, xaxt = \"n\")\naxis(1, 1:11, results.lasttime$Area, cex.axis = 1.5) \npoints(results.lasttime$DIR, type = \"b\", col = 1, lwd = 2, pch = 1) \npoints(results.lasttime$eblup.STFH, type = \"b\", col = 4,lwd = 2,pch = 4) \nlegend(\"top\", legend = c(\"Direct\", \"EBLUP STFH\"), ncol = 2, col = c(1, 4), lwd = rep(2, 2), pch = c(1, 4), cex = 1.3)\n\n\n\n\n\n\n\nplot(results.lasttime$cv.DIR, type = \"n\", xlab = \"area (time=3)\", ylab=\"CV\",cex.axis = 1.5, cex.lab = 1.5, xaxt = \"n\")\naxis(1, 1:11, results.lasttime$Area, cex.axis = 1.5) \npoints(results.lasttime$cv.DIR, type = \"b\", col = 1, lwd = 2, pch = 1) \npoints(results.lasttime$cv.STFH, type = \"b\", col = 4, lwd = 2, pch = 4) \nlegend(\"top\", legend = c(\"Direct\", \"EBLUP STFH\"), ncol = 2, col=c(1,4), lwd = rep(2, 2), pch = c(1, 4), cex = 1.3)\n\n\n\n\n\n\n\n\nThe left figure shows the STFH estimators, together with the direct estimates for each area at the last time point, with areas sorted by increasing CVs of direct estimators. The right figure shows the corresponding CVs. In this example, we can see that, even with a very small number of areas (D = 11) and time instants (T = 3) to borrow strength from, the STFH estimates follow closely direct estimates, but are slightly more stable than them, and have smaller estimated CVs for all the areas.\nWe also apply basic univariate FH models for each time instant, that is, crosssectionally, to analyze the differences with the results obtained from the STFH model. For this, we first select the data corresponding to each time instant and then call the function mseFH() using those separate data sets. STFH estimates are stored in an additional column of the data frame results:\n\ndata.time1&lt;-spacetime[spacetime$Time==1,] \ndata.time2&lt;-spacetime[spacetime$Time==2,] \ndata.time3&lt;-spacetime[spacetime$Time==3,]\neblup.FH.res.time1&lt;-mseFH(Y~X1+X2,vardir=Var,data=data.time1) \neblup.FH.time1&lt;-eblup.FH.res.time1$est$eblup \nresults$eblup.FH[results$Time==1]&lt;-eblup.FH.time1\neblup.FH.res.time2&lt;-mseFH(Y~X1+X2,vardir=Var,data=data.time2) \neblup.FH.time2&lt;-eblup.FH.res.time2$est$eblup \nresults$eblup.FH[results$Time==2]&lt;-eblup.FH.time2\neblup.FH.res.time3&lt;-mseFH(Y~X1+X2,vardir=Var,data=data.time3) \neblup.FH.time3&lt;-eblup.FH.res.time3$est$eblup \nresults$eblup.FH[results$Time==3]&lt;-eblup.FH.time3\n\nLet us now plot the STFH estimates, together with direct and cross-sectional FH estimates, for the first area (coded as Area=2) over the three time instants:\n\nresults.A1 &lt;- results[results$Area == 2, ]\nlayout(1) \nk&lt;-3\nm&lt;-min(results.A1$DIR,results.A1$eblup.STFH,results.A1$eblup.FH) \nM&lt;-max(results.A1$DIR,results.A1$eblup.STFH,results.A1$eblup.FH) \nplot(1:3,results.A1$DIR, type = \"n\", xlab = \"Time\",\nylab = \"Estimates: First area\", ylim = c(m,M+(M-m)/k), cex.axis = 1.5, cex.lab = 1.5, xaxt = \"n\")\naxis(1, 1:3, 1:3, cex.axis = 1.5)\npoints(1:3,results.A1$DIR, type = \"b\", col = 1, lwd = 2, pch = 1) \npoints(1:3,results.A1$eblup.FH, type = \"b\", col = 3,lwd = 2,pch = 3) \npoints(1:3,results.A1$eblup.STFH, type = \"b\", col = 4,lwd = 2,pch = 4) \nlegend(\"top\", legend = c(\"Direct\", \"EBLUP FH\", \"EBLUP STFH\"), ncol = 2,\ncol = c(1,3,4),lwd = rep(2,3), pch = c(1,3,4), cex = 1.3)\n\n\n\n\n\n\n\n\nFinally, we repeat the process for the last area (coded as Area=46):\n\nresults.A11 &lt;- results[results$Area == 46, ]\nlayout(1) \nk&lt;-3\nm&lt;-min(results.A11$DIR,results.A11$eblup.STFH,results.A11$eblup.FH) \nM&lt;-max(results.A11$DIR,results.A11$eblup.STFH,results.A11$eblup.FH) \nplot(1:3,results.A11$DIR, type = \"n\", xlab = \"Time\",\nylab = \"Estimates: Last area\", ylim = c(m, M+(M-m)/k), cex.axis = 1.5, cex.lab = 1.5, xaxt = \"n\")\naxis(1, 1:3, 1:3, cex.axis = 1.5)\npoints(1:3,results.A11$DIR, type = \"b\", col = 1, lwd = 2, pch = 1) \npoints(1:3,results.A11$eblup.FH, type = \"b\", col = 3,lwd = 2,pch = 3) \npoints(1:3,results.A11$eblup.STFH, type = \"b\", col = 4,lwd = 2,pch = 4) \nlegend(\"top\", legend = c(\"Direct\", \"EBLUP FH\", \"EBLUP STFH\"), ncol = 2,\ncol = c(1,3,4), lwd = rep(2,3), pch = c(1,3,4), cex = 1.3)\n\n\n\n\n\n\n\n\nWe did already see in the previous examples that direct estimators are unstable across areas. We can see that they are also the most unstable over time. Cross-sectional FH estimators applied for each time instant t = 1, 2, 3, are more stable across areas, but not necessarily over time. Figure 14 shows that STFH estimators are smoother over time than both, direct and cross-sectional FH estimates.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Multivariate Fay–Herriot</span>"
    ]
  },
  {
    "objectID": "ul.html",
    "href": "ul.html",
    "title": "5  Unit-Level Models",
    "section": "",
    "text": "5.1 Introduction\nIn this section, we will present an implementation of the Empirical Best Prediction (EBP) unit modelling approach in R based on World Bank’s Small Area Estimation Guidelines, (Corral et al. 2022), as well as (Molina and Rao 2010) and (Rao and Molina 2015). Similar to the disclaimer made in the previous section, this practical manual does not present a theoretical statistical primer on unit level poverty mapping. Rather, it presents a combination of practical and simple R scripts with appropriate explanations to expose the simplicity of performing these tasks in R to the user. The studies previously cited are excellent starting points for those interested in understanding the theoretical underpinnings for the code being presented here.\nThis chapter is subdivided as follows to show the whole game of the EBP linear mixed effect modelling approach:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Unit-Level Models</span>"
    ]
  },
  {
    "objectID": "ul.html#introduction",
    "href": "ul.html#introduction",
    "title": "5  Unit-Level Models",
    "section": "",
    "text": "The Data: In this subsection, we present a brief checklist of data items needed for the unit level poverty mapping as well as present the specific data to be used in this chapter.\nData Preparation: Here we present the process of transforming the data in\ninto what is needed for variable selection and then estimating a unit level poverty map\nVariable Selection: We present the LASSO methodology of the GLMNET, glmmLasso and hdm R packages as well as another implementation that uses the stepwise model\nEBP Unit Level Model Estimation: Having selected the set of variables, we proceed to use the povmap package’s povmap::ebp() function to estimate the poverty map.\nPost Estimation Diagnostics: We proceed to test model assumptions of the EBP linear mixed effects model and present functions within the povmap package for producing report ready figure and tables.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Unit-Level Models</span>"
    ]
  },
  {
    "objectID": "ul.html#the-data",
    "href": "ul.html#the-data",
    "title": "5  Unit-Level Models",
    "section": "5.2 The Data",
    "text": "5.2 The Data\nThe main idea of SAE is to combine multiple data sources. Typically, there is a survey data set and a census or administrative/register dataset both at an individual and/or household unit level. The target variable (typically household welfare/income for poverty mapping) is available in the survey but not in the census data. The goal of the exercise is to estimate welfare/income for each unit within the census or administrative register dataset by developing a model of welfare or income within the survey. It is important that the outcome variable has the same definition within the census or administrative dataset as the case maybe. It would be inappropriate to estimate household welfare/income within the survey and use the same model to predict income at the individual level of the census. Below is a brief checklist of data requirements needed for unit level poverty mapping with the povmap R package:\n\nA unit level survey data.frame object with individual and household characteristics including the target area variable, outcome variable (welfare aggregates, income per capita etc)\nA unit census/administrative register dataframe object with individual and household characteristics including the target area variable. The outcome variable is typically missing since the topic of this tutorial is estimating it.\n\nFor the purposes of this tutorial, we will use the European Union Statistics on Income and Living Conditions (EU-SILC) in Austria from 2006. Please see (Kreutzmann et al. 2019) for the full description of the dataset\n\n### load the data\nsurvey_dt &lt;- eusilcA_smp |&gt; as_tibble()\n\n## ignore the eqIncome variable that has been left in the eusilcA_pop dataset\ncensus_dt &lt;- eusilcA_pop |&gt; as_tibble()\n\n#### the survey dataset\nglimpse(survey_dt)\n\nRows: 1,945\nColumns: 18\n$ eqIncome    &lt;dbl&gt; 23485.320, 22704.460, 25946.340, 16152.538, 22587.480, 204…\n$ gender      &lt;fct&gt; male, male, female, male, male, male, female, male, female…\n$ eqsize      &lt;dbl&gt; 2.5, 2.0, 1.0, 2.1, 1.5, 2.0, 1.5, 2.0, 1.8, 1.5, 1.5, 1.0…\n$ cash        &lt;dbl&gt; 33003.39, 20073.23, 0.00, 19972.35, 21503.23, 22282.23, 21…\n$ self_empl   &lt;dbl&gt; 0.00, 0.00, 24736.06, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, …\n$ unempl_ben  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ age_ben     &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 15219.13, 3956.04, 0.0…\n$ surv_ben    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ sick_ben    &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 3016.55, 0.00, 0.00, 0…\n$ dis_ben     &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ rent        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ fam_allow   &lt;dbl&gt; 0.00, 0.00, 0.00, 5247.33, 2031.03, 0.00, 0.00, 0.00, 1860…\n$ house_allow &lt;dbl&gt; 0.00, 0.00, 1083.95, 0.00, 0.00, 3312.42, 0.00, 0.00, 0.00…\n$ cap_inv     &lt;dbl&gt; 17158.84, 4.70, 126.33, 195.32, 19.92, 0.00, 5.96, 1.05, 0…\n$ tax_adj     &lt;dbl&gt; -10972.65, -940.77, 0.00, 0.00, 0.00, 0.00, 0.00, -526.30,…\n$ state       &lt;chr&gt; \"Burgenland\", \"Burgenland\", \"Burgenland\", \"Burgenland\", \"B…\n$ district    &lt;fct&gt; Neusiedl am See, Neusiedl am See, Neusiedl am See, Neusied…\n$ weight      &lt;dbl&gt; 9.6875, 9.6875, 9.6875, 9.6875, 9.6875, 9.6875, 9.6875, 9.…\n\n#### the census dataset\nglimpse(census_dt)\n\nRows: 25,000\nColumns: 17\n$ eqIncome    &lt;dbl&gt; 81304.78, 78017.33, 70096.30, 66120.34, 65166.87, 66703.96…\n$ gender      &lt;fct&gt; female, female, female, male, female, male, female, female…\n$ eqsize      &lt;dbl&gt; 2.1, 2.0, 1.8, 1.5, 2.0, 1.8, 2.3, 3.0, 1.8, 2.1, 1.5, 1.5…\n$ cash        &lt;dbl&gt; 51722.85, 0.00, 16259.90, 0.00, 14272.49, 99867.41, 22551.…\n$ self_empl   &lt;dbl&gt; 0.00, 24399.91, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, …\n$ unempl_ben  &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ age_ben     &lt;dbl&gt; 0.00, 0.00, 0.00, 51131.52, 0.00, 0.00, 0.00, 0.00, 0.00, …\n$ surv_ben    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ sick_ben    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ dis_ben     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ rent        &lt;dbl&gt; 90994.14, 104914.06, 0.00, 0.00, 54269.12, 0.00, 93416.01,…\n$ fam_allow   &lt;dbl&gt; 34641.47, 0.00, 28256.15, 0.00, 35017.15, 5036.37, 3778.09…\n$ house_allow &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 3554…\n$ cap_inv     &lt;dbl&gt; 0.00, 262.29, 64799.93, 1983.63, 0.00, 388.84, 4865.42, 0.…\n$ tax_adj     &lt;dbl&gt; 40947.36, -349.71, -1126.10, 0.00, -376.87, 0.00, -5838.50…\n$ state       &lt;chr&gt; \"Vorarlberg\", \"Vorarlberg\", \"Vorarlberg\", \"Vorarlberg\", \"V…\n$ district    &lt;fct&gt; Bregenz, Bregenz, Bregenz, Bregenz, Bregenz, Bregenz, Breg…\n\n\n\nAll target areas within the survey must be present within the census.\nThe emdi::ebp() and povmap::ebp() function calls will result in an error message if values in the target area variable are missing within the survey, this includes NA values within the survey target area column that are not in the census’ target area column.\n\n\n### counting the number of districts (i.e. target areas) \n### within the survey that are not a subset of the census districts. \n\n### When 0 is returned below: there are no areas as such\nsurvey_dt |&gt; anti_join(census_dt, join_by(district)) |&gt; nrow()\n\n[1] 0\n\n\nOther important data for poverty mapping include:\n\nA shapefile of the sub-national boundaries.\nWhile a table of poverty rates would suffice, a picture is worth more than a 1000 words and as such emdi and povmap have functionality for converting the resulting estimates into a map using the plot() function within the emdi and povmap packages. It is essential that the target area variable in the census and survey (which by now should be consistent between the survey and census) should also match with the target area variable within the shapefile in order to use the plot() function within emdi or povmap.\n\nOnce the following steps have been taken, the next stage is to prepare the set of variables for estimating a model of household welfare and predicting the consumption aggregates into the census.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Unit-Level Models</span>"
    ]
  },
  {
    "objectID": "ul.html#data-preparation-for-unit-level-model",
    "href": "ul.html#data-preparation-for-unit-level-model",
    "title": "5  Unit-Level Models",
    "section": "5.3 Data Preparation for unit level model",
    "text": "5.3 Data Preparation for unit level model\nIn this section, we describe a few common techniques for creating variables with strong correlations to the outcome variable. This includes creating:\n\nvariable interactions\ntarget area average variables\ndummy variables (at the lowest level of national representation, regional dummies, as well as other dummy variables to capture some non-linear relationships between the certain variables and the outcome variable)\n\nFirst a little bit of housekeeping is in order:\n\n### subset the set of candidate variables into a character vector\ncandidate_vars &lt;- survey_dt |&gt; \n  dplyr::select(-any_of(c(\"eqIncome\", \"weight\", \"state\", \"district\"))) |&gt; \n  names()\n\n### we have to ensure candidate variables are numbers:\n###   (numeric or integer class)\n### the only variable that does not meet this criteria is the \n### gender variable\nsurvey_dt &lt;- survey_dt |&gt;\n  mutate(gender = ifelse(gender == \"female\", 1, 0))\n\ncensus_dt &lt;- census_dt |&gt;\n  mutate(gender = ifelse(gender == \"female\", 1, 0))\n\n\n5.3.1 Creating Variable Interactions\nThe create_interactions() function employed in the subsequent code block interacts a specified variable interacter_var with a list of variables of interest var_list. This should be applied to both census and survey data to ensure the same variables are created in both instances.\n\n### show the function we have used to automate interactions between variables\n\n#' A function to interact a variables with a set of variables\n#' \n#' @param dt a data.frame\n#' @param interacter_var the variable to be interacted\n#' @param var_list the set of variables to interact with\n#' \n#' @export\n#' \ncreate_interactions &lt;- function(dt, interacter_var, var_list) {\n  # Ensure dt is a data.table\n  if (!\"data.frame\" %in% class(dt)) {\n    dt &lt;- as.data.table(dt)\n  }\n  \n  # Check if interacter_var exists in the dataset\n  if (!(interacter_var %in% names(dt))) {\n    stop(paste(interacter_var, \"not found in dataset\"))\n  }\n  \n  # Check if var_list contains valid variables that exist in the dataset\n  if (any(!var_list %in% names(dt))) {\n    stop(\"Some variables in var_list are not found in the dataset.\")\n  }\n  \n  # Create an empty data.table to store interactions\n  int_dt &lt;- data.frame(matrix(nrow = nrow(dt)))\n  \n  # Loop over var_list to create interaction terms\n  for (var in var_list) {\n    interaction_name &lt;- paste0(var, \"_X_\", interacter_var)\n    int_dt[[interaction_name]] &lt;- dt[[var]] * dt[[interacter_var]]\n  }\n  \n  int_dt &lt;- int_dt[, -1]\n  return(int_dt)\n}\n\n\nsurvey_dt &lt;- survey_dt |&gt; \n  bind_cols(\n    create_interactions(\n      dt = survey_dt,\n      interacter_var = \"gender\",\n      var_list = candidate_vars[!candidate_vars %in% \"gender\"]\n      ) |&gt; \n      as_tibble()\n    )\n\ncensus_dt &lt;- census_dt |&gt; \n  bind_cols(\n    create_interactions(\n      dt = census_dt,\n      interacter_var = \"gender\",\n      var_list = candidate_vars[!candidate_vars %in% \"gender\"]\n      ) |&gt; \n      as_tibble()\n    )\n\n\n\n5.3.2 Computing target area averages and regional dummies\nIt is often useful to compute target area averages to improve the explanation of intra-area variation in the dependent variable. Below is some efficient code that employs the power of the data.table package to compute the averages and simultaneously include them within the census and survey datasets.\nIn addition, creating dummy variables for the lowest resolution administrative division which is also national representative may help control for some unobserved regional heterogeneity, improving model fit and prediction accuracy, capture spatial dependence in welfare patterns. In rear cases, it should be noted that dummy variables with too few observations in any class might introduce high multicollinearity\n\n#### compute target area level averages to include in the model\ncandidate_vars &lt;- survey_dt |&gt;\n  select(any_of(candidate_vars), contains(\"_X_\")) |&gt;\n  names()\n\nsurvey_dt &lt;- \n  survey_dt |&gt; \n  group_by(district) |&gt; \n  mutate(\n    across(\n      any_of(candidate_vars),\n      ~ weighted.mean(x = ., w = weight, na.rm = TRUE), \n      .names = \"{.col}_targetmean\"\n    )\n  ) |&gt; \n  ungroup()\n\ncensus_dt &lt;-\n  census_dt |&gt; \n  group_by(district) |&gt; \n  mutate(\n    across(\n      any_of(candidate_vars),\n      ~ weighted.mean(x = ., na.rm = TRUE), \n      .names = \"{.col}_targetmean\"\n    )\n  ) |&gt; \n  ungroup()\n\n#### create regional dummies\nsurvey_dt &lt;-\n  survey_dt |&gt;\n  mutate(dummy = 1, state2 = state) |&gt;\n  pivot_wider(names_from = state2,\n              names_prefix = \"state_\",\n              values_from = dummy,\n              values_fill = 0) |&gt;\n  rename_with(~ str_replace(., \" \", \"\"), starts_with(\"state_\")) \n\ncensus_dt &lt;-\n  census_dt |&gt;\n  mutate(dummy = 1, state2 = state) |&gt;\n  pivot_wider(names_from = state2,\n              names_prefix = \"state_\",\n              values_from = dummy,\n              values_fill = 0) |&gt;\n  rename_with(~ str_replace(., \" \", \"\"), starts_with(\"state_\"))\n\n#### lets update the set of candidate variables\ncandidate_vars &lt;- survey_dt |&gt; \n  select(any_of(candidate_vars), contains(\"state_\")) |&gt; \n  names()\n\nIn rear cases, it should be noted that dummy variables with too few observations in any class might introduce high multicollinearity or reduce the degrees of freedom.\n\n\n5.3.3 Final Preparations for Variable Selection\nCertain diagnostics checks ought to be performed before the variable selection process is implemented. First, we check the distribution of the non-transformed outcome variable. In the case of income or household consumption, it is typically expected and preferred that the income/household consumption variable can be transformed into a normally distributed random variable. We apply the log and boxcox transformations as seen in the code below:\n\n#### let's create the set of outcome variables for the different potential\n#### income transformations we might be interested in doing\n\n###### log transformation\nsurvey_dt &lt;- \n  survey_dt |&gt;\n  mutate(logeqIncome = log(eqIncome))\n\n###### boxcox transformation\n#### apply box-cox transformation\nboxcox_result &lt;- MASS::boxcox(lm(eqIncome ~ 1, data = survey_dt), \n                              lambda = seq(-2, 2, by = 0.1))\n\nlambda_opt &lt;- boxcox_result$x[which.max(boxcox_result$y)]\ncat(\"Optimal Lambda:\", lambda_opt, \"\\n\")\n\nOptimal Lambda: 0.3030303 \n\n## Apply the transformation manually\nif (lambda_opt == 0) {\n  survey_dt &lt;-\n    survey_dt |&gt;\n    mutate(bcxeqIncome = log(eqIncome))\n  \n} else {\n  survey_dt &lt;-\n    survey_dt |&gt;\n    mutate(bcxeqIncome = (eqIncome^lambda_opt - 1) / lambda_opt)\n  \n}\n\n#### compare the distributions of the outcome variables created\np1 &lt;-\n  survey_dt |&gt;\n  ggplot() +\n  geom_histogram(\n    aes(x = logeqIncome),\n    binwidth = 0.5,\n    fill = \"blue\",\n    color = \"black\"\n  ) +\n  labs(title = \"Log Income Distribution\") +\n  theme_minimal()\n\np2 &lt;-\n  survey_dt |&gt;\n  ggplot() +\n  geom_histogram(\n    aes(x = bcxeqIncome),\n    binwidth = 0.5,\n    fill = \"red\",\n    color = \"black\"\n  ) +\n  labs(title = \"Box-Cox Income Distribution\") +\n  theme_minimal()\n\np1 + p2\n\n\n\n\n\n\n\nFigure 5.1\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.2\n\n\n\n\n\nSeveral other transformations are also possible including the ordernorm transformation. See bestNormalize::orderNorm() documentation for more details.\nNext, we ensure the candidate variables in the survey and census come from similar distributions. One assumption of the linear mixed effects approach is that the variables selected are drawn from the same distribution.\nIn the following chunk, we apply the povmap::ebp_test_means() function to present a table of means for each survey and census and show the results of the standard z-tests. It is common to drop values that reject the null hypothesis (at the 95% level) that the survey variable (for any given variable) has a different mean from the same variable in the census.\n\ntest_dt &lt;- povmap::ebp_test_means(varlist = candidate_vars,\n                                  smp_data = survey_dt,\n                                  pop_data = census_dt,\n                                  weights = \"weight\")\ntest_dt |&gt; flextable() |&gt; colformat_double(big.mark = \"\", digits = 3)\n\n\n\nTable 5.1\n\n\n\nvariablesmp_meanspop_meansdiffpvalueage_ben5478.6105304.057-174.5530.990age_ben_X_gender2550.1852369.536-180.6490.984cap_inv487.266456.851-30.4140.994cap_inv_X_gender215.913205.825-10.0880.997cash12712.23312706.449-5.7841.000cash_X_gender2982.1243379.769397.6450.972dis_ben523.440542.32618.8850.997dis_ben_X_gender227.350257.05729.7070.991eqsize1.6101.603-0.0070.994eqsize_X_gender0.5090.5390.0300.977fam_allow1617.4881624.9587.4700.999fam_allow_X_gender442.459506.06863.6090.983gender0.3720.3910.0190.978house_allow61.82065.4143.5940.995house_allow_X_gender26.14923.319-2.8300.993rent608.004770.358162.3530.986rent_X_gender184.389387.367202.9780.973self_empl2039.1251966.965-72.1600.995self_empl_X_gender413.684495.88582.2010.987sick_ben61.83767.4515.6140.996sick_ben_X_gender31.42321.924-9.4990.987state_Burgenland0.0130.0320.0190.929state_Carinthia0.0680.0690.0010.999state_LowerAustria0.1660.1850.0190.972state_Salzburg0.0700.067-0.0030.993state_Styria0.1440.135-0.0080.987state_Tyrol0.0730.0760.0020.995state_UpperAustria0.1680.163-0.0050.992state_Vienna0.2550.234-0.0200.974state_Vorarlberg0.0430.039-0.0030.990surv_ben61.91888.95227.0350.984surv_ben_X_gender24.14244.82120.6790.982tax_adj-76.558-82.540-5.9820.998tax_adj_X_gender-18.444-31.138-12.6940.994unempl_ben463.508429.382-34.1260.990unempl_ben_X_gender207.672178.941-28.7300.987\n\n\n\n\n\nHowever, the ebp_test_means function doesn’t control for the fact that the variables are national representative at the state level. We implement another variant to the ebp_test_means that will allow users to cluster the standard errors at the state level.\n\n#' Perform test for difference between survey and census means \n#' \n#' This function computes weighted means of the same set of variables within the census and survey controlling for a cluster level. A test for \n#' difference of the means are performed for each variable with two-tailed p-values returned and the cluster level has to be specified for the\n#' estimation of variance\n#' \n#' @param varlist character vector, the set of variables of interest\n#' @param smp_data the survey data\n#' @param pop_data the population data\n#' @param weights a character string containing the name of a variable that indicates weights in the sample data. If a character string is \n#' provided a weighted version of the ebp will be used.\n#' @param pop_weights a character string containing the name of a variable that indicates population weights in the population data. If a \n#' If a character string is provided weighted indicators are estimated using population weights. The variable has to be numeric. \n#' Defaults to NULL. \n#' \n#' @import survey\n\n\nebp_test_means_clustered &lt;- function(varlist, \n                                     smp_data, \n                                     pop_data, \n                                     cluster, \n                                     weights = NULL, \n                                     pop_weights = NULL) {\n  \n  if (is.null(weights)) {\n    smp_data$weights &lt;- rep(1, nrow(smp_data))\n    weights &lt;- \"weights\"\n  }\n  if (is.null(pop_weights)) {\n    pop_data$pop_weights &lt;- rep(1, nrow(pop_data))\n    pop_weights &lt;- \"pop_weights\"\n  } else {\n    \n    pop_data$pop_weights &lt;- pop_data[[pop_weights]]\n    \n  }\n  \n  smp_df &lt;- smp_data[complete.cases(smp_data[, c(varlist, weights, cluster)]), c(varlist, weights, cluster)]\n  pop_df &lt;- pop_data[complete.cases(pop_data[, c(varlist, \"pop_weights\", cluster)]), c(varlist, \"pop_weights\", cluster)]\n  \n  smp_df &lt;- smp_df |&gt; mutate(across(all_of(varlist), as.numeric))\n  pop_df &lt;- pop_df |&gt; mutate(across(all_of(varlist), as.numeric))\n  \n  # Define survey design with clustering\n  smp_design &lt;- svydesign(id = ~get(cluster), weights = ~get(weights), data = smp_df)\n  pop_design &lt;- svydesign(id = ~get(cluster), weights = ~pop_weights, data = pop_df)\n  \n  smp_means_df &lt;- data.frame(\n    smp_means = sapply(varlist, function(var) svymean(as.formula(paste(\"~\", var)), smp_design)[1]),\n    smp_se = sapply(varlist, function(var) SE(svymean(as.formula(paste(\"~\", var)), smp_design))),\n    variable = varlist\n  )\n  \n  pop_means_df &lt;- data.frame(\n    pop_means = sapply(varlist, function(var) svymean(as.formula(paste(\"~\", var)), pop_design)[1]),\n    pop_se = sapply(varlist, function(var) SE(svymean(as.formula(paste(\"~\", var)), pop_design))),\n    variable = varlist\n  )\n  \n  means_df &lt;- merge(smp_means_df, pop_means_df, by = \"variable\")\n  means_df$diff &lt;- means_df$pop_means - means_df$smp_means\n  means_df$diff_se &lt;- sqrt(means_df$smp_se^2 + means_df$pop_se^2)\n  means_df$zscore &lt;- means_df$diff / means_df$diff_se\n  means_df$pvalue &lt;- 2 * (1 - pnorm(abs(means_df$zscore)))\n  \n  return(means_df[, c(\"variable\", \"smp_means\", \"pop_means\", \"diff\", \"pvalue\")])\n  \n}\n\n\ntest_dt &lt;- \n  ebp_test_means_clustered(\n    varlist = candidate_vars,\n    smp_data = survey_dt,\n    pop_data = census_dt,\n    cluster = \"state\",\n    weights = \"weight\"\n    )\ntest_dt |&gt; flextable() |&gt; colformat_double(big.mark = \"\", digits = 3)\n\n\n\nTable 5.2\n\n\n\nvariablesmp_meanspop_meansdiffpvalueage_ben5478.6105304.057-174.5530.550age_ben_X_gender2550.1852369.536-180.6490.387cap_inv487.266456.851-30.4140.726cap_inv_X_gender215.913205.825-10.0880.829cash12712.23312706.449-5.7840.991cash_X_gender2982.1243379.769397.6450.261dis_ben523.440542.32618.8850.786dis_ben_X_gender227.350257.05729.7070.346eqsize1.6101.603-0.0070.915eqsize_X_gender0.5090.5390.0300.353fam_allow1617.4881624.9587.4700.973fam_allow_X_gender442.459506.06863.6090.210gender0.3720.3910.0190.524house_allow61.82065.4143.5940.717house_allow_X_gender26.14923.319-2.8300.763rent608.004770.358162.3530.418rent_X_gender184.389387.367202.9780.078self_empl2039.1251966.965-72.1600.728self_empl_X_gender413.684495.88582.2010.356sick_ben61.83767.4515.6140.687sick_ben_X_gender31.42321.924-9.4990.249state_Burgenland0.0130.0320.0190.626state_Carinthia0.0680.0690.0010.996state_LowerAustria0.1660.1850.0190.935state_Salzburg0.0700.067-0.0030.977state_Styria0.1440.135-0.0080.966state_Tyrol0.0730.0760.0020.985state_UpperAustria0.1680.163-0.0050.981state_Vienna0.2550.234-0.0200.946state_Vorarlberg0.0430.039-0.0030.957surv_ben61.91888.95227.0350.357surv_ben_X_gender24.14244.82120.6790.138tax_adj-76.558-82.540-5.9820.844tax_adj_X_gender-18.444-31.138-12.6940.659unempl_ben463.508429.382-34.1260.678unempl_ben_X_gender207.672178.941-28.7300.514\n\n\n\n\n\nIn the next section, we fold the final checks into the variable selection process. We drop variables that are highly correlated and set a sparsity threshold. The sparsity threshold ensures that variables that do not have a certain proportion of non-missing observations are dropped. We have folded both these checks into the glmnetlasso_vselect() function.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Unit-Level Models</span>"
    ]
  },
  {
    "objectID": "ul.html#the-variable-selection-process",
    "href": "ul.html#the-variable-selection-process",
    "title": "5  Unit-Level Models",
    "section": "5.4 The Variable Selection Process",
    "text": "5.4 The Variable Selection Process\nThe glmnetlasso_vselect() function takes the dependent variable (welfare or income variable) as well as the set of potential candidate variables which just created and applies the lasso methodology (run here(glmnet::glmnet) for more details). This includes the option to perform a reproducible n-fold cross validation to reduce the risk of overfitting the model. In this instance, we have run applied this function to select the best predictors of household welfare based on the 3 welfare transformations i.e. 3 separate runs of the function, one for each transformation. See below:\n\n#' A function for variable selection using the GLMNET R package\n#' \n#' This function acts as a wrapper to the GLMNET R package which performs variable selection with nfold cross validation. \n#' \n#' @details This function cleans the data `dt` first by standardizing the predictors, dropping columns with missing values, very low variance (near\n#' zero variance), highly correlated variables based on a certain `correlation_threshold`. The function also drops variables with an high percentage #' of zeros based on a `sparsity_threshold`. It then fits the lasso regression with the GLMNET package before returning the best combination of\n#' predictive variables based on the `lambda_type` selection criteria. \n#' \n#' \n#' @param dt data.frame or data.table\n#' @param candidate_vars character, the set of potential predictors within `dt`\n#' @param y character, the variable name of the outcome variable within `dt`\n#' @param weights character, weight variable name\n#' @param alpha integer, 1 for LASSO and 0 for ridge regression\n#' @param nfolds integer, the number of cross fold validations to perform\n#' @param seed integer, randomization seed \n#' @param lambda_type, either \"lambda.min\" or \"lambda.1se\". see `glmnet::glmnet()` for more information\n#' @param correlation_threshold numeric, the threshold for dropping correlated variables (value between 0 and 1)\n#' @param variance_threshold numeric, threshold for dropping variables that appear almost constant by specifying a minimum variance that must be met\n#' @param sparsity_threshold numeric, threshold for dropping sparse variables\n#' \n#' @export\n#' \n#' @import glmnet \n#' @importFrom caret findCorrelation\n\nglmnetlasso_vselect &lt;- function(dt, \n                                candidate_vars, \n                                y, \n                                weights, \n                                alpha = 1, \n                                nfolds, \n                                seed = 123,\n                                lambda_type = \"lambda.min\",\n                                correlation_threshold = 0.9, # Threshold for dropping correlated variables\n                                variance_threshold = 1e-4,\n                                sparsity_threshold = 0.99,# Threshold for dropping sparse variables\n                                ...) {\n  # Load necessary library\n  if (!requireNamespace(\"glmnet\", quietly = TRUE)) {\n    stop(\"The glmnet package is required. Please install it.\")\n  }\n  \n  if (!requireNamespace(\"caret\", quietly = TRUE)) {\n    install.packages(\"caret\")\n  }\n  \n  # Ensure reproducibility\n  set.seed(seed)\n  \n  # Standardize predictors\n  X_scaled &lt;- scale(dt[, candidate_vars])\n  y &lt;- dt[[y]]\n  weights &lt;- dt[[weights]]\n  \n  # Drop any columns with missing values in X_scaled\n  X_scaled &lt;- X_scaled[, colSums(is.na(X_scaled)) == 0]\n  \n  # Drop variables with very low variance\n  variances &lt;- apply(X_scaled, 2, var)\n  X_scaled &lt;- X_scaled[, variances &gt; variance_threshold]\n  \n  # Drop highly correlated variables\n  correlation_matrix &lt;- cor(X_scaled)\n  high_corr &lt;- caret::findCorrelation(correlation_matrix, \n                                      cutoff = correlation_threshold, \n                                      verbose = FALSE)\n  if (length(high_corr) &gt; 0) {\n    X_scaled &lt;- X_scaled[, -high_corr]\n  }\n  \n  # Identify and drop variables with 99% or more zeros\n  # Calculate proportion of zeros for each column\n  proportion_zeros &lt;- colMeans(X_scaled == 0)\n  \n  # Filter out columns where proportion of zeros is above the threshold\n  X_scaled &lt;- X_scaled[, proportion_zeros &lt; sparsity_threshold]\n  \n  # Check for problematic values\n  if (anyNA(X_scaled) || any(is.infinite(X_scaled)) || any(is.nan(X_scaled))) {\n    stop(\"Predictor matrix contains NA, Inf, or NaN values.\")\n  }\n  if (any(weights &lt;= 0)) {\n    stop(\"Weights contain non-positive values.\")\n  }\n\n  # Fit lasso model with cross-validation\n  cv_model &lt;- glmnet::cv.glmnet(\n    x = X_scaled,\n    y = y,\n    weights = weights,\n    alpha = alpha,  # alpha = 1 for lasso, 0 &lt; alpha &lt; 1 for elastic net\n    nfolds = nfolds,\n    family = \"gaussian\", # Change to \"binomial\" or \"poisson\" for other models\n    ...\n  )\n  \n  # Extract coefficients at the lambda with minimum cross-validated error\n  selected_model &lt;- glmnet::glmnet(\n    x = X_scaled,\n    y = y,\n    weights = weights,\n    alpha = alpha,\n    lambda = cv_model[[lambda_type]],\n    family = \"gaussian\",\n    ...\n  )\n  \n  # Identify selected variables\n  selected_variables &lt;- rownames(as.matrix(coef(selected_model)))[as.matrix(coef(selected_model)) != 0]\n  selected_variables &lt;- selected_variables[selected_variables != \"(Intercept)\"]\n  \n  return(selected_variables)\n  \n}\n\n\n\n#### lets call this function to model selection for each transformation\nnotglmnet_list &lt;- \n  glmnetlasso_vselect(dt = survey_dt,\n                      candidate_vars = candidate_vars,\n                      y = \"eqIncome\",\n                      weights = \"weight\",\n                      nfolds = 10,\n                      seed = 123,\n                      lambda_type = \"lambda.1se\",\n                      sparsity_threshold = 0.99)\n \n\nlogglmnet_list &lt;- \n  glmnetlasso_vselect(dt = survey_dt,\n                      candidate_vars = candidate_vars,\n                      y = \"logeqIncome\",\n                      weights = \"weight\",\n                      nfolds = 10,\n                      seed = 123,\n                      lambda_type = \"lambda.1se\",\n                      sparsity_threshold = 0.99)\n \nbcxglmnet_list &lt;- \n  glmnetlasso_vselect(dt = survey_dt,\n                      candidate_vars = candidate_vars,\n                      y = \"bcxeqIncome\",\n                      weights = \"weight\",\n                      nfolds = 10,\n                      seed = 123,\n                      lambda_type = \"lambda.1se\",\n                      sparsity_threshold = 0.99)\n\nNow that we have selected variables for each outcome variable transformation. We are ready to apply this povmap::ebp() to estimate a poverty map for Austria. The function will estimate a linear mixed effects model of the income in Austria from which the poverty line will be applied to estimate the poverty rates. Please run help(povmap::ebp) for details about the structure of the resulting object produced.\nFor the proper functioning of the povmap::ebp function, the following conditions must be met:\n\nthe arguments pop_data and smp_data require the survey and census datasets must be of class data.frame.\npop_data and smp_data must contain no missing observations across all the variables. Use the na.omit() function to ensure only complete cases are included within each object.\nAll target areas (or domains) within the smp_domains must be found within pop_domains i.e. for the Austria example, all domains/target areas in the survey must be found in the census.\nrescale_weights argument set to TRUE may solve parameter estimation convergence issues. This is not available within the emdi::ebp() function.\nNotice the specification of the Ydump argument. This is one difference between the emdi::ebp() and povmap::ebp(). This argument returns the result of L unit-level welfare simulated predictions from which other poverty estimations maybe carried out, as different from the default results present within ebp class object returned by ebp().\nAlso noteworthy, is the use of the weight_type argument present within the povmap::ebp() but not in the emdi::ebp(). Please see documentation for more details.\n\n\nfldr_temp &lt;- \"data-temp\"\nfile.path(fldr_temp, \"Ydump\") |&gt; dir.create(recursive = T, showWarnings = F)\n\n\n# using the ebp function to first estimate into the household survey. \n# We need to see how well the model will estimate poverty first into \n# the training set i.e. the household survey. this helps to check the\n# quality of in-sample estimation without any potential prediction bias\n\nmodel_formula &lt;-\n  paste0(\"eqIncome ~ \", paste(logglmnet_list, collapse = \" + \")) |&gt;\n  as.formula()\n\nlog_smodel &lt;-\n  povmap::ebp(\n    fixed = model_formula,\n    pop_data = survey_dt[, c(logglmnet_list, \"district\")],\n    pop_domains = \"district\",\n    smp_data = survey_dt[, c(\"eqIncome\", logglmnet_list, \"district\", \"weight\")],\n    smp_domains = \"district\",\n    MSE = TRUE,\n    threshold = 18207.2,\n    transformation = \"log\",\n    B = 50,\n    L = 50,\n    weights = \"weight\",\n    rescale_weights = TRUE,\n    Ydump = file.path(fldr_temp, \"Ydump/unitsurvey_logYdump_glmnet.csv\"),\n    cpus = 30\n  )\nsaveRDS(log_smodel, file.path(fldr_temp, \"log_surveymodel.RDS\"))\n\nmodel_formula &lt;-\n  paste0(\"eqIncome ~ \", paste(bcxglmnet_list, collapse = \" + \")) |&gt; \n  as.formula()\n\n\nbcx_smodel &lt;-\n  povmap::ebp(\n    fixed = model_formula,\n    pop_data = survey_dt[, c(bcxglmnet_list, \"district\")],\n    pop_domains = \"district\",\n    smp_data = survey_dt[, c(bcxglmnet_list, \"district\", \"eqIncome\", \"weight\")],\n    smp_domains = \"district\",\n    MSE = TRUE,\n    threshold = 18207.2,\n    transformation = \"box.cox\",\n    B = 50,\n    L = 50,\n    weights = \"weight\",\n    rescale_weights = TRUE,\n    Ydump = file.path(fldr_temp, \"Ydump/unitsurvey_bcxYdump_glmnet.csv\"),\n    cpus = 30,\n    weights_type = \"nlme_lambda\"\n  )\nsaveRDS(bcx_smodel, file.path(fldr_temp, \"bcx_surveymodel.RDS\"))\n\n\nmodel_formula &lt;- \n  paste0(\"eqIncome ~ \", paste(notglmnet_list, collapse = \" + \")) |&gt; \n  as.formula()\n\nnot_smodel &lt;-\n  povmap::ebp(\n    fixed = model_formula,\n    pop_data = survey_dt[, c(notglmnet_list, \"district\")],\n    pop_domains = \"district\",\n    smp_data = survey_dt[, c(notglmnet_list, \"district\", \"eqIncome\", \"weight\")],\n    smp_domains = \"district\",\n    MSE = TRUE,\n    threshold = 18207.2,\n    transformation = \"no\",\n    B = 50,\n    L = 50,\n    weights = \"weight\",\n    rescale_weights = TRUE,\n    Ydump = \"data/Ydump/unitsurvey_notYdump_glmnet.csv\",\n    cpus = 30\n  )\n\nsaveRDS(not_smodel, file.path(fldr_temp, \"not_surveymodel.RDS\"))\n\n### then we actually estimate the census level estimation\nmodel_formula &lt;- \n  paste0(\"eqIncome ~ \", paste(logglmnet_list, collapse = \" + \")) |&gt;\n  as.formula()\n\nlog_model &lt;-\n  povmap::ebp(\n    fixed = model_formula,\n    pop_data = census_dt[, c(logglmnet_list, \"district\")],\n    pop_domains = \"district\",\n    smp_data = survey_dt[, c(\"eqIncome\", logglmnet_list, \"district\", \"weight\")],\n    smp_domains = \"district\",\n    MSE = TRUE,\n    threshold = 18207.2,\n    transformation = \"log\",\n    B = 50,\n    L = 50,\n    weights = \"weight\",\n    rescale_weights = TRUE,\n    Ydump = file.path(fldr_temp, \"Ydump/unitcensus_logYdump_glmnet.csv\"),\n    cpus = 30\n  )\nsaveRDS(log_model, file.path(fldr_temp, \"log_censusmodel.RDS\"))\n\nmodel_formula &lt;-\n  paste0(\"eqIncome ~ \", paste(bcxglmnet_list, collapse = \" + \")) |&gt;\n  as.formula()\n\nbcx_model &lt;-\n  povmap::ebp(\n    fixed = model_formula,\n    pop_data = census_dt[, c(bcxglmnet_list, \"district\")],\n    pop_domains = \"district\",\n    smp_data = survey_dt[, c(bcxglmnet_list, \"district\", \"eqIncome\", \"weight\")],\n    smp_domains = \"district\",\n    MSE = TRUE,\n    threshold = 18207.2,\n    transformation = \"box.cox\",\n    B = 50,\n    L = 50,\n    weights = \"weight\",\n    rescale_weights = TRUE,\n    Ydump = file.path(fldr_temp, \"data/Ydump/unitcensus_bcxYdump_glmnet.csv\"),\n    cpus = 30,\n    weights_type = \"nlme_lambda\"\n  )\n\nsaveRDS(bcx_model, file.path(fldr_temp, \"bcx_censusmodel.RDS\"))\n\nmodel_formula &lt;- \n  paste0(\"eqIncome ~ \", paste(notglmnet_list, collapse = \" + \")) |&gt; \n  as.formula()\n\nnot_model &lt;-\n  povmap::ebp(\n    fixed = model_formula,\n    pop_data = census_dt[, c(notglmnet_list, \"district\")],\n    pop_domains = \"district\",\n    smp_data = survey_dt[, c(notglmnet_list, \"district\", \"eqIncome\", \"weight\")],\n    smp_domains = \"district\",\n    MSE = TRUE,\n    threshold = 18207.2,\n    transformation = \"no\",\n    B = 50,\n    L = 50,\n    weights = \"weight\",\n    rescale_weights = TRUE,\n    Ydump = file.path(fldr_temp, \"Ydump/unitcensus_notYdump_glmnet.csv\"),\n    cpus = 30\n  )\n\nsaveRDS(not_model, file.path(fldr_temp, \"data/not_censusmodel.RDS\"))\n\n\nlog_smodel &lt;- readRDS(file.path(fldr_temp, \"log_surveymodel.RDS\"))\nbcx_smodel &lt;- readRDS(file.path(fldr_temp, \"bcx_surveymodel.RDS\"))\nnot_smodel &lt;- readRDS(file.path(fldr_temp, \"not_surveymodel.RDS\"))\nlog_model &lt;- readRDS(file.path(fldr_temp, \"log_censusmodel.RDS\"))\nbcx_model &lt;- readRDS(file.path(fldr_temp, \"bcx_censusmodel.RDS\"))\nnot_model &lt;- readRDS(file.path(fldr_temp, \"not_censusmodel.RDS\"))",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Unit-Level Models</span>"
    ]
  },
  {
    "objectID": "ul.html#post-estimation-diagnostics",
    "href": "ul.html#post-estimation-diagnostics",
    "title": "5  Unit-Level Models",
    "section": "5.5 Post Estimation Diagnostics",
    "text": "5.5 Post Estimation Diagnostics\nOnce the model has been estimated. It is essential to test the validity of the model based on the model assumptions. Below are the set of typical tests which we will perform:\n\n5.5.1 Checking the quality of in-sample prediction\n\n5.5.1.1 Aggregate Poverty Rates at the lowest level of National Representativeness for EBP vs Direct Poverty Rates\nThe first test is to ensure the model is able to replicate the welfare distribution within the same survey. If this test fails, there is no reason to expect our model to provide unbiased estimates out of sample. We run the function ebp_national_pov() below to compare estimated poverty rates aggregated to the state level to the direct estimates from the survey. We apply the function to each model. Ideally, we want the estimated state poverty rates to be as close as possible to the direct estimates at the same level (typically within the 95% confidence level).\n\n#### first let us look at the quality of in-sample prediction\n\n#' A function to estimate poverty rates in levels different from the target area\n#' \n#' This function will take the ebp object (result of ebp()) as well as the population data to estimate \n#' poverty rates at a level different from the designated target area level stipulated for poverty \n#' mapping in the `ebp()` function. \n#' \n#' @param ebp_obj object of class `\"ebp\" \"emdi\"`\n#' @param wgt_var chr, the weight variable name \n#' @param pop_domain chr, the population domain used for `ebp()`\n#' @param est_domain chr, the estimation domain\n#' @param pop_dt data.frame, the population dataset/census\n#' @param indicators chr, a character or vector of characters for the indicators of interest\n#' \n#' @import data.table dplyr\n\nebp_national_pov &lt;- function(ebp_obj,\n                             wgt_var,\n                             pop_domain,\n                             est_domain,\n                             pop_dt,\n                             indicators = c(\"Head_Count\")) {\n  \n  \n  pop_dt &lt;- \n    pop_dt |&gt;\n    group_by(!!sym(pop_domain), !!sym(est_domain)) |&gt;\n    summarise(population = sum(!!sym(wgt_var), na.rm = TRUE), .groups = \"drop\")\n  \n  # Merge the ebp object data with population data\n  dt &lt;- merge(ebp_obj$ind, \n              pop_dt[, c(pop_domain, \"population\", est_domain)], \n              by.x = \"Domain\", \n              by.y = pop_domain, \n              all.x = TRUE)\n  \n  # Compute weighted means for all indicators\n  \n  pov_dt &lt;- \n    dt %&gt;%\n    as.data.table() %&gt;%\n    .[, lapply(.SD, weighted.mean, w = population, na.rm = TRUE),\n      .SDcols = indicators,\n      by = est_domain]\n\n  return(pov_dt)\n  \n}\n\n\nebpsurveypov_dt &lt;- lapply(\n  X = list(not_smodel, bcx_smodel, log_smodel),\n  FUN = function(ebp_obj) {\n    z &lt;- ebp_national_pov(\n      ebp_obj = ebp_obj,\n      wgt_var = \"weight\",\n      pop_domain = \"district\",\n      pop_dt = survey_dt,\n      est_domain = \"state\"\n    )\n    return(z)\n  }\n)\n\n### rename headcount columns appropriately\nebpsurveypov_dt &lt;-\n  mapply(\n    pov_dt = ebpsurveypov_dt,\n    name_obj = c(\"not\", \"bcx\", \"log\"),\n    FUN = function(pov_dt, name_obj) {\n      pov_dt |&gt;\n        rename_with(~ paste0(name_obj, .), .cols = \"Head_Count\")\n    },\n    SIMPLIFY = FALSE\n  )\n\nebpsurveypov_dt &lt;- Reduce(f = merge,\n                          x = ebpsurveypov_dt)\n\n### we create some fake clusters within the districts to show how to \n### use the survey package to estimate the standard errors on the \n### poverty rates, ideally these would already exist within your survey\n\n#### for simplicity we will assume there are 5 clusters in each district\nsurvey_dt &lt;-\n  survey_dt %&gt;%\n  group_by(district) %&gt;%\n  mutate(num_clusters = pmax(2, round(n() / sample(5:15, 1))), \n         cluster = sample(rep(1:num_clusters, length.out = n()))) %&gt;%\n  ungroup() %&gt;%\n  mutate(cluster_id = dense_rank(interaction(district, cluster)))\n\nWarning: There were 70 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `cluster = sample(rep(1:num_clusters, length.out = n()))`.\nℹ In group 1: `district = Neusiedl am See`.\nCaused by warning in `1:num_clusters`:\n! numerical expression has 16 elements: only the first used\nℹ Run `dplyr::last_dplyr_warnings()` to see the 69 remaining warnings.\n\nsvy_obj &lt;-\n  survey_dt %&gt;%\n  mutate(poor = ifelse(eqIncome &lt; 18207.2, 1, 0)) %&gt;%\n  dplyr::select(district, weight, state, poor, cluster_id) %&gt;%\n  survey::svydesign(\n    ids = ~ cluster_id,\n    weights = ~ weight,\n    strata = ~ state,\n    survey.lonely.psu = \"adjust\",\n    data = .\n  )\n\nvar_dt &lt;-\n  svyby(~ poor, ~ state, svy_obj, svymean) %&gt;%\n  mutate(poorLB = poor - 1.96 * se, poorUB = poor + 1.96 * se)\n\nebpsurveypov_dt &lt;-\n  ebpsurveypov_dt |&gt; \n  left_join(\n    var_dt |&gt; select(any_of(c(\"state\", \"poor\", \"poorLB\", \"poorUB\"))),\n    join_by(\"state\")\n  )\n\n\n5.5.1.1.1 Compare Survey EBP Poverty Rates to Direct Poverty Rates at different poverty lines\nAnother check for the quality of prediction within the survey is testing the predictive power of the model at different poverty lines along the entire income distribution. In the example below, we show compare the predicted poverty rate to the direct poverty rate using each 5th percentile increment in income i.e. using poverty ventile. We implement a function compare_surveyebp_atpovline() which computes poverty from the simulated welfare data (produced by Ydump argument in ebp()) and the survey at each poverty line. The function allows the user specify the percentiles at which poverty lines will be specified. The default is set at every 5th percentile (seq(0, 1, 0.05)).\nWe apply compare_surveyebp_at_povline() to each of the estimated models in comparison with the direct survey estimates at poverty line ventiles (every 5th percentile).\n\n### first let's read in the data ydump data\nydumppath_list &lt;- list.files(\n  path = file.path(fldr_temp, \"Ydump\"),\n  pattern = \"^unitsurvey_.*Ydump_glmnet.csv\",\n  full.names = TRUE\n)\n\n### the following code will read in the Ydump data into a list of \n###    data.frame objects and sequentially: \n### - combine the ydump with household survey\n### - compute poverty rates at each threshold\n\n#### lets write a function to do this for one of our models\nydump_dtlist &lt;- lapply(X = ydumppath_list, FUN = fread)\n\nnames(ydump_dtlist) &lt;-\n  basename(ydumppath_list) %&gt;%\n  sub(pattern = \"^unitsurvey_\", replacement = \"\") %&gt;%\n  sub(pattern = \"_glmnet.csv\", replacement = \"\")\n\n\n### a simple function to carry out the poverty rate comparisons (model vs actual at each poverty ventile, 5% increments) & plot the result\n\n\n#' Compute direct poverty rates and simulated outcome variables at specified poverty lines\n#' \n#' The function uses the simulated outcome from `povmap::ebp()`'s `Ydump` argument and the survey data to compute estimated and direct poverty    \n#' rates at a specified level than is estimated with `ebp()` at different poverty lines. \n#' \n#' @param ydump_data data.frame, the simulated welfare/income data from non-null `Ydump` argument within the `povmap::ebp()`\n#' @param smp_data data.frame, the survey data\n#' @param smp_domains character, a variable name for the domain/geographic level at which poverty rates will be estimated\n#' @param smp_weights character, the weight variable\n#' @param pline_increment numeric vector, a set of percentiles at which poverty rates should be estimated. \n#' \n#' @import data.table dplyr\n#' \n#' @export\n#' \ncompare_surveyebp_atpovline &lt;- function(ydump_data,\n                                        smp_data,\n                                        smp_domains,\n                                        smp_weights,\n                                        outcome_var,\n                                        pline_increment = seq(0, 1, 0.05)){\n  \n  ydump_dt &lt;- as.data.table(ydump_data)\n  smp_dt &lt;- as.data.table(smp_data[, c(smp_domains, smp_weights, outcome_var)])\n  \n  ydump_dt &lt;- cbind(ydump_dt, \n                    smp_dt[, c(smp_domains, smp_weights), with = FALSE] %&gt;%\n                      arrange(smp_domains))\n  \n  threshold_list &lt;-\n    wtd.quantile(x = smp_dt[[outcome_var]],\n                 weights = smp_dt[[smp_weights]],\n                 probs = pline_increment)\n  \n  compute_ptile_pov &lt;- function(welfare_dt,\n                                weights,\n                                welfare,\n                                thresholds){\n  \n    pov_list &lt;- \n    lapply(X = thresholds,\n           FUN = function(x){\n             \n             yy &lt;- \n               welfare_dt %&gt;%\n               mutate(poor = ifelse(!!sym(welfare) &lt; x, 1, 0)) %&gt;%\n               summarize(prate = weighted.mean(x = poor,\n                                               w = !!sym(weights),\n                                               na.rm = TRUE))\n             \n             return(yy)\n             \n           })\n    \n    pov_list &lt;- \n      unlist(pov_list) %&gt;%\n      data.frame() %&gt;%\n      setNames(\"pov\")\n    \n    \n    return(pov_list)\n    \n  }\n  \n  pov_dt &lt;- compute_ptile_pov(welfare_dt = ydump_dt,\n                              weights = smp_weights,\n                              welfare = \"Simulated_Y\",\n                              thresholds = threshold_list)\n  \n  pov_dt &lt;- data.table(ebp_poverty = pov_dt,\n                       probs = as.character(seq(0, 1, 0.05)))\n\n  \n  # povcheck_plots &lt;- \n  #   pov_dt %&gt;%\n  #   mutate(y )\n  #   ggplot(aes(x = as.numeric(probs)))\n  \n  return(pov_dt)\n  \n}\n\n### now lets plot the results\nsvyplinecomp_list &lt;-\n  lapply(\n    X = ydump_dtlist,\n    FUN = function(ydump) {\n      compare_surveyebp_atpovline(\n        ydump_data = ydump,\n        smp_data = survey_dt,\n        smp_domains = \"district\",\n        smp_weights = \"weight\",\n        outcome_var = \"eqIncome\"\n      )\n    }\n  )\n\n## a bit of cleaning and then making the plots\n\n\nsvyplinecomp_dt &lt;-\n  mapply(\n    comp_dt = svyplinecomp_list,\n    name_list = names(svyplinecomp_list),\n    FUN = function(comp_dt, name_list) {\n      comp_dt %&gt;%\n        rename(ebp = \"ebp_poverty.pov\") %&gt;%\n        mutate(probs = as.numeric(probs)) %&gt;%\n        mutate(diff = abs(ebp - probs) / probs) %&gt;%\n        mutate(transformation = sub(\"Ydump\", \"\", name_list))\n      \n    },\n    SIMPLIFY = FALSE\n  ) |&gt; \n  bind_rows()\n\nWe plot the absolute value of the percent difference between both poverty rates at each ventile as seen in the plot below.\n\nsvyplinecomp_dt %&gt;%\n  ggplot() +\n  geom_line(aes(x = probs,\n                y = diff,\n                color = transformation)) + \n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n5.5.1.2 Evaluating the Standard Normality Assumptions\n\nPresenting the regression table estimates (use povmap::ebp_reportcoef_table() and then translate into a flextable which can be rendered in Word, PDF or HTML)\nChecking that all model assumptions hold (normality assumptions for the miu and epsilon terms), using povmap::ebp_normalityfit() to present skewness and kurtosis for both errors. Then show a function that uses ebp object to plot the distribution of the errors and compute the kolmogrov-smirnov test statistics. We can also include the shapiro-wilks which will break down for larger sample sizes but is equally well known. Another function that produces q-q plots for miu and epsilon terms from ebp object.\nCheck on model validity: Create a plot to show how poverty rates vary at each ventile i.e. at 5% poverty line increments. This is to show how to check the quality of model prediction without the added bias of out of sample prediction\nComputing MSE and CVs and computing the statistical gains made from performing small area estimation i.e. in practical terms, how much bigger would the survey have to be the get the same level of precision that SAE now gives us with the census data.\nFinal validation: EBP estimates vs Direct Estimates (supposedly truth) at the highest resolution level that is considered nationally representative, this is usually the regional level in Africa.\nPlotting the poverty map using the ebp object and the shapefile\n\n\n#### presenting the results of the linear mixed effects regression\nspecify_decimal &lt;- function(x, k) trimws(format(round(x, k), nsmall=k))\n\n### to quickly create coefficients table\ncoef_dt &lt;- povmap::ebp_reportcoef_table(log_model) \n\n## showing how to present the skewness and kurtosis of the random effects \n##   and model errors\nerr_dt &lt;-\n  povmap::ebp_normalityfit(log_model) %&gt;%\n  mutate(value = specify_decimal(value, 3))\n\n### to convert this into a publication ready report\ncreate_regression_table &lt;- function(coef_table, eval_table) {\n  \n  # Rename columns for consistency\n  coef_table &lt;- \n    coef_table %&gt;%\n    rename(Description = Variable,\n           Estimate = coeff, \n           `Std. Error` = std_error) %&gt;%\n    mutate(Type = \"Coefficients\") %&gt;%\n    \n    # Format std. error under estimates\n    mutate(Estimate = paste0(Estimate, \"\\n(\", `Std. Error`, \")\")) %&gt;%  \n    dplyr::select(Type, Description, Estimate)\n\n  eval_table &lt;- eval_table %&gt;%\n    rename(Description = indicator, Estimate = value) %&gt;%\n    mutate(Type = \"Diagnostics\")\n\n  # Combine coefficient and diagnostic tables\n  full_table &lt;- rbind(coef_table, \n                      eval_table %&gt;%\n                        dplyr::select(colnames(coef_table)))\n\n  # Identify where diagnostics start\n  diag_start &lt;- nrow(coef_table) + 1\n\n  # Create flextable\n  regression_table &lt;- \n  flextable(full_table) %&gt;%\n    set_header_labels(\n      Type = \"Type\",\n      Description = \"Description\",\n      Estimate = \"Estimate\"\n    ) %&gt;%\n    hline(part = \"all\") %&gt;%\n    merge_v(j = \"Type\") %&gt;%\n    align(j = \"Description\", align = \"left\", part = \"all\") %&gt;%\n    align(j = \"Estimate\", align = \"center\", part = \"all\") %&gt;%\n    vline(j = 1, border = fp_border(color = \"black\", width = 1)) %&gt;%\n    hline(i = diag_start - 1, border = fp_border(color = \"black\", width = 1)) %&gt;%\n    autofit() %&gt;%\n    width(j = \"Type\", width = 1.2) %&gt;%\n    width(j = \"Description\", width = 2) %&gt;%\n    width(j = \"Estimate\", width = 1.5)\n\n  # Styling for diagnostics\n  regression_table &lt;- regression_table %&gt;%\n    bold(i = diag_start:nrow(full_table), bold = TRUE) %&gt;%\n    italic(i = diag_start:nrow(full_table), italic = TRUE) %&gt;%\n    font(fontname = \"Times New Roman\", part = \"all\") %&gt;%\n    fontsize(size = 10, part = \"all\")\n\n  return(regression_table)\n  \n}\n\n### regression tables\ncreate_regression_table(coef_table = coef_dt,\n                        eval_table = err_dt)\n\nTypeDescriptionEstimateCoefficients(Intercept)9.161***(0.032)eqsize-0.043***(0.013)cash0.00003***(0.0000008)self_empl0.000022***(0.000001)unempl_ben0.000021***(0.0000038)age_ben0.00003***(0.0000011)surv_ben0.000032***(0.0000078)sick_ben0.000028**(0.00001)dis_ben0.000036***(0.0000025)rent0.000013***(0.0000014)house_allow0.000051**(0.000021)cap_inv0.000015***(0.0000027)tax_adj-0.000014**(0.0000043)cash_X_gender0.0000047***(0.000001)self_empl_X_gender0.0000082***(0.0000023)age_ben_X_gender0.0000022(0.0000014)rent_X_gender0.0000055**(0.0000025)cap_inv_X_gender0.0000078(0.0000043)Diagnosticsrsq_marginal0.518rsq_conditional0.601epsilon_skewness-2.213epsilon_kurtosis18.270random_skewness-0.721random_kurtosis3.508\n\n\n\n### start by explaining that the plot function could be used in R \n###  to show the error diagnostics as follows\n# plot(log_model, whitch = 1)\n\n### a more thorough test on normality using the Kolmogrov-Smirnov tests\nebp_kstest &lt;- function(ebp_obj) {\n  \n  epsilon_values &lt;- residuals(ebp_obj$model, level = 0, type = \"pearson\") |&gt;\n    as.numeric()\n  mu_values &lt;- as.numeric(nlme::ranef(ebp_obj$model)$\"(Intercept)\")\n  \n  mu_values &lt;- (mu_values - mean(mu_values, na.rm = TRUE)) /\n    sd(mu_values, na.rm = TRUE)\n  \n  xx &lt;- ks.test(\n    x = epsilon_values,\n    y = \"pnorm\",\n    mean = mean(epsilon_values),\n    sd = sd(epsilon_values)\n  )\n  \n  yy &lt;- ks.test(\n    x = mu_values,\n    y = \"pnorm\",\n    mean = mean(mu_values),\n    sd = sd(mu_values)\n  )\n  \n  plot_random &lt;-\n    mu_values %&gt;%\n    as.data.table() %&gt;%\n    setnames(new = \"values\") %&gt;%\n    ggplot() +\n    geom_density(aes(x = values), fill = \"blue\", alpha = 0.5) +\n    labs(\n      x = \"Random Effects\", \n      y = \"Density\", \n      title = \"Random Effect Residuals (\\u03BC)\"\n    ) +\n    theme_minimal() +\n    ylim(0, 0.4) +\n    xlim(-4, 4) +\n    annotate(\n      \"richtext\",\n      x = 0,\n      y = 0.3,\n      label = \"&lt;b&gt;Kolmogrov-Smirnov Normality Test&lt;/b&gt;\",\n      fill = NA,\n      label.color = NA\n    ) +\n    annotate(\n      \"text\",\n      x = 0,\n      y = 0.28,\n      label =\n        paste0(\n          \"D-stat = \",\n          specify_decimal(yy$statistic, 3),\n          \"; p-value = \",\n          specify_decimal(yy$p.value, 3)\n        ),\n      size = 3\n    )\n  \n  plot_error &lt;-\n    epsilon_values %&gt;%\n    as.data.table() %&gt;%\n    setnames(new = \"values\") %&gt;%\n    ggplot() +\n    geom_density(aes(x = values), fill = \"blue\", alpha = 0.5) +\n    labs(x = \"Model Error Terms\",\n         y = \"Density\", \n         title = \"Standardized Error Residuals (\\u03B5)\") +\n    theme_minimal() +\n    annotate(\n      \"richtext\",\n      x = 0.75,\n      y = 0.3,\n      label = \"&lt;b&gt;Kolmogrov-Smirnov Normality Test&lt;/b&gt;\",\n      fill = NA,\n      label.color = NA\n    ) +\n    annotate(\n      \"text\",\n      x = 0,\n      y = 0.28,\n      label = paste0(\n        \"D-stat = \",\n        specify_decimal(xx$statistic, 3),\n        \"; p-value = \",\n        specify_decimal(xx$p.value, 3)\n      ),\n      size = 3\n    )\n  \n  plot_random + plot_error\n}\n\nebp_kstest(log_model)\n\n\n\n\n\n\n\n\n\n\n\n5.5.2 CV Estimation\n\n### it is important to compare Coefficient of Variation between the survey direct estimates and the census computations. \n\n#### first using the povmap::direct() function to estimate the direct and model estimate cvs for the poverty rates\n\nebp_compare_cv &lt;- function(ebp_obj, ...) {\n  \n  # Convert to data.table\n  ind_dt &lt;- as.data.table(ebp_obj$ind)\n  mse_dt &lt;- as.data.table(ebp_obj$MSE)\n\n  # Ensure mse_dt and ind_dt are numeric except Domain\n  mse_numeric &lt;- mse_dt[, !(\"Domain\"), with = FALSE]\n  ind_numeric &lt;- ind_dt[, !(\"Domain\"), with = FALSE]\n\n  # Compute CV\n  ebpcv_dt &lt;- sqrt(mse_numeric) / ind_numeric\n\n  # Add Domain back\n  ebpcv_dt[, Domain := ind_dt$Domain]\n\n  # Compute direct estimation\n  direct_dt &lt;- povmap::direct(...)\n\n  # Identify numeric columns common to both\n  common_cols &lt;- intersect(names(direct_dt$ind), names(direct_dt$MSE))\n  numeric_cols &lt;- common_cols[sapply(as.data.table(direct_dt$ind)[, ..common_cols], is.numeric)]\n\n  # Compute CV for direct estimation\n  directcv_dt &lt;- sqrt(as.data.table(direct_dt$MSE)[, ..numeric_cols]) / \n                 as.data.table(direct_dt$ind)[, ..numeric_cols]\n\n  # Add Domain back and rename\n  directcv_dt &lt;- \n    cbind(as.data.table(direct_dt$ind)[, .(Domain)], directcv_dt) %&gt;%\n          dplyr::select(Domain, Head_Count) %&gt;%\n          rename(HT_Head_Count = Head_Count)\n\n  # Merge direct and EBP results\n  cv_dt &lt;- directcv_dt[ebpcv_dt, on = \"Domain\"]\n\n  # Rename columns for clarity\n  setnames(cv_dt, \"Head_Count\", \"Head_Count_CV\")\n  setnames(cv_dt, \"HT_Head_Count\", \"HT_Head_Count_CV\")\n\n  return(cv_dt)\n  \n}\n\n\ncv_dt &lt;- ebp_compare_cv(ebp_obj = log_model,\n                        y = \"eqIncome\",\n                        smp_data = survey_dt,\n                        smp_domains = \"district\",\n                        weights = \"weight\",\n                        var = TRUE,\n                        threshold = 18207.2,\n                        design = \"state\",\n                        na.rm = TRUE,\n                        HT = TRUE)\n\n\n### it is useful to be able to be able to figure out how \ngains_value &lt;- \n  cv_dt |&gt;\n  mutate(gains = HT_Head_Count_CV / Head_Count_CV) |&gt;\n  summarize(mean(gains, na.rm = TRUE))\n\n#### create visualization to show how the EBP CVs are an improvement on the Horwitz-Thompson Headcount CVs\n#### as a result of SAE\n\ncv_dt |&gt;\n  ggplot() + \n  geom_point(aes(y = Head_Count_CV, x = HT_Head_Count_CV)) + \n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"blue\") + \n  labs(x = \"Horwitz-Thompson Headcount CV\",\n       y = \"EBP Head Count CV\") + \n  xlim(0, 1.5) + \n  ylim(0, 1.5) + \n  scale_color_viridis_d(option = \"plasma\") + \n  theme_minimal()\n\nWarning: Removed 26 rows containing missing values or values outside the scale range\n(`geom_point()`).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Unit-Level Models</span>"
    ]
  },
  {
    "objectID": "ul.html#poverty-map",
    "href": "ul.html#poverty-map",
    "title": "5  Unit-Level Models",
    "section": "5.6 Poverty map",
    "text": "5.6 Poverty map\n\n### plot the poverty map\n\n#### load the shapefile from the povmap\npovmap::load_shapeaustria()\n\nlog_model$ind[c(\"Domain\", \"Head_Count\")] |&gt;\n  merge(shape_austria_dis[, c(\"PB\")],\n        by.x = \"Domain\", by.y = \"PB\") |&gt;\n  sf::st_as_sf(crs = 4326) |&gt;\n  ggplot() + \n  geom_sf(aes(fill = Head_Count)) + \n  scale_fill_viridis_c(option = \"magma\",\n                       direction = -1,\n                       name = \"Poverty Rate\") + \n  labs(title = \"Poverty Map: Austria\") + \n  theme_minimal() + \n  theme(axis.title = element_blank(), # Remove axis labels\n        axis.text = element_blank(),  # Remove axis text\n        panel.grid = element_blank(), # Remove grid lines\n        plot.title = element_text(hjust = 0.5, size = 16), # Center and style title\n        plot.subtitle = element_text(hjust = 0.5, size = 12))\n\n\n\n\n\n\n\nFigure 5.3\n\n\n\n\n\n\n\n\n\n\n\nCorral, Paul, Isabel Molina, Alexandru Cojocaru, and Sandra Segovia. 2022. Guidelines to Small Area Estimation for Poverty Mapping. World Bank Washington.\n\n\nKreutzmann, Ann-Kristin, Sören Pannier, Natalia Rojas-Perilla, Timo Schmid, Matthias Templ, and Nikos Tzavidis. 2019. “The r Package Emdi for Estimating and Mapping Regionally Disaggregated Indicators.” Journal of Statistical Software 91: 1–33.\n\n\nMolina, Isabel, and Jon NK Rao. 2010. “Small Area Estimation of Poverty Indicators.” Canadian Journal of Statistics 38 (3): 369–85.\n\n\nRao, John NK, and Isabel Molina. 2015. Small Area Estimation. John Wiley & Sons.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Unit-Level Models</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Brown, G., R. Chambers, P. Heady, and D. Heasman. 2001.\n“Evaluation of Small Area Estimation Methods - an Application to\nUnemployment Estimates from the UK\nLFS.” In Proceedings of Statistics Canada\nSymposium.\n\n\nCasas-Cordero, C., J. Encina, and P. Lahiri. 2016. “Poverty\nMapping for the Chilean Comunas.” In Analysis of Poverty by\nSmall Area Estimation, by M. Pratesi, 379–403. John Wiley &\nSons. https://doi.org/10.1002/9781118814963.ch20.\n\n\nChandra, H., N. Salvati, and R. Chambers. 2015. “A Spatially\nNonstationary Fay-Herriot Model for Small Area\nEstimation.” Journal of the Survey Statistics and\nMethodology 3 (2): 109–35. https://doi.org/10.1093/jssam/smu026.\n\n\nCorral, Paul, Isabel Molina, Alexandru Cojocaru, and Sandra Segovia.\n2022a. Guidelines to Small Area Estimation for Poverty Mapping.\nWorld Bank Washington.\n\n\n———. 2022b. Guidelines to Small Area Estimation for Poverty\nMapping. World Bank Washington.\n\n\nDatta, G. S., M. Ghosh, R. Steorts, and J. Maples. 2011. “Bayesian\nBenchmarking with Applications to Small Area Estimation.”\nTEST 20 (3): 574–88. https://doi.org/10.1007/s11749-010-0218-y.\n\n\nFay, Robert E., and Roger A. Herriot. 1979. “Estimates of Income\nfor Small Places: An Application of\nJames-Stein Procedures to Census Data.”\nJournal of the American Statistical Association 74: 269–77.\n\n\nHarmening, Sylvia, Ann-Kristin Kreutzmann, Sören Schmidt, Nicola\nSalvati, and Timo Schmid. 2023. “A Framework for Producing Small\nArea Estimates Based on Area-Level Models in r.” The R\nJournal 15 (1): 316–41. https://doi.org/10.32614/RJ-2023-039.\n\n\nKomsta, Lukasz, and Frederick Novomestky. 2015. Moments: Moments,\nCumulants, Skewness, Kurtosis and Related Tests. https://CRAN.R-project.org/package=moments.\n\n\nKreutzmann, Ann-Kristin, Sören Pannier, Natalia Rojas-Perilla, Timo\nSchmid, Matthias Templ, and Nikos Tzavidis. 2019. “The r Package\nEmdi for Estimating and Mapping Regionally Disaggregated\nIndicators.” Journal of Statistical Software 91: 1–33.\n\n\nLahiri, P., and J. Suntornchost. 2015. “Variable Selection for\nLinear Mixed Models with Applications in Small Area Estimation.”\nThe Indian Journal of Statistics 77-B (2): 312–20. https://www.jstor.org/stable/43694416.\n\n\nLumley, Thomas. 2024. “Survey: Analysis of Complex Survey\nSamples.”\n\n\nMarhuenda, Y., D. Morales, and M. del Camen Pardo. 2014.\n“Information Criteria for Fay-Herriot Model\nSelection.” Computational Statistics and Data Analysis\n70: 268–80. https://doi.org/10.1016/j.csda.2013.09.016.\n\n\nMolina, Isabel, and Yolanda Marhuenda. 2015. “sae: An R Package for Small Area\nEstimation.” The R Journal 7 (1): 81–98. https://journal.r-project.org/archive/2015/RJ-2015-007/RJ-2015-007.pdf.\n\n\nMolina, Isabel, and Jon NK Rao. 2010. “Small Area Estimation of\nPoverty Indicators.” Canadian Journal of Statistics 38\n(3): 369–85.\n\n\nRao, J. N. K., and Isabel Molina. 2015. Small Area Estimation.\nJohn Wiley; Sons, Inc, Hoboken, NJ, USA.\n\n\nRao, John NK, and Isabel Molina. 2015. Small Area Estimation.\nJohn Wiley & Sons.\n\n\nSchmid, T., F. Bruckschen, N. Salvati, and T. Zbiranski. 2017.\n“Constructing Sociodemographic Indicators for National Statistical\nInstitutes Using Mobile Phone Data: Estimating Literacy Rates in\nSenegal.” Journal of the Royal Statistical\nSociety A 180 (4): 1163–90. https://doi.org/10.1111/rssa.12305.\n\n\nYou, Yong, and Mike Hidiroglou. 2023. “Application of Sampling\nVariance Smoothing Methods for Small Area Proportion Estimation.”\nJournal of Official Statistics 39 (4): 571–90.",
    "crumbs": [
      "References"
    ]
  }
]